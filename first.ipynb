{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e535b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f96bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Samreen fathima Sanaullah\n"
     ]
    }
   ],
   "source": [
    "File= open(\"notes.txt\", \"r\")\n",
    "Content = File.read()\n",
    "print(Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56e5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "File= open(\"notes.text\",'w')\n",
    "File.write(\"Content  Changed\")\n",
    "File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31cd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('notes.txt', 'w')\n",
    "file.write('content changed')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8ef14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('notes.txt', 'a')\n",
    "file.write('\\ncontent changed\\n')\n",
    "file.write('Second line\\n')\n",
    "file.write('Beautifully added')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7ab152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content changed\n",
      "content changed\n",
      "Second line\n",
      "Beautifully added\n"
     ]
    }
   ],
   "source": [
    "with open('notes.txt', 'r') as file:\n",
    " Content=file.read()\n",
    " print(Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ba4157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Samreen Fathima\\\\Desktop\\\\Python_Tutorial'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fbf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84b19ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first.ipynb', 'notes.text', 'notes.txt', 'samreen', 'test', 'test.py']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740eb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting langchain-core<1.0.0,>=0.3.66\n",
      "  Downloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "     -------------------------------------- 441.4/441.4 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting langchain<1.0.0,>=0.3.26\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 2.7 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.8/64.8 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "     -------------------------------------- 162.0/162.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.12.14-cp311-cp311-win_amd64.whl (452 kB)\n",
      "     -------------------------------------- 452.3/452.3 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.2/45.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting langsmith>=0.1.125\n",
      "  Downloading langsmith-0.4.5-py3-none-any.whl (367 kB)\n",
      "     -------------------------------------- 367.8/367.8 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Collecting numpy>=1.26.2\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "     ---------------------------------------- 13.0/13.0 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "     -------------------------------------- 63.8/63.8 kB 850.4 kB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp311-cp311-win_amd64.whl (45 kB)\n",
      "     -------------------------------------- 45.9/45.9 kB 566.7 kB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.7/86.7 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.9/50.9 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "     -------------------------------------- 444.8/444.8 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging<25,>=23.2\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 73.5/73.5 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.18-cp311-cp311-win_amd64.whl (134 kB)\n",
      "     ------------------------------------ 134.6/134.6 kB 797.2 kB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.5/54.5 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "     ------------------------------------ 495.4/495.4 kB 646.9 kB/s eta 0:00:00\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "     -------------------------------------- 105.4/105.4 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     -------------------------------------- 129.8/129.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "     -------------------------------------- 162.7/162.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-win_amd64.whl (297 kB)\n",
      "     ------------------------------------ 297.0/297.0 kB 874.3 kB/s eta 0:00:00\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "     -------------------------------------- 100.9/100.9 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.8/78.8 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tenacity, sniffio, PyYAML, python-dotenv, pydantic-core, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, httpx-sse, h11, greenlet, frozenlist, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, marshmallow, jsonpatch, httpcore, anyio, aiosignal, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 certifi-2025.7.14 charset_normalizer-3.4.2 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langsmith-0.4.5 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 numpy-2.3.1 orjson-3.10.18 packaging-24.2 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.1 urllib3-2.5.0 yarl-1.20.1 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6812104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b319982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TextLoader(\"notes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5fdd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'notes.txt'}, page_content='content changed\\ncontent changed\\nSecond line\\nBeautifully added')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74aeb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f76f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
      "     -------------------------------------- 309.7/309.7 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d087afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = PyPDFLoader(\"llm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b39eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 0, 'page_label': '1'}, page_content='LLM documentation\\nRelease 0.26-31-g0bf655a\\nSimon Willison\\nJun 20, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 1, 'page_label': '2'}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 2, 'page_label': 'i'}, page_content='CONTENTS\\n1 Quick start 3\\n2 Contents 5\\n2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Upgrading to the latest version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.3 Using uvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.4 A note about Homebrew and PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.5 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.6 API key management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.1.7 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.2 Starting an interactive chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2.3 Listing available models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.2.4 Setting default options for models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3 OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3.2 OpenAI language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.3.3 Model features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.3.4 OpenAI embedding models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3.5 OpenAI completion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3.6 Adding more OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.4.1 Installing and using a local model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.4.2 OpenAI-compatible models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n2.5 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.1 How tools work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.2 Trying out tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.5.3 LLM’s implementation of tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.5.4 Default tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.5.5 Tips for implementing tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6 Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6.1 Schemas tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6.2 Using JSON schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.6.3 Ways to specify a schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n2.6.4 Concise LLM schema syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n2.6.5 Saving reusable schemas in templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.6.6 Browsing logged JSON objects created using schemas . . . . . . . . . . . . . . . . . . . . 48\\n2.7 Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\ni'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 3, 'page_label': 'ii'}, page_content='2.7.1 Getting started with –save . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n2.7.2 Using a template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.3 Listing available templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.4 Templates as YAML files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.5 Template loaders from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.8 Fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.8.1 Using fragments in a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.8.2 Using fragments in chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.8.3 Browsing fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.8.4 Setting aliases for fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.8.5 Viewing fragments in your logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.8.6 Using fragments from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.8.7 Listing available fragment prefixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9 Model aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9.1 Listing aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9.2 Adding a new alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n2.9.3 Removing an alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.9.4 Viewing the aliases file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n2.10.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . . 74\\n2.10.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.11.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n2.11.4 Developing a model plugin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n2.11.5 Advanced model plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n2.11.6 Utility functions for plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n2.12 Python API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n2.12.1 Basic prompt execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n2.12.2 Async models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n2.12.3 Conversations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n2.12.4 Listing models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n2.12.5 Running code when a response has completed . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n2.12.6 Other functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n2.13 Logging to SQLite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n2.13.1 Viewing the logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n2.13.2 Browsing logs using Datasette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n2.13.3 Backing up your database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n2.13.4 SQL schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n2.14 Related tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.1 strip-tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.2 ttok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.3 Symbex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.15 CLI reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.15.1 llm –help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.16 Contributing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n2.16.1 Updating recorded HTTP API interactions and associated snapshots . . . . . . . . . . . . . 150\\n2.16.2 Debugging tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n2.16.3 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n2.16.4 Release process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17 Changelog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\nii'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 4, 'page_label': 'iii'}, page_content='2.17.1 0.26 (2025-05-27) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17.2 0.26a1 (2025-05-25) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17.3 0.26a0 (2025-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n2.17.4 0.25 (2025-05-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n2.17.5 0.25a0 (2025-04-10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.6 0.24.2 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.7 0.24.1 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.8 0.24 (2025-04-07) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.9 0.24a1 (2025-04-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.10 0.24a0 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.11 0.23 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.12 0.22 (2025-02-16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n2.17.13 0.21 (2025-01-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.14 0.20 (2025-01-22) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.15 0.19.1 (2024-12-05) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.16 0.19 (2024-12-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.17 0.19a2 (2024-11-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.18 0.19a1 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.19 0.19a0 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.20 0.18 (2024-11-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.21 0.18a1 (2024-11-14) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.22 0.18a0 (2024-11-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.23 0.17 (2024-10-29) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.24 0.17a0 (2024-10-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.25 0.16 (2024-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.26 0.15 (2024-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.27 0.14 (2024-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.28 0.13.1 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.29 0.13 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.30 0.12 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.31 0.11.2 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.32 0.11.1 (2023-10-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.33 0.11 (2023-09-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.34 0.10 (2023-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n2.17.35 0.10a1 (2023-09-11) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.36 0.10a0 (2023-09-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.37 0.9 (2023-09-03) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.38 0.8.1 (2023-08-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n2.17.39 0.8 (2023-08-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n2.17.40 0.7.1 (2023-08-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.41 0.7 (2023-08-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.42 0.6.1 (2023-07-24) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.43 0.6 (2023-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n2.17.44 0.5 (2023-07-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n2.17.45 0.4.1 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n2.17.46 0.4 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n2.17.47 0.3 (2023-05-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n2.17.48 0.2 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n2.17.49 0.1 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\nIndex 175\\niii'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 5, 'page_label': 'iv'}, page_content='iv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 6, 'page_label': '1'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nA CLI tool and Python library for interacting withOpenAI, Anthropic’s Claude, Google’s Gemini, Meta’s Llama\\nand dozens of other Large Language Models, both via remote APIs and with models that can be installed and run on\\nyour own machine.\\nWatchLanguage models on the command-lineon YouTube for a demo or read the accompanying detailed notes.\\nWith LLM you can:\\n• Run prompts from the command-line\\n• Store prompts and responses in SQLite\\n• Generate and store embeddings\\n• Extract structured content from text and images\\n• Grant models the ability to execute tools\\n• ... and much, much more\\nCONTENTS 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 7, 'page_label': '2'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2 CONTENTS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 8, 'page_label': '3'}, page_content='CHAPTER\\nONE\\nQUICK START\\nFirst, install LLM usingpipor Homebrew orpipxor uv:\\npip install llm\\nOr with Homebrew (seewarning note):\\nbrew install llm\\nOr with pipx:\\npipx install llm\\nOr with uv\\nuv tool install llm\\nIf you have an OpenAI API key key you can run this:\\n# Paste your OpenAI API key into this\\nllm keys set openai\\n# Run a prompt (with the default gpt-4o-mini model)\\nllm \"Ten fun names for a pet pelican\"\\n# Extract text from an image\\nllm \"extract text\" -a scanned-document.jpg\\n# Use a system prompt against a file\\ncat myfile.py | llm -s \"Explain this code\"\\nRun prompts against Gemini or Anthropic with their respective plugins:\\nllm install llm-gemini\\nllm keys set gemini\\n# Paste Gemini API key here\\nllm -m gemini-2.0-flash \\'Tell me fun facts about Mountain View\\'\\nllm install llm-anthropic\\nllm keys set anthropic\\n# Paste Anthropic API key here\\nllm -m claude-4-opus \\'Impress me with wild facts about turnips\\'\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 9, 'page_label': '4'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nYou can alsoinstall a pluginto access models that can run on your local device. If you use Ollama:\\n# Install the plugin\\nllm install llm-ollama\\n# Download and run a prompt against the Orca Mini 7B model\\nollama pull llama3.2:latest\\nllm -m llama3.2:latest 'What is the capital of France?'\\nTo startan interactive chatwith a model, usellm chat:\\nllm chat -m gpt-4.1\\nChatting with gpt-4.1\\nType 'exit' or 'quit' to exit\\nType '!multi' to enter multiple lines, then '!end' to finish\\nType '!edit' to open your default editor and modify the prompt.\\nType '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\\n> Tell me a joke about a pelican\\nWhy don't pelicans like to tip waiters?\\nBecause they always have a big bill!\\nMore background on this project:\\n• llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs\\n• The LLM CLI tool now supports self-hosted language models via plugins\\n• LLM now provides tools for working with embeddings\\n• Build an image search engine with llm-clip, chat with models with llm chat\\n• You can now run prompts against images, audio and video in your terminal using LLM\\n• Structured data extraction from unstructured content using LLM schemas\\n• Long context support in LLM 0.24 using fragments and template plugins\\nSee also the llm tag on my blog.\\n4 Chapter 1. Quick start\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 10, 'page_label': '5'}, page_content='CHAPTER\\nTWO\\nCONTENTS\\n2.1 Setup\\n2.1.1 Installation\\nInstall this tool usingpip:\\npip install llm\\nOr using pipx:\\npipx install llm\\nOr using uv (more tips below):\\nuv tool install llm\\nOr using Homebrew (seewarning note):\\nbrew install llm\\n2.1.2 Upgrading to the latest version\\nIf you installed usingpip:\\npip install -U llm\\nFor pipx:\\npipx upgrade llm\\nFor uv:\\nuv tool upgrade llm\\nFor Homebrew:\\nbrew upgrade llm\\nIf the latest version is not yet available on Homebrew you can upgrade like this instead:\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 11, 'page_label': '6'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm install -U llm\\n2.1.3 Using uvx\\nIf you have uv installed you can also use theuvxcommand to try LLM without first installing it like this:\\nexport OPENAI_API_KEY='sx-...'\\nuvx llm 'fun facts about skunks'\\nThis will install and run LLM using a temporary virtual environment.\\nYou can use the--withoption to add extra plugins. To use Anthropic’s models, for example:\\nexport ANTHROPIC_API_KEY='...'\\nuvx --with llm-anthropic llm -m claude-3.5-haiku 'fun facts about skunks'\\nAll of the usual LLM commands will work withuvx llm. Here’s how to set your OpenAI key without needing an\\nenvironment variable for example:\\nuvx llm keys set openai\\n# Paste key here\\n2.1.4 A note about Homebrew and PyTorch\\nTheversionofLLMpackagedforHomebrewcurrentlyusesPython3.12. ThePyTorchprojectdonotyethaveastable\\nrelease of PyTorch for that version of Python.\\nThis means that LLM plugins that depend on PyTorch such as llm-sentence-transformers may not install cleanly with\\nthe Homebrew version of LLM.\\nYou can workaround this by manually installing PyTorch before installingllm-sentence-transformers:\\nllm install llm-python\\nllm python -m pip install \\\\\\n--pre torch torchvision \\\\\\n--index-url https://download.pytorch.org/whl/nightly/cpu\\nllm install llm-sentence-transformers\\nThis should produce a working installation of that plugin.\\n2.1.5 Installing plugins\\nPlugins can be used to add support for other language models, including models that can run on your own device.\\nFor example, the llm-gpt4all plugin adds support for 17 new models that can be installed on your own machine. You\\ncan install that like so:\\nllm install llm-gpt4all\\n6 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 12, 'page_label': '7'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.1.6 API key management\\nManyLLMmodelsrequireanAPIkey. TheseAPIkeyscanbeprovidedtothistoolusingseveraldifferentmechanisms.\\nYou can obtain an API key for OpenAI’s language models from the API keys page on their site.\\nSaving and using stored keys\\nThe easiest way to store an API key is to use thellm keys setcommand:\\nllm keys set openai\\nYou will be prompted to enter the key like this:\\n% llm keys set openai\\nEnter key:\\nOnce stored, this key will be automatically used for subsequent calls to the API:\\nllm \"Five ludicrous names for a pet lobster\"\\nYou can list the names of keys that have been set using this command:\\nllm keys\\nKeysthatarestoredinthiswayliveinafilecalled keys.json. Thisfileislocatedatthepathshownwhenyourunthe\\nfollowing command:\\nllm keys path\\nOnmacOSthiswillbe ~/Library/Application Support/io.datasette.llm/keys.json. OnLinuxitmaybe\\nsomething like~/.config/io.datasette.llm/keys.json.\\nPassing keys using the –key option\\nKeys can be passed directly using the--keyoption, like this:\\nllm \"Five names for pet weasels\" --key sk-my-key-goes-here\\nYoucanalsopass thealiasofakeystored inthe keys.jsonfile. Forexample, ifyouwanttomaintainapersonal API\\nkey you could add that like this:\\nllm keys set personal\\nAnd then use it for prompts like so:\\nllm \"Five friendly names for a pet skunk\" --key personal\\n2.1. Setup 7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 13, 'page_label': '8'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nKeys in environment variables\\nKeys can also be set using an environment variable. These are different for different models.\\nFor OpenAI models the key will be read from theOPENAI_API_KEYenvironment variable.\\nThe environment variable will be used if no--keyoption is passed to the command and there is not a key configured\\nin keys.json\\nTo use an environment variable in place of thekeys.jsonkey run the prompt like this:\\nllm 'my prompt' --key $OPENAI_API_KEY\\n2.1.7 Configuration\\nYou can configure LLM in a number of different ways.\\nSetting a custom default model\\nThe model used when callingllm without the-m/--model option defaults togpt-4o-mini - the fastest and least\\nexpensive OpenAI model.\\nYou can use thellm models default command to set a different default model. For GPT-4o (slower and more\\nexpensive, but more capable) run this:\\nllm models default gpt-4o\\nYou can view the current model by running this:\\nllm models default\\nAny of the supported aliases for a model can be passed to this command.\\nSetting a custom directory location\\nThis tool stores various files - prompt templates, stored keys, preferences, a database of logs - in a directory on your\\ncomputer.\\nOn macOS this is~/Library/Application Support/io.datasette.llm/.\\nOn Linux it may be something like~/.config/io.datasette.llm/.\\nYou can set a custom location for this directory by setting theLLM_USER_PATHenvironment variable:\\nexport LLM_USER_PATH=/path/to/my/custom/directory\\n8 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 14, 'page_label': '9'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nTurning SQLite logging on and off\\nBy default, LLM will log every prompt and response you make to a SQLite database - seeLogging to SQLitefor more\\ndetails.\\nYou can turn this behavior off by default by running:\\nllm logs off\\nOr turn it back on again with:\\nllm logs on\\nRunllm logs statusto see the current states of the setting.\\n2.2 Usage\\nThe command to run a prompt isllm prompt 'your prompt'. This is the default command, so you can usellm\\n'your prompt' as a shortcut.\\n2.2.1 Executing a prompt\\nThese examples use the default OpenAIgpt-4o-minimodel, which requires you to firstset an OpenAI API key.\\nYoucan installLLMplugins tousemodelsfromotherproviders,includingopenlylicensedmodelsyoucanrundirectly\\non your own computer.\\nTo run a prompt, streaming tokens as they come in:\\nllm 'Ten names for cheesecakes'\\nTo disable streaming and only return the response once it has completed:\\nllm 'Ten names for cheesecakes' --no-stream\\nTo switch from ChatGPT 4o-mini (the default) to GPT-4o:\\nllm 'Ten names for cheesecakes' -m gpt-4o\\nYou can use-m 4oas an even shorter shortcut.\\nPass--model <model name>to use a different model. Runllm modelsto see a list of available models.\\nOrifyouknowthenameistoolongtotype, use -qonceormoretoprovidesearchterms-themodelwiththeshortest\\nmodel ID that matches all of those terms (as a lowercase substring) will be used:\\nllm 'Ten names for cheesecakes' -q 4o -q mini\\nTo change the default model for the current session, set theLLM_MODELenvironment variable:\\nexport LLM_MODEL=gpt-4.1-mini\\nllm 'Ten names for cheesecakes' # Uses gpt-4.1-mini\\nYou can send a prompt directly to standard input like this:\\n2.2. Usage 9\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 15, 'page_label': '10'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\necho \\'Ten names for cheesecakes\\' | llm\\nIfyousendtexttostandardinputandprovidearguments,theresultingpromptwillconsistofthepipedcontentfollowed\\nby the arguments:\\ncat myscript.py | llm \\'explain this code\\'\\nWill run a prompt of:\\n<contents of myscript.py> explain this code\\nFor models that support them,system promptsare a better tool for this kind of prompting.\\nModel options\\nSomemodelssupportoptions. Youcanpasstheseusing -o/--option name value-forexample, tosetthetemper-\\nature to 1.5 run this:\\nllm \\'Ten names for cheesecakes\\' -o temperature 1.5\\nUse thellm models --optionscommand to see which options are supported by each model.\\nYou can alsoconfigure default optionsfor a model using thellm models optionscommands.\\nAttachments\\nSome models are multi-modal, which means they can accept input in more than just text. GPT-4o and GPT-4o mini\\ncan accept images, and models such as Google Gemini 1.5 can accept audio and video as well.\\nLLM calls theseattachments. You can pass attachments using the-aoption like this:\\nllm \"describe this image\" -a https://static.simonwillison.net/static/2024/pelicans.jpg\\nAttachments can be passed using URLs or file paths, and you can attach more than one attachment to a single prompt:\\nllm \"extract text\" -a image1.jpg -a image2.jpg\\nYou can also pipe an attachment to LLM by using-as the filename:\\ncat image.jpg | llm \"describe this image\" -a -\\nLLM will attempt to automatically detect the content type of the image. If this doesn’t work you can instead use the\\n--attachment-typeoption (--atfor short) which takes the URL/path plus an explicit content type:\\ncat myfile | llm \"describe this image\" --at - image/jpeg\\n10 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 16, 'page_label': '11'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nSystem prompts\\nYou can use-s/--system \\'...\\' to set a system prompt.\\nllm \\'SQL to calculate total sales by month\\' \\\\\\n--system \\'You are an exaggerated sentient cheesecake that knows SQL and talks about␣\\n˓→cheesecake a lot\\'\\nThis is useful for piping content to standard input, for example:\\ncurl -s \\'https://simonwillison.net/2023/May/15/per-interpreter-gils/\\' | \\\\\\nllm -s \\'Suggest topics for this post as a JSON array\\'\\nOr to generate a description of changes made to a Git repository since the last commit:\\ngit diff | llm -s \\'Describe these changes\\'\\nDifferent models support system prompts in different ways.\\nTheOpenAImodelsareparticularlygoodatusingsystempromptsasinstructionsforhowtheyshouldprocessadditional\\ninput sent as part of the regular prompt.\\nOther models might use system prompts change the default voice and attitude of the model.\\nSystem prompts can be saved astemplates to create reusable tools. For example, you can create a template called\\npytestlike this:\\nllm -s \\'write pytest tests for this code\\' --save pytest\\nAnd then use the new template like this:\\ncat llm/utils.py | llm -t pytest\\nSee prompt templatesfor more.\\nTools\\nManymodelssupporttheabilitytocall externaltools. Toolscanbeprovided byplugins oryoucanpassa --functions\\nCODEoption to LLM to define one or more Python functions that the model can then call.\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\'\\nAdd --td/--tools-debug to see full details of the tools that are being executed. You can also set the\\nLLM_TOOLS_DEBUGenvironment variable to1to enable this for all prompts.\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\' --td\\nOutput:\\n2.2. Usage 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 17, 'page_label': '12'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nTool call: multiply({\\'x\\': 34234, \\'y\\': 213345})\\n7303652730\\n34234 multiplied by 213345 is 7,303,652,730.\\nOr add--ta/--tools-approveto approve each tool call interactively before it is executed:\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\' --ta\\nOutput:\\nTool call: multiply({\\'x\\': 34234, \\'y\\': 213345})\\nApprove tool call? [y/N]:\\nThe --functions option can be passed more than once, and can also point to the filename of a.py file containing\\none or more functions.\\nIf you have any tools that have been made available via plugins you can add them to the prompt using--tool/-T\\noption. For example, using llm-tools-simpleeval like this:\\nllm install llm-tools-simpleeval\\nllm --tool simple_eval \"4444 * 233423\" --td\\nRun this command to see a list of available tools from plugins:\\nllm tools\\nIf you run a prompt that uses tools from plugins (as opposed to tools provided using the--functions option) con-\\ntinuing that conversation usingllm -c will reuse the tools from the first prompt. Runningllm chat -c will start a\\nchat that continues using those same tools. For example:\\nllm -T simple_eval \"12345 * 12345\" --td\\nTool call: simple_eval({\\'expression\\': \\'12345 * 12345\\'})\\n152399025\\n12345 multiplied by 12345 equals 152,399,025.\\nllm -c \"that * 6\" --td\\nTool call: simple_eval({\\'expression\\': \\'152399025 * 6\\'})\\n914394150\\n152,399,025 multiplied by 6 equals 914,394,150.\\nllm chat -c --td\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\n> / 123\\nTool call: simple_eval({\\'expression\\': \\'914394150 / 123\\'})\\n7434098.780487805\\n914,394,150 divided by 123 is approximately 7,434,098.78.\\nSome tools are bundled in a configurable collection of tools called atoolbox. This means a single--tooloption can\\nload multiple related tools.\\nllm-tools-datasette is one example. Using a toolbox looks like this:\\n12 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 18, 'page_label': '13'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm install llm-tools-datasette\\nllm -T \\'Datasette(\"https://datasette.io/content\")\\' \"Show tables\" --td\\nToolboxesalwaysstartwithacapitalletter. Theycanbeconfiguredbypassingatoolspecification,whichshouldfitthe\\nfollowing patterns:\\n• Empty: ToolboxNameor ToolboxName()- has no configuration arguments\\n• JSON object:ToolboxName({\"key\": \"value\", \"other\": 42})\\n• Single JSON value:ToolboxName(\"hello\")or ToolboxName([1,2,3])\\n• Key-value pairs:ToolboxName(name=\"test\", count=5, items=[1,2]) - treated the same as{\"name\":\\n\"test\", \"count\": 5, \"items\": [1, 2]} , all values must be valid JSON\\nToolboxes are not currently supported with thellm -coption, but they work well withllm chat. Try chatting with\\nthe Datasette content database like this:\\nllm chat -T \\'Datasette(\"https://datasette.io/content\")\\' --td\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\n...\\n> show tables\\nExtracting fenced code blocks\\nIf you are using an LLM to generate code it can be useful to retrieve just the code it produces without any of the\\nsurrounding explanatory text.\\nThe -x/--extractoption will scan the response for the first instance of a Markdown fenced code block - something\\nthat looks like this:\\n```python\\ndef my_function():\\n# ...\\n```\\nIt will extract and returns just the content of that block, excluding the fenced coded delimiters. If there are no fenced\\ncode blocks it will return the full response.\\nUse --xl/--extract-lastto return the last fenced code block instead of the first.\\nThe entire response including explanatory text is still logged to the database, and can be viewed usingllm logs -c.\\nSchemas\\nSome models include the ability to return JSON that matches a provided JSON schema. Models from OpenAI, An-\\nthropic and Google Gemini all include this capability.\\nTake a look at theschemas documentationfor a detailed guide to using this feature.\\nYou can pass JSON schemas directly to the--schemaoption:\\n2.2. Usage 13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 19, 'page_label': '14'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm --schema \\'{\\n\"type\": \"object\",\\n\"properties\": {\\n\"dogs\": {\\n\"type\": \"array\",\\n\"items\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"bio\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n}\\n}\\n}\\' -m gpt-4o-mini \\'invent two dogs\\'\\nOr use LLM’s customconcise schema syntaxlike this:\\nllm --schema \\'name,bio\\' \\'invent a dog\\'\\nTwo use the same concise schema for multiple items use--schema-multi:\\nllm --schema-multi \\'name,bio\\' \\'invent two dogs\\'\\nYou can also save the JSON schema to a file and reference the filename using--schema:\\nllm --schema dogs.schema.json \\'invent two dogs\\'\\nOr save your schemato a templatelike this:\\nllm --schema dogs.schema.json --save dogs\\n# Then to use it:\\nllm -t dogs \\'invent two dogs\\'\\nBe warned that different models may support different dialects of the JSON schema specification.\\nSee Browsing logged JSON objects created using schemasfor tips on using thellm logs --schema Xcommand to\\naccess JSON objects you have previously logged using this option.\\nFragments\\nYoucanusethe -f/--fragmentoptiontoreferencefragmentsofcontextthatyouwouldliketoloadintoyourprompt.\\nFragments can be specified as URLs, file paths or as aliases to previously saved fragments.\\nFragmentsaredesignedforrunninglongerprompts. LLM storespromptsinadatabase ,andthesamepromptrepeated\\nmanytimescanendupstoredasmultiplecopies,wastingdiskspace. Afragmentwillbestoredjustonceandreferenced\\nby all of the prompts that use it.\\nThe -f option can accept a path to a file on disk, a URL or the hash or alias of a previous fragment.\\nFor example, to ask a question about therobots.txtfile onllm.datasette.io:\\n14 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 20, 'page_label': '15'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm -f https://llm.datasette.io/robots.txt 'explain this'\\nFor a poem inspired by some Python code on disk:\\nllm -f cli.py 'a short snappy poem inspired by this code'\\nYou can use as many-f options as you like - the fragments will be concatenated together in the order you provided,\\nwith any additional prompt added at the end.\\nFragments can also be used for the system prompt using the--sf/--system-fragment option. If you have a file\\ncalled explain_code.txtcontaining this:\\nExplain this code in detail. Include copies of the code quoted in the explanation.\\nYou can run it as the system prompt like this:\\nllm -f cli.py --sf explain_code.txt\\nYou can use thellm fragments setcommand to load a fragment and give it an alias for use in future queries:\\nllm fragments set cli cli.py\\n# Then\\nllm -f cli 'explain this code'\\nUse llm fragmentsto list all fragments that have been stored:\\nllm fragments\\nYoucansearchbypassingoneormore -q Xsearchstrings. Thiswillreturnresultsmatchingallofthosestrings,across\\nthe source, hash, aliases and content:\\nllm fragments -q pytest -q asyncio\\nThe llm fragments remove command removes an alias. It does not delete the fragment record itself as those are\\nlinked to previous prompts and responses and cannot be deleted independently of them.\\nllm fragments remove cli\\nContinuing a conversation\\nBy default, the tool will start a new conversation each time you run it.\\nYou can opt to continue the previous conversation by passing the-c/--continueoption:\\nllm 'More names' -c\\nThis will re-send the prompts and responses for the previous conversation as part of the call to the language model.\\nNote that this can add up quickly in terms of tokens, especially if you are using expensive models.\\n--continue will automatically use the same model as the conversation that you are continuing, even if you omit the\\n-m/--modeloption.\\nTo continue a conversation that is not the most recent one, use the--cid/--conversation <id>option:\\nllm 'More names' --cid 01h53zma5txeby33t1kbe3xk8q\\nYou can find these conversation IDs using thellm logscommand.\\n2.2. Usage 15\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 21, 'page_label': '16'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nTips for using LLM with Bash or Zsh\\nTo learn more about your computer’s operating system based on the output ofuname -a, run this:\\nllm \"Tell me about my operating system: $(uname -a)\"\\nThis pattern of using$(command)inside a double quoted string is a useful way to quickly assemble prompts.\\nCompletion prompts\\nSome models are completion models - rather than being tuned to respond to chat style prompts, they are designed to\\ncomplete a sentence or paragraph.\\nAn example of this is thegpt-3.5-turbo-instructOpenAI model.\\nYou can prompt that model the same way as the chat models, but be aware that the prompt format that works best is\\nlikely to differ.\\nllm -m gpt-3.5-turbo-instruct \\'Reasons to tame a wild beaver:\\'\\n2.2.2 Starting an interactive chat\\nThe llm chatcommand starts an ongoing interactive chat with a model.\\nThis is particularly useful for models that run on your own machine, since it saves them from having to be loaded into\\nmemory each time a new prompt is added to a conversation.\\nRunllm chat, optionally with a-m model_id, to start a chat conversation:\\nllm chat -m chatgpt\\nEach chat starts a new conversation. A record of each conversation can be accessed throughthe logs.\\nYou can pass-c to start a conversation as a continuation of your most recent prompt. This will automatically use the\\nmost recently used model:\\nllm chat -c\\nFor models that support them, you can pass options using-o/--option:\\nllm chat -m gpt-4 -o temperature 0.5\\nYou can pass a system prompt to be used for your chat conversation:\\nllm chat -m gpt-4 -s \\'You are a sentient cheesecake\\'\\nYou can also passa template- useful for creating chat personas that you wish to return to.\\nHere’s how to create a template for your GPT-4 powered cheesecake:\\nllm --system \\'You are a sentient cheesecake\\' -m gpt-4 --save cheesecake\\nNow you can start a new chat with your cheesecake any time you like using this:\\nllm chat -t cheesecake\\n16 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 22, 'page_label': '17'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> who are you?\\nI am a sentient cheesecake, meaning I am an artificial\\nintelligence embodied in a dessert form, specifically a\\ncheesecake. However, I don\\'t consume or prepare foods\\nlike humans do, I communicate, learn and help answer\\nyour queries.\\nTypequitor exitfollowed by<enter>to end a chat session.\\nSometimes you may want to paste multiple lines of text into a chat at once - for example when debugging an error\\nmessage.\\nTo do that, type!multito start a multi-line input. Type or paste your text, then type!endand hit<enter>to finish.\\nIfyourpastedtextmightitselfcontaina !endline,youcansetacustomdelimiterusing !multi abcfollowedby !end\\nabcat the end:\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !multi custom-end\\nExplain this error:\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1391, in https_open\\nreturn self.do_open(http.client.HTTPSConnection, req,\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1351, in do_open\\nraise URLError(err)\\nurllib.error.URLError: <urlopen error [Errno 8] nodename nor servname provided, or not␣\\n˓→known>\\n!end custom-end\\nYou can also use!editto open your default editor and modify the prompt before sending it to the model.\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !edit\\nllm chattakesthesame --tool/-Tand--functionsoptionsas llm prompt. Youcanusethistostartachatwith\\nthe specifiedtoolsenabled.\\n2.2. Usage 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 23, 'page_label': '18'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.2.3 Listing available models\\nThe llm models command lists every model that can be used with LLM, along with their aliases. This includes\\nmodels that have been installed usingplugins.\\nllm models\\nExample output:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nGeminiPro: gemini-1.5-pro-002\\nGeminiPro: gemini-1.5-flash-002\\n...\\nAdd one or more-q termoptions to search for models matching all of those search terms:\\nllm models -q gpt-4o\\nllm models -q 4o -q mini\\nUse one or more-moptions to indicate specific models, either by their model ID or one of their aliases:\\nllm models -m gpt-4o -m gemini-1.5-pro-002\\nAdd--optionsto also see documentation for the options supported by each model:\\nllm models --options\\nOutput:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOptions:\\ntemperature: float\\nWhat sampling temperature to use, between 0 and 2. Higher values like\\n0.8 will make the output more random, while lower values like 0.2 will\\nmake it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,\\nwhere the model considers the results of the tokens with top_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but not both.\\nfrequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non their existing frequency in the text so far, decreasing the model's\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non whether they appear in the text so far, increasing the model's\\nlikelihood to talk about new topics.\\n(continues on next page)\\n18 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 24, 'page_label': '19'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing in the completion.\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\njson_object: boolean\\nOutput a valid JSON object {...}. Prompt must mention JSON.\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n(continues on next page)\\n2.2. Usage 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 25, 'page_label': '20'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\n(continues on next page)\\n20 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 26, 'page_label': '21'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOptions:\\n(continues on next page)\\n2.2. Usage 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 27, 'page_label': '22'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\n(continues on next page)\\n22 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 28, 'page_label': '23'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\n(continues on next page)\\n2.2. Usage 23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 29, 'page_label': '24'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-1106-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-0125-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\n(continues on next page)\\n24 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 30, 'page_label': '25'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.5-preview-2025-02-27\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\n(continues on next page)\\n2.2. Usage 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 31, 'page_label': '26'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\n(continues on next page)\\n26 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 32, 'page_label': '27'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nOpenAI Chat: o1-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\n(continues on next page)\\n2.2. Usage 27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 33, 'page_label': '28'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o3-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o3\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o4-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\n(continues on next page)\\n28 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 34, 'page_label': '29'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nOptions:\\ntemperature: float\\nWhat sampling temperature to use, between 0 and 2. Higher values like\\n0.8 will make the output more random, while lower values like 0.2 will\\nmake it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,\\nwhere the model considers the results of the tokens with top_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but not both.\\nfrequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non their existing frequency in the text so far, decreasing the model\\'s\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non whether they appear in the text so far, increasing the model\\'s\\nlikelihood to talk about new topics.\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing in the completion.\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\nlogprobs: int\\nInclude the log probabilities of most likely N per token\\nFeatures:\\n- streaming\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\n2.2. Usage 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 35, 'page_label': '30'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nWhen running a prompt you can pass the full model name or any of the aliases to the-m/--modeloption:\\nllm -m 4o \\\\\\n'As many names for cheesecakes as you can think of, with detailed descriptions'\\n2.2.4 Setting default options for models\\nTo configure a default option for a specific model, use thellm models options setcommand:\\nllm models options set gpt-4o temperature 0.5\\nThis option will then be applied automatically any time you run a prompt through thegpt-4omodel.\\nDefault options are stored in themodel_options.jsonfile in the LLM configuration directory.\\nYou can list all default options across all models using thellm models options listcommand:\\nllm models options list\\nOr show them for an individual model withllm models options show <model_id>:\\nllm models options show gpt-4o\\nTo clear a default option, use thellm models options clearcommand:\\nllm models options clear gpt-4o temperature\\nOr clear all default options for a model like this:\\nllm models options clear gpt-4o\\nDefaultmodel optionsare respectedby boththe llm promptandthe llm chatcommands. Theywill notbe applied\\nwhen you use LLM as aPython library.\\n2.3 OpenAI models\\nLLM ships with a default plugin for talking to OpenAI’s API. OpenAI offer both language models and embedding\\nmodels, and LLM can access both types.\\n2.3.1 Configuration\\nAll OpenAI models are accessed using an API key. You can obtain one from the API keys page on their site.\\nOnce you have created a key, configure LLM to use it by running:\\nllm keys set openai\\nThen paste in the API key.\\n30 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 36, 'page_label': '31'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.3.2 OpenAI language models\\nRunllm modelsfor a full list of available models. The OpenAI models supported by LLM are:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: gpt-4o-audio-preview\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOpenAI Chat: gpt-4.5-preview-2025-02-27\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOpenAI Chat: o1\\nOpenAI Chat: o1-2024-12-17\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nOpenAI Chat: o3-mini\\nOpenAI Chat: o3\\nOpenAI Chat: o4-mini\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nSee the OpenAI models documentation for details of each of these.\\ngpt-4o-mini(aliased to4o-mini) is the least expensive model, and is the default for if you don’t specify a model at\\nall. Consult OpenAI’s model documentation for details of the other models.\\no1-pro is not available through the Chat Completions API used by LLM’s default OpenAI plugin. You can install the\\nnew llm-openai-plugin plugin to access that model.\\n2.3.3 Model features\\nThe following features work with OpenAI models:\\n• System promptscan be used to provide instructions that have a higher weight than the prompt itself.\\n• Attachments. Many OpenAI models support image inputs - check which ones usingllm models --options.\\nAny model that accepts images can also accept PDFs.\\n• Schemascan be used to influence the JSON structure of the model output.\\n• Model optionscan be used to set parameters liketemperature. Usellm models --optionsfor a full list of\\nsupported options.\\n2.3. OpenAI models 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 37, 'page_label': '32'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.3.4 OpenAI embedding models\\nRunllm embed-modelsfor a list ofembedding models. The following OpenAI embedding models are supported by\\nLLM:\\nada-002 (aliases: ada, oai)\\n3-small\\n3-large\\n3-small-512\\n3-large-256\\n3-large-1024\\nThe3-smallmodeliscurrentlythemostinexpensive. 3-largecostsmorebutismorecapable-seeNewembedding\\nmodels and API updates on the OpenAI blog for details and benchmarks.\\nAn important characteristic of any embedding model is the size of the vector it returns. Smaller vectors cost less to\\nstore and query, but may be less accurate.\\nOpenAI3-smalland3-largevectorscanbesafelytruncatedtolowerdimensionswithoutlosingtoomuchaccuracy.\\nThe -int models provided by LLM are pre-configured to do this, so3-large-256 is the3-large model truncated\\nto 256 dimensions.\\nThe vector size of the supported OpenAI embedding models are as follows:\\nModel Size\\nada-002 1536\\n3-small 1536\\n3-large 3072\\n3-small-512 512\\n3-large-256 256\\n3-large-1024 1024\\n2.3.5 OpenAI completion models\\nThegpt-3.5-turbo-instructmodelisalittledifferent-itisacompletionmodelratherthanachatmodel,described\\nin the OpenAI completions documentation.\\nCompletion models can be called with the-o logprobs 3 option (not supported by chat models) which will cause\\nLLM to store 3 log probabilities for each returned token in the SQLite database. Consult this issue for details on how\\nto read these values.\\n2.3.6 Adding more OpenAI models\\nOpenAI occasionally release new models with new names. LLM aims to ship new releases to support these, but you\\ncan also configure them directly, by adding them to aextra-openai-models.yamlconfiguration file.\\nRun this command to find the directory in which this file should be created:\\ndirname \"$(llm logs path)\"\\nOn my Mac laptop I get this:\\n~/Library/Application Support/io.datasette.llm\\n32 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 38, 'page_label': '33'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nCreate a file in that directory calledextra-openai-models.yaml.\\nLet’s say OpenAI have just released thegpt-3.5-turbo-0613 model and you want to use it, despite LLM not yet\\nshipping support. You could configure that by adding this to the file:\\n- model_id: gpt-3.5-turbo-0613\\nmodel_name: gpt-3.5-turbo-0613\\naliases: [\"0613\"]\\nThemodel_idistheidentifierthatwillberecordedintheLLMlogs. Youcanusethistospecifythemodel,oryoucan\\noptionallyincludealistofaliasesforthatmodel. The model_nameistheactualmodelidentifierthatwillbepassedto\\nthe API, which must match exactly what the API expects.\\nIf the model is a completion model (such asgpt-3.5-turbo-instruct) addcompletion: true to the configu-\\nration.\\nIf the model supports structured extraction using json_schema, addsupports_schema: true to the configuration.\\nFor reasoning models likeo1or o3-miniadd reasoning: true .\\nWith this configuration in place, the following command should run a prompt against the new model:\\nllm -m 0613 \\'What is the capital of France?\\'\\nRunllm modelsto confirm that the new model is now available:\\nllm models\\nExample output:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-3.5-turbo-0613 (aliases: 0613)\\nRunningllm logs -n 1should confirm that the prompt and response has been correctly logged to the database.\\n2.4 Other models\\nLLM supports OpenAI models by default. You can installplugins to add support for other models. You can also add\\nadditional OpenAI-API-compatible modelsusing a configuration file.\\n2.4.1 Installing and using a local model\\nLLM pluginscan provide local models that run on your machine.\\nTo installllm-gpt4all, providing 17 models from the GPT4All project, run this:\\nllm install llm-gpt4all\\nRunllm modelsto see the expanded list of available models.\\nTo run a prompt through one of the models from GPT4All specify it using-m/--model:\\n2.4. Other models 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 39, 'page_label': '34'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France?\\'\\nThe model will be downloaded and cached the first time you use it.\\nCheck theplugin directoryfor the latest list of available plugins for other models.\\n2.4.2 OpenAI-compatible models\\nProjectssuchasLocalAIofferaRESTAPIthatimitatestheOpenAIAPIbutcanbeusedtorunothermodels,including\\nmodels that can be installed on your own machine. These can be added using the same configuration mechanism.\\nThe model_id is the name LLM will use for the model. Themodel_name is the name which needs to be passed to\\nthe API - this might differ from themodel_id, especially if themodel_idcould potentially clash with other installed\\nmodels.\\nThe api_basekey can be used to point the OpenAI client library at a different API endpoint.\\nToaddthe orca-mini-3bmodelhostedbyalocalinstallationofLocalAI,addthistoyour extra-openai-models.\\nyamlfile:\\n- model_id: orca-openai-compat\\nmodel_name: orca-mini-3b.ggmlv3\\napi_base: \"http://localhost:8080\"\\nIf theapi_baseis set, the existing configuredopenaiAPI key will not be sent by default.\\nYou can setapi_key_nameto the name of a key stored using theAPI key managementfeature.\\nAdd completion: true if the model is a completion model that uses a /completion as opposed to a /\\ncompletion/chatendpoint.\\nIf a model does not support streaming, addcan_stream: false to disable the streaming option.\\nIf a model supports structured output via JSON schemas, you can addsupports_schema: true to support this\\nfeature.\\nIf a model is a vision model, you can addvision: true to support this feature and use image attachments.\\nIf a model is an audio model, you can addaudio: true to support this feature and use audio attachments.\\nHaving configured the model like this, runllm modelsto check that it installed correctly. You can then run prompts\\nagainst it like so:\\nllm -m orca-openai-compat \\'What is the capital of France?\\'\\nAnd confirm they were logged correctly with:\\nllm logs -n 1\\n34 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 40, 'page_label': '35'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nExtra HTTP headers\\nSome providers such as openrouter.ai may require the setting of additional HTTP headers. You can set those using the\\nheaders: key like this:\\n- model_id: claude\\nmodel_name: anthropic/claude-2\\napi_base: \"https://openrouter.ai/api/v1\"\\napi_key_name: openrouter\\nheaders:\\nHTTP-Referer: \"https://llm.datasette.io/\"\\nX-Title: LLM\\n2.5 Tools\\nMany Large Language Models have been trained to execute tools as part of responding to a prompt. LLM supports\\ntool usage with both the command-line interface and the Python API.\\nExposing tools to LLMscarries risks! Be sure to read thewarning below.\\n2.5.1 How tools work\\nA tool is effectively a function that the model can request to be executed. Here’s how that works:\\n1. The initial prompt to the model includes a list of available tools, containing their names, descriptions and pa-\\nrameters.\\n2. The model can choose to call one (or sometimes more than one) of those tools, returning a request for the tool\\nto execute.\\n3. The code that calls the model - in this case LLM itself - then executes the specified tool with the provided\\narguments.\\n4. LLM prompts the model a second time, this time including the output of the tool execution.\\n5. The model can then use that output to generate its next response.\\nThissequencecanrunseveraltimesinaloop,allowingtheLLMtoaccessdata,actonthatdataandthenpassthatdata\\noff to other tools for further processing.\\nTools can be dangerous\\nWarning: Tools can be dangerous\\nApplications built on top of LLMs suffer from a class of attacks called prompt injection attacks. These occur when a\\nmaliciousthirdpartyinjectscontentintotheLLMwhichcausesittotaketool-basedactionsthatactagainsttheinterests\\nof the user of that application.\\nBe very careful about which tools you enable when you potentially might be exposed to untrusted sources of content -\\nwebpages,GitHubissuespostedbyotherpeople,emailandmessagesthathavebeensenttoyouthatcouldcomefrom\\nan attacker.\\nWatch out for thelethal trifectaof prompt injection exfiltration attacks. If your tool-enabled LLM has the following:\\n• access to private data\\n2.5. Tools 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 41, 'page_label': '36'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n• exposure to malicious instructions\\n• the ability to exfiltrate information\\nAnyone who can feed malicious instructions into your LLM - by leaving them on a web page it visits, or sending\\nan email to an inbox that it monitors - could be able to trick your LLM into using other tools to access your private\\ninformation and then exfiltrate (pass out) that data to somewhere the attacker can see it.\\n2.5.2 Trying out tools\\nLLM comes with a default tool installed, calledllm_version. You can try that out like this:\\nllm --tool llm_version \"What version of LLM is this?\" --td\\nYou can also use-T llm_versionas a shortcut for--tool llm_version.\\nThe output should look like this:\\nTool call: llm_version({})\\n0.26a0\\nThe installed version of the LLM is 0.26a0.\\nFurther tools can be installed using plugins, or you can use thellm --functions option to pass tools implemented\\nas PYthon functions directly, asdescribed here.\\n2.5.3 LLM’s implementation of tools\\nIn LLM every tool is a defined as a Python function. The function can take any number of arguments and can return a\\nstring or an object that can be converted to a string.\\nTool functions should include a docstring that describes what the function does. This docstring will become the de-\\nscription that is passed to the model.\\nToolscanalsobedefinedas toolboxclasses,asubclassof llm.Toolboxthatallowsmultiplerelatedtoolstobebundled\\ntogether. Toolbox classes can be be configured when they are instantiated, and can also maintain state in between\\nmultiple tool calls.\\nThe Python API can accept functions directly. The command-line interface has two ways for tools to be defined: via\\nplugins that implement theregister_tools() plugin hook, or directly on the command-line using the--functions\\nargument to specify a block of Python code defining one or more functions - or a path to a Python file containing the\\nsame.\\nYou can use toolswith the LLM command-line toolorwith the Python API.\\n36 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 42, 'page_label': '37'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.5.4 Default tools\\nLLM includes some default tools for you to try out:\\n• llm_version()returns the current version of LLM\\n• llm_time()returns the current local and UTC time\\nTry them like this:\\nllm -T llm_version -T llm_time \\'Give me the current time and LLM version\\' --td\\n2.5.5 Tips for implementing tools\\nConsult theregister_tools() plugin hookdocumentation for examples of how to implement tools in plugins.\\nIfyourpluginneedsaccesstoAPIsecretsIrecommendstoringthoseusing llm keys set api-nameandthenreading\\nthem using thellm.get_key()utility function. This avoids secrets being logged to the database as part of tool calls.\\n2.6 Schemas\\nLarge Language Models are very good at producing structured output as JSON or other formats. LLM’sschemas\\nfeature allows you to define the exact structure of JSON data you want to receive from a model.\\nThis feature is supported by models from OpenAI, Anthropic, Google Gemini and can be implemented for othersvia\\nplugins.\\nThis page describes schemas used via thellmcommand-line tool. Schemas can also be used from thePython API.\\n2.6.1 Schemas tutorial\\nIn this tutorial we’re going to use schemas to analyze some news stories.\\nBut first, let’s invent some dogs!\\nGetting started with dogs\\nLLMs are great at creating test data. Let’s define a simple schema for a dog, using LLM’sconcise schema syntax.\\nWe’ll pass that to LLm withllm --schemaand prompt it to “invent a cool dog”:\\nllm --schema \\'name, age int, one_sentence_bio\\' \\'invent a cool dog\\'\\nI got back Ziggy:\\n{\\n\"name\": \"Ziggy\",\\n\"age\": 4,\\n\"one_sentence_bio\": \"Ziggy is a hyper-intelligent, bioluminescent dog who loves to␣\\n˓→perform tricks in the dark and guides his owner home using his glowing fur.\"\\n}\\n2.6. Schemas 37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 43, 'page_label': '38'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThe response matched my schema, withnameand one_sentence_biostring columns and an integer forage.\\nWe’re using the default LLM model here -gpt-4o-mini. Add-m modelto use another model - for example use-m\\no3-minito have O3 mini invent some dogs.\\nFor a list of available models that support schemas, run this command:\\nllm models --schemas\\nWant several more dogs? You can pass in that same schema using--schema-multiand ask for several at once:\\nllm --schema-multi \\'name, age int, one_sentence_bio\\' \\'invent 3 really cool dogs\\'\\nHere’s what I got:\\n{\\n\"items\": [\\n{\\n\"name\": \"Echo\",\\n\"age\": 3,\\n\"one_sentence_bio\": \"Echo is a sleek, silvery-blue Siberian Husky with mesmerizing␣\\n˓→blue eyes and a talent for mimicking sounds, making him a natural entertainer.\"\\n},\\n{\\n\"name\": \"Nova\",\\n\"age\": 2,\\n\"one_sentence_bio\": \"Nova is a vibrant, spotted Dalmatian with an adventurous␣\\n˓→spirit and a knack for agility courses, always ready to leap into action.\"\\n},\\n{\\n\"name\": \"Pixel\",\\n\"age\": 4,\\n\"one_sentence_bio\": \"Pixel is a playful, tech-savvy Poodle with a rainbow-colored␣\\n˓→coat, known for her ability to interact with smart devices and her love for puzzle␣\\n˓→toys.\"\\n}\\n]\\n}\\nSo that’s the basic idea: we can feed in a schema and LLM will pass it to the underlying model and (usually) get back\\nJSON that conforms to that schema.\\nThis stuff gets alot more useful when you start applying it to larger amounts of text, extracting structured details from\\nunstructured content.\\nExtracting people from a news articles\\nWearegoingtoextractdetailsofthepeoplewhoarementionedindifferentnewsstories,andthenusethosetocompile\\na database.\\nLet’s start by compiling a schema. For each person mentioned we want to extract the following details:\\n• Their name\\n• The organization they work for\\n• Their role\\n38 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 44, 'page_label': '39'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n• What we learned about them from the story\\nWe will also record the article headline and the publication date, to make things easier for us later on.\\nUsing LLM’s custom, concise schema language, this time with newlines separating the individual fields (for the dogs\\nexample we used commas):\\nname: the person\\'s name\\norganization: who they represent\\nrole: their job title or role\\nlearned: what we learned about them from this story\\narticle_headline: the headline of the story\\narticle_date: the publication date in YYYY-MM-DD\\nAs you can see, this schema definition is pretty simple - each line has the name of a property we want to capture, then\\nan optional: followed by a description, which doubles as instructions for the model.\\nThe full syntax isdescribed below- you can also include type information for things like numbers.\\nLet’s run this against a news article.\\nVisit AP News and grab the URL to an article. I’m using this one:\\nhttps://apnews.com/article/trump-federal-employees-firings-\\n˓→a85d1aaf1088e050d39dcf7e3664bb9f\\nThere’s quite a lot of HTML on that page, possibly even enough to exceed GPT-4o mini’s 128,000 token input limit.\\nWe’ll use another tool called strip-tags to reduce that. If you have uv installed you can call it usinguvx strip-tags,\\notherwise you’ll need to install it first:\\nuv tool install strip-tags\\n# Or \"pip install\" or \"pipx install\"\\nNow we can run this command to extract the people from that article:\\ncurl \\'https://apnews.com/article/trump-federal-employees-firings-\\n˓→a85d1aaf1088e050d39dcf7e3664bb9f\\' | \\\\\\nuvx strip-tags | \\\\\\nllm --schema-multi \"\\nname: the person\\'s name\\norganization: who they represent\\nrole: their job title or role\\nlearned: what we learned about them from this story\\narticle_headline: the headline of the story\\narticle_date: the publication date in YYYY-MM-DD\\n\" --system \\'extract people mentioned in this article\\'\\nThe output I got started like this:\\n{\\n\"items\": [\\n{\\n\"name\": \"William Alsup\",\\n\"organization\": \"U.S. District Court\",\\n\"role\": \"Judge\",\\n\"learned\": \"He ruled that the mass firings of probationary employees were likely␣\\n˓→unlawful and criticized the authority exercised by the Office of Personnel Management.\\n(continues on next page)\\n2.6. Schemas 39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 45, 'page_label': '40'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n˓→\",\\n\"article_headline\": \"Judge finds mass firings of federal probationary workers were␣\\n˓→likely unlawful\",\\n\"article_date\": \"2025-02-26\"\\n},\\n{\\n\"name\": \"Everett Kelley\",\\n\"organization\": \"American Federation of Government Employees\",\\n\"role\": \"National President\",\\n\"learned\": \"He hailed the court\\'s decision as a victory for employees who were␣\\n˓→illegally fired.\",\\n\"article_headline\": \"Judge finds mass firings of federal probationary workers were␣\\n˓→likely unlawful\",\\n\"article_date\": \"2025-02-26\"\\n}\\nThis data has been logged to LLM’sSQLite database. We can retrieve the data back out again using thellm logs\\ncommand like this:\\nllm logs -c --data\\nThe -cflag means “use most recent conversation”, and the--dataflag outputs just the JSON data that was captured\\nin the response.\\nWe’re going to want to use the same schema for other things. Schemas that we use are automatically logged to the\\ndatabase - we can view them usingllm schemas:\\nllm schemas\\nHere’s the output:\\n- id: 3b7702e71da3dd791d9e17b76c88730e\\nsummary: |\\n{items: [{name, organization, role, learned, article_headline, article_date}]}\\nusage: |\\n1 time, most recently 2025-02-28T04:50:02.032081+00:00\\nTo view the full schema, run that command with--full:\\nllm schemas --full\\nWhich outputs:\\n- id: 3b7702e71da3dd791d9e17b76c88730e\\nschema: |\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"items\": {\\n\"type\": \"array\",\\n\"items\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n(continues on next page)\\n40 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 46, 'page_label': '41'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"type\": \"string\",\\n\"description\": \"the person\\'s name\"\\n},\\n...\\nThat 3b7702e71da3dd791d9e17b76c88730eID can be used to run the same schema again. Let’s try that now on a\\ndifferent URL:\\ncurl \\'https://apnews.com/article/bezos-katy-perry-blue-origin-launch-\\n˓→4a074e534baa664abfa6538159c12987\\' | \\\\\\nuvx strip-tags | \\\\\\nllm --schema 3b7702e71da3dd791d9e17b76c88730e \\\\\\n--system \\'extract people mentioned in this article\\'\\nHere we are using--schemabecause our schema ID already corresponds to an array of items.\\nThe result starts like this:\\n{\\n\"items\": [\\n{\\n\"name\": \"Katy Perry\",\\n\"organization\": \"Blue Origin\",\\n\"role\": \"Singer\",\\n\"learned\": \"Katy Perry will join the all-female celebrity crew for a spaceflight␣\\n˓→organized by Blue Origin.\",\\n\"article_headline\": \"Katy Perry and Gayle King will join Jeff Bezos’ fiancee␣\\n˓→Lauren Sanchez on Blue Origin spaceflight\",\\n\"article_date\": \"2023-10-15\"\\n},\\nOne more trick: let’s turn our schema and system prompt combination into atemplate.\\nllm --schema 3b7702e71da3dd791d9e17b76c88730e \\\\\\n--system \\'extract people mentioned in this article\\' \\\\\\n--save people\\nThis creates a new template called “people”. We can confirm the template was created correctly using:\\nllm templates show people\\nWhich will output the YAML version of the template looking like this:\\nname: people\\nschema_object:\\nproperties:\\nitems:\\nitems:\\nproperties:\\narticle_date:\\ndescription: the publication date in YYYY-MM-DD\\ntype: string\\narticle_headline:\\ndescription: the headline of the story\\n(continues on next page)\\n2.6. Schemas 41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 47, 'page_label': '42'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntype: string\\nlearned:\\ndescription: what we learned about them from this story\\ntype: string\\nname:\\ndescription: the person\\'s name\\ntype: string\\norganization:\\ndescription: who they represent\\ntype: string\\nrole:\\ndescription: their job title or role\\ntype: string\\nrequired:\\n- name\\n- organization\\n- role\\n- learned\\n- article_headline\\n- article_date\\ntype: object\\ntype: array\\nrequired:\\n- items\\ntype: object\\nsystem: extract people mentioned in this article\\nWe can now run our people extractor against another fresh URL. Let’s use one from The Guardian:\\ncurl https://www.theguardian.com/commentisfree/2025/feb/27/billy-mcfarland-new-fyre-\\n˓→festival-fantasist | \\\\\\nstrip-tags | llm -t people\\nStoring the schema in a template means we can just usellm -t peopleto run the prompt. Here’s what I got back:\\n{\\n\"items\": [\\n{\\n\"name\": \"Billy McFarland\",\\n\"organization\": \"Fyre Festival\",\\n\"role\": \"Organiser\",\\n\"learned\": \"Billy McFarland is known for organizing the infamous Fyre Festival and␣\\n˓→was sentenced to six years in prison for wire fraud related to it. He is attempting to␣\\n˓→revive the festival with Fyre 2.\",\\n\"article_headline\": \"Welcome back Billy McFarland and a new Fyre festival. Shows␣\\n˓→you can’t keep a good fantasist down\",\\n\"article_date\": \"2025-02-27\"\\n}\\n]\\n}\\nDepending on the model, schema extraction may work against images and PDF files as well.\\nI took a screenshot of part of this story in the Onion and saved it to the following URL:\\n42 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 48, 'page_label': '43'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nhttps://static.simonwillison.net/static/2025/onion-zuck.jpg\\nWe can pass that as anattachment using the-aoption. This time let’s use GPT-4o:\\nllm -t people -a https://static.simonwillison.net/static/2025/onion-zuck.jpg -m gpt-4o\\nWhich gave me back this:\\n{\\n\"items\": [\\n{\\n\"name\": \"Mark Zuckerberg\",\\n\"organization\": \"Facebook\",\\n\"role\": \"CEO\",\\n\"learned\": \"He addressed criticism by suggesting anyone with similar values and␣\\n˓→thirst for power could make the same mistakes.\",\\n\"article_headline\": \"Mark Zuckerberg Insists Anyone With Same Skewed Values And␣\\n˓→Unrelenting Thirst For Power Could Have Made Same Mistakes\",\\n\"article_date\": \"2018-06-14\"\\n}\\n]\\n}\\nNow that we’ve extracted people from a number of different sources, let’s load them into a database.\\nThellmlogs commandhasseveralfeaturesforworkingwithloggedJSONobjects. Sincewe’vebeenrecordingmultiple\\nobjects from each page in an\"items\" array using ourpeople template we can access those using the following\\ncommand:\\nllm logs --schema t:people --data-key items\\nIn place oft:people we could use the3b7702e71da3dd791d9e17b76c88730e schema ID or even the original\\nschema string instead, seespecifying a schema.\\nThis command outputs newline-delimited JSON for every item that has been captured using the specified schema:\\n{\"name\": \"Katy Perry\", \"organization\": \"Blue Origin\", \"role\": \"Singer\", \"learned\": \"She␣\\n˓→is one of the passengers on the upcoming spaceflight with Blue Origin.\"}\\n{\"name\": \"Gayle King\", \"organization\": \"Blue Origin\", \"role\": \"TV Journalist\", \"learned\\n˓→\": \"She is participating in the upcoming Blue Origin spaceflight.\"}\\n{\"name\": \"Lauren Sanchez\", \"organization\": \"Blue Origin\", \"role\": \"Helicopter Pilot and␣\\n˓→former TV Journalist\", \"learned\": \"She selected the crew for the Blue Origin␣\\n˓→spaceflight.\"}\\n{\"name\": \"Aisha Bowe\", \"organization\": \"Engineering firm\", \"role\": \"Former NASA Rocket␣\\n˓→Scientist\", \"learned\": \"She is part of the crew for the spaceflight.\"}\\n{\"name\": \"Amanda Nguyen\", \"organization\": \"Research Scientist\", \"role\": \"Activist and␣\\n˓→Scientist\", \"learned\": \"She is included in the crew for the upcoming Blue Origin␣\\n˓→flight.\"}\\n{\"name\": \"Kerianne Flynn\", \"organization\": \"Movie Producer\", \"role\": \"Producer\", \"learned\\n˓→\": \"She will also be a passenger on the upcoming spaceflight.\"}\\n{\"name\": \"Billy McFarland\", \"organization\": \"Fyre Festival\", \"role\": \"Organiser\",\\n˓→\"learned\": \"He was sentenced to six years in prison for wire fraud in 2018 and has␣\\n˓→launched a new festival called Fyre 2.\", \"article_headline\": \"Welcome back Billy␣\\n˓→McFarland and a new Fyre festival. Shows you can\\\\u2019t keep a good fantasist down\",\\n(continues on next page)\\n2.6. Schemas 43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 49, 'page_label': '44'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n˓→\"article_date\": \"2025-02-27\"}\\n{\"name\": \"Mark Zuckerberg\", \"organization\": \"Facebook\", \"role\": \"CEO\", \"learned\": \"He␣\\n˓→attempted to dismiss criticism by suggesting that anyone with similar values and␣\\n˓→thirst for power could have made the same mistakes.\", \"article_headline\": \"Mark␣\\n˓→Zuckerberg Insists Anyone With Same Skewed Values And Unrelenting Thirst For Power␣\\n˓→Could Have Made Same Mistakes\", \"article_date\": \"2018-06-14\"}\\nIf we add--data-arraywe’ll get back a valid JSON array of objects instead:\\nllm logs --schema t:people --data-key items --data-array\\nOutput starts:\\n[{\"name\": \"Katy Perry\", \"organization\": \"Blue Origin\", \"role\": \"Singer\", \"learned\": \"She␣\\n˓→is one of the passengers on the upcoming spaceflight with Blue Origin.\"},\\n{\"name\": \"Gayle King\", \"organization\": \"Blue Origin\", \"role\": \"TV Journalist\", \"learned\\n˓→\": \"She is participating in the upcoming Blue Origin spaceflight.\"},\\nWe can load this into a SQLite database using sqlite-utils, in particular the sqlite-utils insert command.\\nuv tool install sqlite-utils\\n# or pip install or pipx install\\nNow we can pipe the JSON into that tool to create a database with apeopletable:\\nllm logs --schema t:people --data-key items --data-array | \\\\\\nsqlite-utils insert data.db people -\\nTo see a table of the name, organization and role columns use sqlite-utils rows:\\nsqlite-utils rows data.db people -t -c name -c organization -c role\\nWhich produces:\\nname organization role\\n--------------- ------------------ -----------------------------------------\\nKaty Perry Blue Origin Singer\\nGayle King Blue Origin TV Journalist\\nLauren Sanchez Blue Origin Helicopter Pilot and former TV Journalist\\nAisha Bowe Engineering firm Former NASA Rocket Scientist\\nAmanda Nguyen Research Scientist Activist and Scientist\\nKerianne Flynn Movie Producer Producer\\nBilly McFarland Fyre Festival Organiser\\nMark Zuckerberg Facebook CEO\\nWe can also explore the database in a web interface using Datasette:\\nuvx datasette data.db\\n# Or install datasette first:\\nuv tool install datasette # or pip install or pipx install\\ndatasette data.db\\nVisit http://127.0.0.1:8001/data/peopleto start navigating the data.\\n44 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 50, 'page_label': '45'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.6.2 Using JSON schemas\\nThe above examples have both usedconcise schema syntax. LLM converts this format to JSON schema, and you can\\nuse JSON schema directly yourself if you wish.\\nJSON schema covers the following:\\n• The data types of fields (string, number, array, object, etc.)\\n• Required vs. optional fields\\n• Nested data structures\\n• Constraints on values (minimum/maximum, patterns, etc.)\\n• Descriptions of those fields - these can be used to guide the language model\\nDifferentmodelsmaysupportdifferentsubsetsoftheoverallJSONschemalanguage. Youshouldexperimenttofigure\\nout what works for the model you are using.\\nLLMrecommendsthatthetopleveloftheschemaisanobject,notanarray,forincreasedcompatibilityacrossmultiple\\nmodels. I suggest using{\"items\": [array of objects]} if you want to return an array.\\nThe dogs schema above,name, age int, one_sentence_bio, would look like this as a full JSON schema:\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n},\\n\"one_sentence_bio\": {\\n\"type\": \"string\"\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\",\\n\"one_sentence_bio\"\\n]\\n}\\nThis JSON can be passed directly to the--schemaoption, or saved in a file and passed as the filename.\\nllm --schema \\'{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n},\\n\"one_sentence_bio\": {\\n\"type\": \"string\"\\n(continues on next page)\\n2.6. Schemas 45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 51, 'page_label': '46'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\",\\n\"one_sentence_bio\"\\n]\\n}\\' \\'a surprising dog\\'\\nExample output:\\n{\\n\"name\": \"Baxter\",\\n\"age\": 3,\\n\"one_sentence_bio\": \"Baxter is a rescue dog who learned to skateboard and now performs␣\\n˓→tricks at local parks, astonishing everyone with his skill!\"\\n}\\n2.6.3 Ways to specify a schema\\nLLMacceptsschemadefinitionsforbothrunningpromptsandexploringloggedresponses,usingthe --schemaoption.\\nThis option can take multiple forms:\\n• A string providing a JSON schema:--schema \\'{\"type\": \"object\", ...} \\'\\n• Acondensed schema definition: --schema \\'name,age int\\'\\n• The name or path of a file on disk containing a JSON schema:--schema dogs.schema.json\\n• The hexadecimal ID of a previously logged schema:--schema 520f7aabb121afd14d0c6c237b39ba2d -\\nthese IDs can be found using thellm schemascommand.\\n• A schema that has beensaved in a template: --schema t:name-of-template, seeSaving reusable schemas\\nin templates.\\n2.6.4 Concise LLM schema syntax\\nJSON schema’s can be time-consuming to construct by hand. LLM also supports a concise alternative syntax for\\nspecifying a schema.\\nA simple schema for an object with two string properties callednameand biolooks like this:\\nname, bio\\nYou can include type information by adding a type indicator after the property name, separated by a space.\\nname, bio, age int\\nSupported types areint for integers,float for floating point numbers,str for strings (the default) andbool for\\ntrue/false booleans.\\nTo include a description of the field to act as a hint to the model, add one after a colon:\\n46 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 52, 'page_label': '47'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nname: the person\\'s name, age int: their age, bio: a short bio\\nIf your schema is getting long you can switch from comma-separated to newline-separated, which also allows you to\\nuse commas in those descriptions:\\nname: the person\\'s name\\nage int: their age\\nbio: a short bio, no more than three sentences\\nYou can experiment with the syntax using thellm schemas dsl command, which converts the input into a JSON\\nschema:\\nllm schemas dsl \\'name, age int\\'\\nOutput:\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\"\\n]\\n}\\nThe Python utility functionllm.schema_dsl(schema) can be used to convert this syntax into the equivalent JSON\\nschema dictionary when working with schemasin the Python API.\\n2.6.5 Saving reusable schemas in templates\\nIf you want to store a schema with a name so you can reuse it easily in the future, the easiest way to do so is to save it\\nin a template.\\nThe quickest way to do that is with thellm --saveoption:\\nllm --schema \\'name, age int, one_sentence_bio\\' --save dog\\nNow you can use it like this:\\nllm --schema t:dog \\'invent a dog\\'\\nOr:\\nllm --schema-multi t:dog \\'invent three dogs\\'\\n2.6. Schemas 47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 53, 'page_label': '48'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.6.6 Browsing logged JSON objects created using schemas\\nBy default, all JSON produced using schemas is logged toa SQLite database. You can use special options to thellm\\nlogscommand to extract just those JSON objects in a useful format.\\nThe llm logs --schema X filter option can be used to filter just for responses that were created using the specified\\nschema. You can pass the full schema JSON, a path to the schema on disk or the schema ID.\\nThe --dataoption causes just the JSON data collected by that schema to be outputted, as newline-delimited JSON.\\nIf you instead want a JSON array of objects (with starting and ending square braces) you can use--data-array\\ninstead.\\nLet’s invent some dogs:\\nllm --schema-multi \\'name, ten_word_bio\\' \\'invent 3 cool dogs\\'\\nllm --schema-multi \\'name, ten_word_bio\\' \\'invent 2 cool dogs\\'\\nHaving logged these cool dogs, you can see just the data that was returned by those prompts like this:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data\\nWeneedtouse --schema-multiherebecauseweusedthatwhenwefirstcreatedtheserecords. The --schemaoption\\nis also supported, and can be passed a filename or JSON schema or schema ID as well.\\nOutput:\\n{\"items\": [{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"}, {\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a␣\\n˓→talent for agility and tricks.\"}]}\\n{\"items\": [{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee␣\\n˓→and outdoor adventures.\"}, {\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with␣\\n˓→mesmerizing blue eyes, enjoys snow and play.\"}, {\"name\": \"Ziggy\", \"ten_word_bio\":\\n˓→\"Quirky pug who loves belly rubs and quirky outfits.\"}]}\\nNote that the dogs are nested in that\"items\"key. To access the list of items from that key use--data-key items:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items\\nOutput:\\n{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee and␣\\n˓→outdoor adventures.\"}\\n{\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with mesmerizing blue eyes, enjoys snow␣\\n˓→and play.\"}\\n{\"name\": \"Ziggy\", \"ten_word_bio\": \"Quirky pug who loves belly rubs and quirky outfits.\"}\\n{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"}\\n{\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a talent for agility and␣\\n˓→tricks.\"}\\nFinally, to output a JSON array instead of newline-delimited JSON use--data-array:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items --data-array\\nOutput:\\n48 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 54, 'page_label': '49'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n[{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee and␣\\n˓→outdoor adventures.\"},\\n{\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with mesmerizing blue eyes, enjoys␣\\n˓→snow and play.\"},\\n{\"name\": \"Ziggy\", \"ten_word_bio\": \"Quirky pug who loves belly rubs and quirky outfits.\"}\\n˓→,\\n{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"},\\n{\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a talent for agility and␣\\n˓→tricks.\"}]\\nAdd--data-idstoinclude \"response_id\"and \"conversation_id\"fieldsineachofthereturnedobjectsreflect-\\ningthedatabaseIDsoftheresponseandconversationtheywereapartof. Thiscanbeusefulfortrackingthesourceof\\neach individual row.\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items --data-ids\\nOutput:\\n{\"name\": \"Nebula\", \"ten_word_bio\": \"A cosmic puppy with starry fur, loves adventures in␣\\n˓→space.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Echo\", \"ten_word_bio\": \"A clever hound with extraordinary hearing, master of␣\\n˓→hide-and-seek.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Biscuit\", \"ten_word_bio\": \"An adorable chef dog, bakes treats that everyone␣\\n˓→loves.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Cosmo\", \"ten_word_bio\": \"Galactic explorer, loves adventures and chasing␣\\n˓→shooting stars.\", \"response_id\": \"01jn4daycb3svj0x7kvp7zrp4q\", \"conversation_id\":\\n˓→\"01jn4daycb3svj0x7kvp7zrp4q\"}\\n{\"name\": \"Pixel\", \"ten_word_bio\": \"Tech-savvy pup, builds gadgets and loves virtual␣\\n˓→playtime.\", \"response_id\": \"01jn4daycb3svj0x7kvp7zrp4q\", \"conversation_id\":\\n˓→\"01jn4daycb3svj0x7kvp7zrp4q\"}\\nIf a row already has a property called\"conversation_id\" or \"response_id\" additional underscores will be ap-\\npended to the ID key until it no longer overlaps with the existing keys.\\nThe--id-gt $IDand --id-gte $IDoptionscan beusefulforignoring loggedschemadataprior toacertain point,\\nsee Filtering past a specific IDfor details.\\n2.7 Templates\\nAtemplatecancombineaprompt,systemprompt,model,defaultmodeloptions,schema,andfragmentsintoasingle\\nreusable unit.\\nOnly one template can be used at a time. To compose multiple shorter pieces of prompts together consider using\\nfragmentsinstead.\\n2.7. Templates 49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 55, 'page_label': '50'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.7.1 Getting started with –save\\nThe easiest way to create a template is using the--save template_nameoption.\\nHere’s how to create a template for summarizing text:\\nllm '$input - summarize this' --save summarize\\nPut $input where you would like the user’s input to be inserted. If you omit this their input will be added to the end\\nof your regular prompt:\\nllm 'Summarize the following: ' --save summarize\\nYou can also create templates using system prompts:\\nllm --system 'Summarize this' --save summarize\\nYou can set the default model for a template using--model:\\nllm --system 'Summarize this' --model gpt-4o --save summarize\\nYou can also save default options:\\nllm --system 'Speak in French' -o temperature 1.8 --save wild-french\\nIf you want to include a literal$sign in your prompt, use$$instead:\\nllm --system 'Estimate the cost in $$ of this: $input' --save estimate\\nUse --tool/-Tone or more times to add tools to the template:\\nllm -T llm_time --system 'Always include the current time in the answer' --save time\\nYou can also use--functionsto add Python function code directly to the template:\\nllm --functions 'def reverse_string(s): return s[::-1]' --system 'reverse any input' --\\n˓→save reverse\\nllm -t reverse 'Hello, world!'\\nAdd--schemato bake aschemainto your template:\\nllm --schema dog.schema.json 'invent a dog' --save dog\\nIf you add--extractthe setting toextract the first fenced code blockwill be persisted in the template.\\nllm --system 'write a Python function' --extract --save python-function\\nllm -t python-function 'calculate haversine distance between two points'\\nIn each of these cases the template will be saved in YAML format in a dedicated directory on disk.\\n50 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 56, 'page_label': '51'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.7.2 Using a template\\nYou can execute a named template using the-t/--templateoption:\\ncurl -s https://example.com/ | llm -t summarize\\nThis can be combined with the-moption to specify a different model:\\ncurl -s https://llm.datasette.io/en/latest/ | \\\\\\nllm -t summarize -m gpt-3.5-turbo-16k\\nTemplates can also be specified as a direct path to a YAML file on disk:\\nllm -t path/to/template.yaml \\'extra prompt here\\'\\nOr as a URL to a YAML file hosted online:\\nllm -t https://raw.githubusercontent.com/simonw/llm-templates/refs/heads/main/python-app.\\n˓→yaml \\\\\\n\\'Python app to pick a random line from a file\\'\\nNotethattemplatesloadedviaURLswillhaveany functions: keysignored,toavoidaccidentallyexecutingarbitrary\\ncode. This restriction also applies to templates loaded via thetemplate loaders plugin mechanism.\\n2.7.3 Listing available templates\\nThis command lists all available templates:\\nllm templates\\nThe output looks something like this:\\ncmd : system: reply with macos terminal commands only, no extra information\\nglados : system: You are GlaDOS prompt: Summarize this:\\n2.7.4 Templates as YAML files\\nTemplates are stored as YAML files on disk.\\nYou can edit (or create) a YAML file for a template using thellm templates editcommand:\\nllm templates edit summarize\\nThis will open the system default editor.\\nTip: You can control which editor will be used here using theEDITORenvironment variable - for example, to use VS\\nCode:\\nexport EDITOR=\"code -w\"\\nAdd that to your~/.zshrc or ~/.bashrc file depending on which shell you use (zsh is the default on macOS since\\nmacOS Catalina in 2019).\\n2.7. Templates 51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 57, 'page_label': '52'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nYou can create or edit template files directly in the templates directory. The location of this directory is shown by the\\nllm templates pathcommand:\\nllm templates path\\nExample output:\\n/Users/simon/Library/Application Support/io.datasette.llm/templates\\nA basic YAML template looks like this:\\nprompt: 'Summarize this: $input'\\nOr use YAML multi-line strings for longer inputs. I created this usingllm templates edit steampunk:\\nprompt: >\\nSummarize the following text.\\nInsert frequent satirical steampunk-themed illustrative anecdotes.\\nReally go wild with that.\\nText to summarize: $input\\nTheprompt: > causes the followingindented text to be treatedas a single string, with newlinescollapsed to spaces.\\nUse prompt: | to preserve newlines.\\nRunningthatwith llm -t steampunkagainstGPT-4o(viastrip-tagstoremoveHTMLtagsfromtheinputandminify\\nwhitespace):\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio' | \\\\\\nstrip-tags -m | llm -t steampunk -m gpt-4o\\nOutput:\\nIn a fantastical steampunk world, Simon Willison decided to merge an old MP3 recording with slides\\nfrom the talk using iMovie. After exporting the slides as images and importing them into iMovie, he had\\nto disable the default Ken Burns effect using the “Crop” tool. Then, Simon manually synchronized the\\naudiobyadjustingthedurationofeachimage. Finally,hepublishedthemasterpiecetoYouTube,withthe\\nwhimsical magic of steampunk-infused illustrations leaving his viewers in awe.\\nSystem prompts\\nWhen working with models that support system prompts you can set a system prompt using asystem: key like so:\\nsystem: Summarize this\\nIf you specify only a system prompt you don’t need to use the$input variable -llm will use the user’s input as the\\nwhole of the regular prompt, which will then be processed using the instructions set in that system prompt.\\nYou can combine system and regular prompts like so:\\nsystem: You speak like an excitable Victorian adventurer\\nprompt: 'Summarize this: $input'\\n52 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 58, 'page_label': '53'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nFragments\\nTemplates can referenceFragmentsusing thefragments: and system_fragments: keys. These should be a list of\\nfragment URLs, filepaths or hashes:\\nfragments:\\n- https://example.com/robots.txt\\n- /path/to/file.txt\\n- 993fd38d898d2b59fd2d16c811da5bdac658faa34f0f4d411edde7c17ebb0680\\nsystem_fragments:\\n- https://example.com/systm-prompt.txt\\nOptions\\nDefault options can be set using theoptions: key:\\nname: wild-french\\nsystem: Speak in French\\noptions:\\ntemperature: 1.8\\nTools\\nThe tools: key can provide a list of tool names from other plugins - either function names or toolbox specifiers:\\nname: time-plus\\ntools:\\n- llm_time\\n- Datasette(\"https://example.com/timezone-lookup\")\\nThe functions: key can provide a multi-line string of Python code defining additional functions:\\nname: my-functions\\nfunctions: |\\ndef reverse_string(s: str):\\nreturn s[::-1]\\ndef greet(name: str):\\nreturn f\"Hello, {name}!\"\\nSchemas\\nUsethe schema_object: keytoembedaJSONschema(asYAML)inyourtemplate. Theeasiestwaytocreatethese\\niswiththe llm --schema ... --save name-of-templatecommand-theresultshouldlooksomethinglikethis:\\nname: dogs\\nschema_object:\\nproperties:\\ndogs:\\nitems:\\nproperties:\\n(continues on next page)\\n2.7. Templates 53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 59, 'page_label': '54'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbio:\\ntype: string\\nname:\\ntype: string\\ntype: object\\ntype: array\\ntype: object\\nAdditional template variables\\nTemplates that work against the user’s normal prompt input (content that is either piped to the tool via standard input\\nor passed as a command-line argument) can use the$inputvariable.\\nYou can use additional named variables. These will then need to be provided using the-p/--param option when\\nexecuting the template.\\nHere’s an example YAML template calledrecipe, which you can create usingllm templates edit recipe:\\nprompt: |\\nSuggest a recipe using ingredients: $ingredients\\nIt should be based on cuisine from this country: $country\\nThis can be executed like so:\\nllm -t recipe -p ingredients 'sausages, milk' -p country Germany\\nMy output started like this:\\nRecipe: German Sausage and Potato Soup\\nIngredients:\\n• 4 German sausages\\n• 2 cups whole milk\\nThis example combines input piped to the tool with additional parameters. Call thissummarize:\\nsystem: Summarize this text in the voice of $voice\\nThen to run it:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio' | \\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nI got this:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\n54 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 60, 'page_label': '55'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nSpecifying default parameters\\nWhen creating a template using the--save option you can pass-p name value to store the default values for pa-\\nrameters:\\nllm --system 'Summarize this text in the voice of $voice' \\\\\\n--model gpt-4o -p voice GlaDOS --save summarize\\nYou can specify default values for parameters in the YAML using thedefaults: key.\\nsystem: Summarize this text in the voice of $voice\\ndefaults:\\nvoice: GlaDOS\\nWhen running without-pit will choose the default:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio' | \\\\\\nstrip-tags -m | llm -t summarize\\nBut you can override the defaults with-p:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio' | \\\\\\nstrip-tags -m | llm -t summarize -p voice Yoda\\nI got this:\\nText,summarizeinYoda’svoice,Iwill: “Hmm,youngpadawan. Summaryofthistext,youseek. Hmmm.\\n...\\nConfiguring code extraction\\nTo configure theextract first fenced code blocksetting for the template, add this:\\nextract: true\\nSetting a default model for a template\\nTemplates executed usingllm -t template-namewill execute using the default model that the user has configured\\nfor the tool - orgpt-3.5-turboif they have not configured their own default.\\nYou can specify a new default model for a template using themodel: key in the associated YAML. Here’s a template\\ncalled roast:\\nmodel: gpt-4o\\nsystem: roast the user at every possible opportunity, be succinct\\nExample:\\nllm -t roast 'How are you today?'\\nI’m doing great but with your boring questions, I must admit, I’ve seen more life in a cemetery.\\n2.7. Templates 55\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 61, 'page_label': '56'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.7.5 Template loaders from plugins\\nLLM plugins canregister prefixesthat can be used to load templates from external sources.\\nllm-templates-github is an example which adds agh: prefix which can be used to load templates from GitHub.\\nYou can install that plugin like this:\\nllm install llm-templates-github\\nUse thellm templates loaderscommand to see details of the registered loaders.\\nllm templates loaders\\nOutput:\\ngh:\\nLoad a template from GitHub or local cache if available\\nFormat: username/repo/template_name (without the .yaml extension)\\nor username/template_name which means username/llm-templates/template_name\\nThen you can then use it like this:\\ncurl -sL 'https://llm.datasette.io/' | llm -t gh:simonw/summarize\\nThe -sLflags tocurlare used to follow redirects and suppress progress meters.\\nThis command will fetch the content of the LLM index page and feed it to the template defined by summarize.yaml in\\nthe simonw/llm-templates GitHub repository.\\nIf two template loader plugins attempt to register the same prefix one of them will have_1 added to the end of their\\nprefix. Usellm templates loadersto check if this has occurred.\\n2.8 Fragments\\nLLM prompts can optionally be composed out offragments - reusable pieces of text that are logged just once to the\\ndatabase and can then be attached to multiple prompts.\\nTheseareparticularlyusefulwhenyouareworkingwithlongcontextmodels, whichsupportfeedinglargeamountsof\\ntext in as part of your prompt.\\nFragmentsprimarilyexisttosavespaceinthedatabase,butmaybeusedtosupportotherfeaturessuchasvendorprompt\\ncaching as well.\\nFragments can be specified using several different mechanisms:\\n• URLs to text files online\\n• Paths to text files on disk\\n• Aliases that have been attached to a specific fragment\\n• Hash IDs of stored fragments, where the ID is the SHA256 hash of the fragment content\\n• Fragments that are provided by custom plugins - these look likeplugin-name:argument\\n56 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 62, 'page_label': '57'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.8.1 Using fragments in a prompt\\nUse the-f/--fragmentoption to specify one or more fragments to be used as part of your prompt:\\nllm -f https://llm.datasette.io/robots.txt \"Explain this robots.txt file in detail\"\\nHere we are specifying a fragment using a URL. The contents of that URL will be included in the prompt that is sent\\nto the model, prepended prior to the prompt text.\\nThe -f option can be used multiple times to combine together multiple fragments.\\nFragments can also be files on disk, for example:\\nllm -f setup.py \\'extract the metadata\\'\\nUse -to specify a fragment that is read from standard input:\\nllm -f - \\'extract the metadata\\' < setup.py\\nThis will read the contents ofsetup.pyfrom standard input and use it as a fragment.\\nFragmentscanalsobeusedaspartofyoursystemprompt. Use --sf valueor --system-fragment valueinstead\\nof -f.\\n2.8.2 Using fragments in chat\\nThe chatcommand also supports the-f and --sf arguments to start a chat with fragments.\\nllm chat -f my_doc.txt\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> Explain this document to me\\nFragments can also be addedduring a chat conversation using the!fragment <my_fragment>command.\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !fragment https://llm.datasette.io/en/stable/fragments.html\\nThis can be combined with!multi:\\n> !multi\\nExplain the difference between fragments and templates to me\\n!fragment https://llm.datasette.io/en/stable/fragments.html https://llm.datasette.io/en/\\n˓→stable/templates.html\\n!end\\nAny!fragmentlines found in a prompt created with!editwill not be parsed.\\n2.8. Fragments 57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 63, 'page_label': '58'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.8.3 Browsing fragments\\nYoucanviewatruncatedversionofthefragmentsyouhavepreviouslystoredinyourdatabasewiththe llm fragments\\ncommand:\\nllm fragments\\nThe output from that command looks like this:\\n- hash: 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\naliases: []\\ndatetime_utc: \\'2025-04-06 07:36:53\\'\\nsource: https://raw.githubusercontent.com/simonw/llm-docs/refs/heads/main/llm/0.22.txt\\ncontent: |-\\n<documents>\\n<document index=\"1\">\\n<source>docs/aliases.md</source>\\n<document_content>\\n(aliases)=\\n#...\\n- hash: 16b686067375182573e2aa16b5bfc1e64d48350232535d06444537e51f1fd60c\\naliases: []\\ndatetime_utc: \\'2025-04-06 23:03:47\\'\\nsource: simonw/files-to-prompt/pyproject.toml\\ncontent: |-\\n[project]\\nname = \"files-to-prompt\"\\nversion = \"0.6\"\\ndescription = \"Concatenate a directory full of...\\nThose longhashvalues are IDs that can be used to reference a fragment in the future:\\nllm -f 16b686067375182573e2aa16b5bfc1e64d48350232535d06444537e51f1fd60c \\'Extract metadata\\n˓→\\'\\nUse -q searchtermone or more times to search for fragments that match a specific set of search terms.\\nTo view the full content of a fragment usellm fragments show:\\nllm fragments show 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\n2.8.4 Setting aliases for fragments\\nYou can assign aliases to fragments that you use often using thellm fragments setcommand:\\nllm fragments set mydocs ./docs.md\\nTo remove an alias, usellm fragments remove:\\nllm fragments remove mydocs\\nYou can then use that alias in place of the fragment hash ID:\\n58 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 64, 'page_label': '59'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm -f mydocs 'How do I access metadata?'\\nUse llm fragments --aliasesto see a full list of fragments that have been assigned aliases:\\nllm fragments --aliases\\n2.8.5 Viewing fragments in your logs\\nThe llm logscommand lists the fragments that were used for a prompt. By default these are listed as fragment hash\\nIDs, but you can use the--expandoption to show the full content of each fragment.\\nThis command will show the expanded fragments for your most recent conversation:\\nllm logs -c --expand\\nYou can filter for logs that used a specific fragment using the-f/--fragmentoption:\\nllm logs -c -f 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\nThis accepts URLs, file paths, aliases, and hash IDs.\\nMultiple -f options will return responses that usedall of the specified fragments.\\nFragmentsarereturnedby llm logs --jsonaswell. Bydefaultthesearetruncatedbutyoucanaddthe -e/--expand\\noption to show the full content of each fragment.\\nllm logs -c --json --expand\\n2.8.6 Using fragments from plugins\\nLLM plugins can provide custom fragment loaders which do useful things.\\nOne example is the llm-fragments-github plugin. This can convert the files from a public GitHub repository into a list\\nof fragments, allowing you to ask questions about the full repository.\\nHere’s how to try that out:\\nllm install llm-fragments-github\\nllm -f github:simonw/s3-credentials 'Suggest new features for this tool'\\nThis plugin turns a single call to-f github:simonw/s3-credentials into multiple fragments, one for every text\\nfile in the simonw/s3-credentials GitHub repository.\\nRunningllm logs -cwill show that this prompt incorporated 26 fragments, one for each file.\\nRunningllm logs -c --usage --expand(shortcut: llm logs -cue)includestokenusageinformationandturns\\neach fragment ID into a full copy of that file. Here’s the output of that command.\\nFragment plugins can returnattachments(such as images) as well.\\nSeethe register_fragment_loaders()pluginhook documentationfordetailsonwritingyourowncustomfragmentplu-\\ngin.\\n2.8. Fragments 59\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 65, 'page_label': '60'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.8.7 Listing available fragment prefixes\\nThe llm fragments loaders command shows all prefixes that have been installed by plugins, along with their\\ndocumentation:\\nllm install llm-fragments-github\\nllm fragments loaders\\nExample output:\\ngithub:\\nLoad files from a GitHub repository as fragments\\nArgument is a GitHub repository URL or username/repository\\nissue:\\nFetch GitHub issue and comments as Markdown\\nArgument is either \"owner/repo/NUMBER\"\\nor \"https://github.com/owner/repo/issues/NUMBER\"\\n2.9 Model aliases\\nLLM supports model aliases, which allow you to refer to a model by a short name instead of its full ID.\\n2.9.1 Listing aliases\\nTo list current aliases, run this:\\nllm aliases\\nExample output:\\n4o : gpt-4o\\nchatgpt-4o : chatgpt-4o-latest\\n4o-mini : gpt-4o-mini\\n4.1 : gpt-4.1\\n4.1-mini : gpt-4.1-mini\\n4.1-nano : gpt-4.1-nano\\n3.5 : gpt-3.5-turbo\\nchatgpt : gpt-3.5-turbo\\nchatgpt-16k : gpt-3.5-turbo-16k\\n3.5-16k : gpt-3.5-turbo-16k\\n4 : gpt-4\\ngpt4 : gpt-4\\n4-32k : gpt-4-32k\\ngpt-4-turbo-preview : gpt-4-turbo\\n4-turbo : gpt-4-turbo\\n4t : gpt-4-turbo\\ngpt-4.5 : gpt-4.5-preview\\n3.5-instruct : gpt-3.5-turbo-instruct\\n(continues on next page)\\n60 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 66, 'page_label': '61'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nchatgpt-instruct : gpt-3.5-turbo-instruct\\nada : text-embedding-ada-002 (embedding)\\nada-002 : text-embedding-ada-002 (embedding)\\n3-small : text-embedding-3-small (embedding)\\n3-large : text-embedding-3-large (embedding)\\n3-small-512 : text-embedding-3-small-512 (embedding)\\n3-large-256 : text-embedding-3-large-256 (embedding)\\n3-large-1024 : text-embedding-3-large-1024 (embedding)\\nAdd--jsonto get that list back as JSON:\\nllm aliases list --json\\nExample output:\\n{\\n\"3.5\": \"gpt-3.5-turbo\",\\n\"chatgpt\": \"gpt-3.5-turbo\",\\n\"4\": \"gpt-4\",\\n\"gpt4\": \"gpt-4\",\\n\"ada\": \"ada-002\"\\n}\\n2.9.2 Adding a new alias\\nThe llm aliases set <alias> <model-id>command can be used to add a new alias:\\nllm aliases set mini gpt-4o-mini\\nYou can also pass one or more-q searchoptions to set an alias on the first model matching those search terms:\\nllm aliases set mini -q 4o -q mini\\nNow you can run thegpt-4o-minimodel using theminialias like this:\\nllm -m mini \\'An epic Greek-style saga about a cheesecake that builds a SQL database from␣\\n˓→scratch\\'\\nAliases can be set for both regular models andembedding modelsusing the same command. To set an alias ofoaifor\\nthe OpenAIada-002embedding model use this:\\nllm aliases set oai ada-002\\nNow you can embed a string using that model like so:\\nllm embed -c \\'hello world\\' -m oai\\nOutput:\\n[-0.014945968054234982, 0.0014304015785455704, ...]\\n2.9. Model aliases 61'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 67, 'page_label': '62'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.9.3 Removing an alias\\nThe llm aliases remove <alias>command will remove the specified alias:\\nllm aliases remove mini\\n2.9.4 Viewing the aliases file\\nAliases are stored in analiases.jsonfile in the LLM configuration directory.\\nTo see the path to that file, run this:\\nllm aliases path\\nTo view the content of that file, run this:\\ncat \"$(llm aliases path)\"\\n2.10 Embeddings\\nEmbedding models allow you to take a piece of text - a word, sentence, paragraph or even a whole article, and convert\\nthat into an array of floating point numbers.\\nThis floating point array is called an “embedding vector”, and works as a numerical representation of the semantic\\nmeaning of the content in a many-multi-dimensional space.\\nBy calculating the distance between embedding vectors, we can identify which content is semantically “nearest” to\\nother content.\\nThis can be used to build features like related article lookups. It can also be used to build semantic search, where a\\nuser can search for a phrase and get back results that are semantically similar to that phrase even if they do not share\\nany exact keywords.\\nSomeembeddingmodelslikeCLIPcanevenworkagainstbinaryfilessuchasimages. Thesecanbeusedtosearchfor\\nimages that are similar to other images, or to search for images that are semantically similar to a piece of text.\\nLLM supports multiple embedding models throughplugins. Once installed, an embedding model can be used on the\\ncommand-line or via the Python API to calculate and store embeddings for content, and then to perform similarity\\nsearches against those embeddings.\\nSee LLM now provides tools for working with embeddings for an extended explanation of embeddings, why they are\\nuseful and what you can do with them.\\n2.10.1 Embedding with the CLI\\nLLM provides command-line utilities for calculating and storing embeddings for pieces of content.\\n62 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 68, 'page_label': '63'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm embed\\nThe llm embed command can be used to calculate embedding vectors for a string of content. These can be returned\\ndirectly to the terminal, stored in a SQLite database, or both.\\nReturning embeddings to the terminal\\nThe simplest way to use this command is to pass content to it using the-c/--contentoption, like this:\\nllm embed -c 'This is some content' -m 3-small\\n-m 3-smallspecifiestheOpenAI text-embedding-3-smallmodel. YouwillneedtohavesetanOpenAIAPIkey\\nusing llm keys set openaifor this to work.\\nYou can install plugins to access other models. The llm-sentence-transformers plugin can be used to run models on\\nyour own laptop, such as the MiniLM-L6 model:\\nllm install llm-sentence-transformers\\nllm embed -c 'This is some content' -m sentence-transformers/all-MiniLM-L6-v2\\nThe llm embedcommand returns a JSON array of floating point numbers directly to the terminal:\\n[0.123, 0.456, 0.789...]\\nYou can omit the-m/--modeloption if you set adefault embedding model.\\nYoucanalsosetthe LLM_EMBEDDING_MODELenvironmentvariabletosetadefaultmodelforall llm embedcommands\\nin the current shell session:\\nexport LLM_EMBEDDING_MODEL=3-small\\nllm embed -c 'This is some content'\\nLLM also offers a binary storage format for embeddings, described inembeddings storage format.\\nYoucanoutputembeddingsusingthatformatasrawbytesusing --format blob,orinhexadecimalusing --format\\nhex, or in Base64 using--format base64:\\nllm embed -c 'This is some content' -m 3-small --format base64\\nThis outputs:\\n8NGzPFtdgTqHcZw7aUT6u+++WrwwpZo8XbSxv...\\nSome models such as llm-clip can run against binary data. You can pass in binary data using the-i and --binary\\noptions:\\nllm embed --binary -m clip -i image.jpg\\nOr from standard input like this:\\ncat image.jpg | llm embed --binary -m clip -i -\\n2.10. Embeddings 63\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 69, 'page_label': '64'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nStoring embeddings in SQLite\\nEmbeddingsaremuchmoreusefulifyoustorethemsomewhere,soyoucancalculatesimilarityscoresbetweendifferent\\nembeddings later on.\\nLLM includes the concept of acollection of embeddings. A collection groups together a set of stored embeddings\\ncreated using the same model, each with a unique ID within that collection.\\nEmbeddings also store a hash of the content that was embedded. This hash is later used to avoid calculating duplicate\\nembeddings for the same content.\\nFirst, we’ll set a default model so we don’t have to keep repeating it:\\nllm embed-models default 3-small\\nThe llm embedcommand can store results directly in a named collection like this:\\nllm embed quotations philkarlton-1 -c \\\\\\n'There are only two hard things in Computer Science: cache invalidation and naming␣\\n˓→things'\\nThis stores the given text in thequotationscollection under the keyphilkarlton-1.\\nYou can also pipe content to standard input, like this:\\ncat one.txt | llm embed files one\\nThis will store the embedding for the contents ofone.txtin thefilescollection under the keyone.\\nA collection will be created the first time you mention it.\\nCollections have a fixed embedding model, which is the model that was used for the first embedding stored in that\\ncollection.\\nIn the above example this would have been the default embedding model at the time that the command was run.\\nThe following example stores the embedding for the string “my happy hound” in a collection calledphrases under\\nthe keyhoundand using the model3-small:\\nllm embed phrases hound -m 3-small -c 'my happy hound'\\nBydefault,theSQLitedatabaseusedtostoreembeddingsisthe embeddings.dbintheusercontentdirectorymanaged\\nby LLM.\\nYou can see the path to this directory by runningllm collections path.\\nYou can store embeddings in a different SQLite database by passing a path to it using the-d/--database option to\\nllm embed. If this file does not exist yet the command will create it:\\nllm embed phrases hound -d my-embeddings.db -c 'my happy hound'\\nThis creates a database file calledmy-embeddings.dbin the current directory.\\n64 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 70, 'page_label': '65'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nStoring content and metadata\\nBy default, only the entry ID and the embedding vector are stored in the database table.\\nYou can store a copy of the original text in thecontentcolumn by passing the--storeoption:\\nllm embed phrases hound -c \\'my happy hound\\' --store\\nYou can also store a JSON object containing arbitrary metadata in themetadatacolumn by passing the--metadata\\noption. This example uses both--storeand --metadataoptions:\\nllm embed phrases hound \\\\\\n-m 3-small \\\\\\n-c \\'my happy hound\\' \\\\\\n--metadata \\'{\"name\": \"Hound\"}\\' \\\\\\n--store\\nData stored in this way will be returned by calls tollm similar, for example:\\nllm similar phrases -c \\'hound\\'\\n{\"id\": \"hound\", \"score\": 0.8484683588631485, \"content\": \"my happy hound\", \"metadata\": {\\n˓→\"name\": \"Hound\"}}\\nllm embed-multi\\nThe llm embedcommand embeds a single string at a time.\\nllm embed-multi can be used to embed multiple strings at once, taking advantage of any efficiencies that the em-\\nbedding model may provide when processing multiple strings.\\nThis command can be called in one of three ways:\\n1. With a CSV, TSV, JSON or newline-delimited JSON file\\n2. With a SQLite database and a SQL query\\n3. With one or more paths to directories, each accompanied by a glob pattern\\nAll three mechanisms support these options:\\n• -m model_idto specify the embedding model to use\\n• -d database.dbto specify a different database file to store the embeddings in\\n• --storeto store the original content in the embeddings table in addition to the embedding vector\\n• --prefixto prepend a prefix to the stored ID of each item\\n• --prependto prepend a string to the content before embedding\\n• --batch-size SIZEto process embeddings in batches of the specified size\\nThe--prependoptionisusefulforembeddingmodelsthatrequireyoutoprependaspecialtokentothecontentbefore\\nembedding it. nomic-embed-text-v2-moe for example requires documents to be prepended\\'search_document: \\'\\nand search queries to be prepended\\'search_query: \\'.\\n2.10. Embeddings 65'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 71, 'page_label': '66'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nEmbedding data from a CSV, TSV or JSON file\\nYoucanembeddatafromaCSV,TSVorJSONfilebypassingthatfiletothecommandasthesecondoption, afterthe\\ncollection name.\\nYourfilemustcontainatleasttwocolumns. ThefirstoneisexpectedtocontaintheIDoftheitem,andanysubsequent\\ncolumns will be treated as containing content to be embedded.\\nAn example CSV file might look like this:\\nid,content\\none,This is the first item\\ntwo,This is the second item\\nTSV would use tabs instead of commas.\\nJSON files can be structured like this:\\n[\\n{\"id\": \"one\", \"content\": \"This is the first item\"},\\n{\"id\": \"two\", \"content\": \"This is the second item\"}\\n]\\nOr as newline-delimited JSON like this:\\n{\"id\": \"one\", \"content\": \"This is the first item\"}\\n{\"id\": \"two\", \"content\": \"This is the second item\"}\\nIn each of these cases the file can be passed tollm embed-multilike this:\\nllm embed-multi items mydata.csv\\nThe first argument is the name of the collection, the second is the filename.\\nYou can also pipe content to standard input of the tool using-:\\ncat mydata.json | llm embed-multi items -\\nLLMwillattempttodetecttheformatofyourdataautomatically. Ifthisdoesn’tworkyoucanspecifytheformatusing\\nthe --formatoption. This is required if you are piping newline-delimited JSON to standard input.\\ncat mydata.json | llm embed-multi items - --format nl\\nOther supported--formatoptions arecsv, tsvand json.\\nThis example embeds the data from a JSON file in a collection calleditems in database calleddocs.db using the\\n3-smallmodelandstorestheoriginalcontentinthe embeddingstableaswell,addingaprefixof my-items/toeach\\nID:\\nllm embed-multi items mydata.json \\\\\\n-d docs.db \\\\\\n-m 3-small \\\\\\n--prefix my-items/ \\\\\\n--store\\n66 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 72, 'page_label': '67'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nEmbedding data from a SQLite database\\nYoucanembeddatafromaSQLitedatabaseusing --sql,optionallycombinedwith --attachtoattachanadditional\\ndatabase.\\nIf you are storing embeddings in the same database as the source data, you can do this:\\nllm embed-multi docs \\\\\\n-d docs.db \\\\\\n--sql 'select id, title, content from documents' \\\\\\n-m 3-small\\nThe docs.db database here contains adocuments table, and we want to embed thetitle and content columns\\nfrom that table and store the results back in the same database.\\nTo load content from a database other than the one you are using to store embeddings, attach it with the--attach\\noption and usealias.tablein your SQLite query:\\nllm embed-multi docs \\\\\\n-d embeddings.db \\\\\\n--attach other other.db \\\\\\n--sql 'select id, title, content from other.documents' \\\\\\n-m 3-small\\nEmbedding data from files in directories\\nLLM can embed the content of every text file in a specified directory, using the file’s path and name as the ID.\\nConsider a directory structure like this:\\ndocs/aliases.md\\ndocs/contributing.md\\ndocs/embeddings/binary.md\\ndocs/embeddings/cli.md\\ndocs/embeddings/index.md\\ndocs/index.md\\ndocs/logging.md\\ndocs/plugins/directory.md\\ndocs/plugins/index.md\\nTo embed all of those documents, you can run the following:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md' \\\\\\n-d documentation.db \\\\\\n--store\\nHere--files docs '**/*.md' specifies that thedocsdirectory should be scanned for files matching the**/*.md\\nglob pattern - which will match Markdown files in any nested directory.\\nThe result of the above command is aembeddingstable with the following IDs:\\n2.10. Embeddings 67\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 73, 'page_label': '68'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\naliases.md\\ncontributing.md\\nembeddings/binary.md\\nembeddings/cli.md\\nembeddings/index.md\\nindex.md\\nlogging.md\\nplugins/directory.md\\nplugins/index.md\\nEach corresponding to embedded content for the file in question.\\nThe --prefixoption can be used to add a prefix to each ID:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md' \\\\\\n-d documentation.db \\\\\\n--store \\\\\\n--prefix llm-docs/\\nThis will result in the following IDs instead:\\nllm-docs/aliases.md\\nllm-docs/contributing.md\\nllm-docs/embeddings/binary.md\\nllm-docs/embeddings/cli.md\\nllm-docs/embeddings/index.md\\nllm-docs/index.md\\nllm-docs/logging.md\\nllm-docs/plugins/directory.md\\nllm-docs/plugins/index.md\\nFilesareassumedtobe utf-8, butLLMwillfallbackto latin-1ifitencountersanencodingerror. Youcanspecify\\na different set of encodings using the--encodingoption.\\nThis example will tryutf-16first and thenmac_romanbefore falling back tolatin-1:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md' \\\\\\n-d documentation.db \\\\\\n--encoding utf-16 \\\\\\n--encoding mac_roman \\\\\\n--encoding latin-1\\nIf a file cannot be read it will be logged to standard error but the script will keep on running.\\nIf you are embedding binary content such as images for use with CLIP, add the--binaryoption:\\nllm embed-multi photos \\\\\\n-m clip \\\\\\n--files photos/ '*.jpeg' --binary\\n68 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 74, 'page_label': '69'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm similar\\nThellm similarcommandsearchesacollectionofembeddingsfortheitemsthataremostsimilartoagivenoritem\\nID, based on cosine similarity.\\nThis currently uses a slow brute-force approach which does not scale well to large collections. See issue 216 for plans\\nto add a more scalable approach via vector indexes provided by plugins.\\nTo search thequotationscollection for items that are semantically similar to\\'computer science\\':\\nllm similar quotations -c \\'computer science\\'\\nThis embeds the provided string and returns a newline-delimited list of JSON objects like this:\\n{\"id\": \"philkarlton-1\", \"score\": 0.8323904531677017, \"content\": null, \"metadata\": null}\\nUse -p/--plainto get back results in plain text instead of JSON:\\nllm similar quotations -c \\'computer science\\' -p\\nExample output:\\nphilkarlton-1 (0.8323904531677017)\\nYou can compare against text stored in a file using-i filename:\\nllm similar quotations -i one.txt\\nOr feed text to standard input using-i -:\\necho \\'computer science\\' | llm similar quotations -i -\\nWhen using a model like CLIP, you can find images similar to an input image using-i filenamewith --binary:\\nllm similar photos -i image.jpg --binary\\nYou can filter results to only show IDs that begin with a specific prefix using –prefix:\\nllm similar quotations --prefix \\'movies/\\' -c \\'star wars\\'\\nllm embed-models\\nTo list all available embedding models, including those provided by plugins, run this command:\\nllm embed-models\\nThe output should look something like this:\\nOpenAIEmbeddingModel: text-embedding-ada-002 (aliases: ada, ada-002)\\nOpenAIEmbeddingModel: text-embedding-3-small (aliases: 3-small)\\nOpenAIEmbeddingModel: text-embedding-3-large (aliases: 3-large)\\n...\\nAdd-qone or more times to search for models matching those terms:\\n2.10. Embeddings 69'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 75, 'page_label': '70'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm embed-models -q 3-small\\nllm embed-models default\\nThis command can be used to get and set the default embedding model.\\nThis will return the name of the current default model:\\nllm embed-models default\\nYou can set a different default like this:\\nllm embed-models default 3-small\\nThis will set the default model to OpenAI’s3-smallmodel.\\nAny of the supported aliases for a model can be passed to this command.\\nYou can unset the default model using--remove-default:\\nllm embed-models default --remove-default\\nWhennodefaultmodelisset,the llm embedandllm embed-multicommandswillrequirethatamodelisspecified\\nusing -m/--model.\\nllm collections list\\nTo list all of the collections in the embeddings database, run this command:\\nllm collections list\\nAdd--jsonfor JSON output:\\nllm collections list --json\\nAdd-d/--databaseto specify a different database file:\\nllm collections list -d my-embeddings.db\\nllm collections delete\\nTo delete a collection from the database, run this:\\nllm collections delete collection-name\\nPass-dto specify a different database file:\\nllm collections delete collection-name -d my-embeddings.db\\n70 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 76, 'page_label': '71'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.10.2 Using embeddings from Python\\nYou can load an embedding model using its model ID or alias like this:\\nimport llm\\nembedding_model = llm.get_embedding_model(\"3-small\")\\nTo embed a string, returning a Python list of floating point numbers, use the.embed()method:\\nvector = embedding_model.embed(\"my happy hound\")\\nIf the embedding model can handle binary input, you can call.embed()with a byte string instead. You can check the\\nsupports_binaryproperty to see if this is supported:\\nif embedding_model.supports_binary:\\nvector = embedding_model.embed(open(\"my-image.jpg\", \"rb\").read())\\nThe embedding_model.supports_textproperty indicates if the model supports text input.\\nMany embeddings models are more efficient when you embed multiple strings or binary strings at once. To embed\\nmultiple strings at once, use the.embed_multi()method:\\nvectors = list(embedding_model.embed_multi([\"my happy hound\", \"my dissatisfied cat\"]))\\nThis returns a generator that yields one embedding vector per string.\\nEmbeddings are calculated in batches. By default all items will be processed in a single batch, unless the underlying\\nembedding model has defined its own preferred batch size. You can pass a custom batch size usingbatch_size=N,\\nfor example:\\nvectors = list(embedding_model.embed_multi(lines_from_file, batch_size=20))\\nWorking with collections\\nThe llm.Collectionclass can be used to work withcollections of embeddings from Python code.\\nA collection is a named group of embedding vectors, each stored along with their IDs in a SQLite database table.\\nToworkwithembeddingsinthiswayyouwillneedaninstanceofasqlite-utilsDatabaseobject. Youcanthenpassthat\\nto thellm.Collectionconstructor along with the unique string name of the collection and the ID of the embedding\\nmodel you will be using with that collection:\\nimport sqlite_utils\\nimport llm\\n# This collection will use an in-memory database that will be\\n# discarded when the Python process exits\\ncollection = llm.Collection(\"entries\", model_id=\"3-small\")\\n# Or you can persist the database to disk like this:\\ndb = sqlite_utils.Database(\"my-embeddings.db\")\\ncollection = llm.Collection(\"entries\", db, model_id=\"3-small\")\\n# You can pass a model directly using model= instead of model_id=\\n(continues on next page)\\n2.10. Embeddings 71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 77, 'page_label': '72'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nembedding_model = llm.get_embedding_model(\"3-small\")\\ncollection = llm.Collection(\"entries\", db, model=embedding_model)\\nIf the collection already exists in the database you can omit themodel or model_id argument - the model ID will be\\nread from thecollectionstable.\\nTo embed a single string and store it in the collection, use theembed()method:\\ncollection.embed(\"hound\", \"my happy hound\")\\nThis stores the embedding for the string “my happy hound” in theentriescollection under the keyhound.\\nAddstore=Trueto store the text content itself in the database table along with the embedding vector.\\nTo attach additional metadata to an item, pass a JSON-compatible dictionary as themetadata=argument:\\ncollection.embed(\"hound\", \"my happy hound\", metadata={\"name\": \"Hound\"}, store=True)\\nThis additional metadata will be stored as JSON in themetadatacolumn of the embeddings database table.\\nStoring embeddings in bulk\\nThe collection.embed_multi() method can be used to store embeddings for multiple items at once. This can be\\nmore efficient for some embedding models.\\ncollection.embed_multi(\\n[\\n(\"hound\", \"my happy hound\"),\\n(\"cat\", \"my dissatisfied cat\"),\\n],\\n# Add this to store the strings in the content column:\\nstore=True,\\n)\\nTo include metadata to be stored with each item, callembed_multi_with_metadata():\\ncollection.embed_multi_with_metadata(\\n[\\n(\"hound\", \"my happy hound\", {\"name\": \"Hound\"}),\\n(\"cat\", \"my dissatisfied cat\", {\"name\": \"Cat\"}),\\n],\\n# This can also take the store=True argument:\\nstore=True,\\n)\\nThebatch_size=argumentdefaultsto100,andwillbeusedunlesstheembeddingmodelitselfdefinesalowerbatch\\nsize. You can adjust this if you are having trouble with memory while embedding large collections:\\ncollection.embed_multi(\\n(\\n(i, line)\\nfor i, line in enumerate(lines_in_file)\\n),\\n(continues on next page)\\n72 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 78, 'page_label': '73'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbatch_size=10\\n)\\nCollection class reference\\nA collection instance has the following properties and methods:\\n• id- the integer ID of the collection in the database\\n• name- the string name of the collection (unique in the database)\\n• model_id- the string ID of the embedding model used for this collection\\n• model()- returns theEmbeddingModelinstance, based on thatmodel_id\\n• count()- returns the integer number of items in the collection\\n• embed(id: str, text: str, metadata: dict=None, store: bool=False) - embeds the given\\nstring and stores it in the collection under the given ID. Can optionally include metadata (stored as JSON) and\\nstore the text content itself in the database table.\\n• embed_multi(entries: Iterable, store: bool=False, batch_size: int=100) - see above\\n• embed_multi_with_metadata(entries: Iterable, store: bool=False, batch_size:\\nint=100)- see above\\n• similar(query: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembedding\\nof the given query string\\n• similar_by_id(id: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembed-\\nding of the item with the given ID\\n• similar_by_vector(vector: List[float], number: int=10, skip_id: str=None) - returns a\\nlist of entries that are most similar to the given embedding vector, optionally skipping the entry with the given\\nID\\n• delete()- deletes the collection and its embeddings from the database\\nThere is also aCollection.exists(db, name) class method which returns a boolean value and can be used to\\ndetermine if a collection exists or not in a database:\\nif Collection.exists(db, \"entries\"):\\nprint(\"The entries collection exists\")\\nRetrieving similar items\\nOnceyouhavepopulatedacollectionofembeddingsyoucanretrievetheentriesthataremostsimilartoagivenstring\\nusing thesimilar()method.\\nThis method uses a brute force approach, calculating distance scores against every document. This is fine for small\\ncollections, but will not scale to large collections. See issue 216 for plans to add a more scalable approach via vector\\nindexes provided by plugins.\\nfor entry in collection.similar(\"hound\"):\\nprint(entry.id, entry.score)\\n2.10. Embeddings 73'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 79, 'page_label': '74'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThe string will first by embedded using the model for the collection.\\nThe entryobject returned is an object with the following properties:\\n• id- the string ID of the item\\n• score- the floating point similarity score between the item and the query string\\n• content- the string text content of the item, if it was stored - orNone\\n• metadata- the dictionary (from JSON) metadata for the item, if it was stored - orNone\\nThis defaults to returning the 10 most similar items. You can change this by passing a differentnumber=argument:\\nfor entry in collection.similar(\"hound\", number=5):\\nprint(entry.id, entry.score)\\nThesimilar_by_id()methodtakestheIDofanotheriteminthecollectionandreturnsthemostsimilaritemstothat\\none, based on the embedding that has already been stored for it:\\nfor entry in collection.similar_by_id(\"cat\"):\\nprint(entry.id, entry.score)\\nThe item itself is excluded from the results.\\nSQL schema\\nHere’s the SQL schema used by the embeddings database:\\nCREATE TABLE [collections] (\\n[id] INTEGER PRIMARY KEY,\\n[name] TEXT,\\n[model] TEXT\\n)\\nCREATE TABLE \"embeddings\" (\\n[collection_id] INTEGER REFERENCES [collections]([id]),\\n[id] TEXT,\\n[embedding] BLOB,\\n[content] TEXT,\\n[content_blob] BLOB,\\n[content_hash] BLOB,\\n[metadata] TEXT,\\n[updated] INTEGER,\\nPRIMARY KEY ([collection_id], [id])\\n)\\n2.10.3 Writing plugins to add new embedding models\\nRead theplugin tutorialfor details on how to develop and package a plugin.\\nThis page shows an example plugin that implements and registers a new embedding model.\\nThere are two components to an embedding model plugin:\\n1. An implementation of theregister_embedding_models()hook, which takes aregistercallback function\\nand calls it to register the new model with the LLM plugin system.\\n74 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 80, 'page_label': '75'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2. A class that extends thellm.EmbeddingModelabstract base class.\\nThe only required method on this class isembed_batch(texts), which takes an iterable of strings and returns\\nan iterator over lists of floating point numbers.\\nThefollowingexampleusesthesentence-transformerspackagetoprovideaccesstotheMiniLM-L6embeddingmodel.\\nimport llm\\nfrom sentence_transformers import SentenceTransformer\\n@llm.hookimpl\\ndef register_embedding_models(register):\\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\"\\nregister(SentenceTransformerModel(model_id, model_id), aliases=(\"all-MiniLM-L6-v2\",))\\nclass SentenceTransformerModel(llm.EmbeddingModel):\\ndef __init__(self, model_id, model_name):\\nself.model_id = model_id\\nself.model_name = model_name\\nself._model = None\\ndef embed_batch(self, texts):\\nif self._model is None:\\nself._model = SentenceTransformer(self.model_name)\\nresults = self._model.encode(texts)\\nreturn (list(map(float, result)) for result in results)\\nOnce installed, the model provided by this plugin can be used with thellm embedcommand like this:\\ncat file.txt | llm embed -m sentence-transformers/all-MiniLM-L6-v2\\nOr via its registered alias like this:\\ncat file.txt | llm embed -m all-MiniLM-L6-v2\\nllm-sentence-transformers is a complete example of a plugin that provides an embedding model.\\nExecute Jina embeddings with a CLI using llm-embed-jina talks through a similar process to add support for the Jina\\nembeddings models.\\nEmbedding binary content\\nIf your model can embed binary content, use thesupports_binaryproperty to indicate that:\\nclass ClipEmbeddingModel(llm.EmbeddingModel):\\nmodel_id = \"clip\"\\nsupports_binary = True\\nsupports_text= True\\nsupports_text defaults toTrue and so is not necessary here. You can set it toFalse if your model only supports\\nbinary data.\\nIf your model accepts binary, your.embed_batch() model may be called with a list of Python bytestrings. These\\nmay be mixed with regular strings if the model accepts both types of input.\\n2.10. Embeddings 75'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 81, 'page_label': '76'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm-clip is an example of a model that can embed both binary and text content.\\n2.10.4 Embedding storage format\\nThe default output format of thellm embedcommand is a JSON array of floating point numbers.\\nLLM stores embeddings in space-efficient format: a little-endian binary sequences of 32-bit floating point numbers,\\neach represented using 4 bytes.\\nThese are stored in aBLOBcolumn in a SQLite database.\\nThe following Python functions can be used to convert between this format and an array of floating point numbers:\\nimport struct\\ndef encode(values):\\nreturn struct.pack(\"<\" + \"f\" * len(values), *values)\\ndef decode(binary):\\nreturn struct.unpack(\"<\" + \"f\" * (len(binary) // 4), binary)\\nThese functions are available asllm.encode()and llm.decode().\\nIf you are using NumPy you can decode one of these binary values like this:\\nimport numpy as np\\nnumpy_array = np.frombuffer(value, \"<f4\")\\nThe <f4format string here ensures NumPy will treat the data as a little-endian sequence of 32-bit floats.\\n2.11 Plugins\\nLLMpluginscanenhanceLLMbymakingalternativeLargeLanguageModelsavailable,eitherviaAPIorbyrunning\\nthe models locally on your machine.\\nPlugins can also add new commands to thellmCLI tool.\\nThe plugin directorylists available plugins that you can install and use.\\nDeveloping a model plugindescribes how to build a new plugin in detail.\\n2.11.1 Installing plugins\\nPlugins must be installed in the same virtual environment as LLM itself.\\nYou can find names of plugins to install in theplugin directory\\nUse thellm installcommand (a thin wrapper aroundpip install) to install plugins in the correct environment:\\nllm install llm-gpt4all\\nPlugins can be uninstalled withllm uninstall:\\nllm uninstall llm-gpt4all -y\\n76 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 82, 'page_label': '77'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThe -yflag skips asking for confirmation.\\nYou can see additional models that have been added by plugins by running:\\nllm models\\nOr add--optionsto include details of the options available for each model:\\nllm models --options\\nTo run a prompt against a newly installed model, pass its name as the-m/--modeloption:\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France?\\'\\nListing installed plugins\\nRunllm pluginsto list installed plugins:\\nllm plugins\\n[\\n{\\n\"name\": \"llm-anthropic\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\": \"0.11\"\\n},\\n{\\n\"name\": \"llm-gguf\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n],\\n\"version\": \"0.1a0\"\\n},\\n{\\n\"name\": \"llm-clip\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_embedding_models\"\\n],\\n\"version\": \"0.1\"\\n},\\n{\\n\"name\": \"llm-cmd\",\\n\"hooks\": [\\n\"register_commands\"\\n],\\n\"version\": \"0.2a0\"\\n},\\n{\\n\"name\": \"llm-gemini\",\\n(continues on next page)\\n2.11. Plugins 77'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 83, 'page_label': '78'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"hooks\": [\\n\"register_embedding_models\",\\n\"register_models\"\\n],\\n\"version\": \"0.3\"\\n}\\n]\\nRunning with a subset of plugins\\nBy default, LLM will load all plugins that are installed in the same virtual environment as LLM itself.\\nYou can control the set of plugins that is loaded using theLLM_LOAD_PLUGINSenvironment variable.\\nSet that to the empty string to disable all plugins:\\nLLM_LOAD_PLUGINS=\\'\\' llm ...\\nOr to a comma-separated list of plugin names to load only those plugins:\\nLLM_LOAD_PLUGINS=\\'llm-gpt4all,llm-cluster\\' llm ...\\nYou can use thellm pluginscommand to check that it is working correctly:\\nLLM_LOAD_PLUGINS=\\'\\' llm plugins\\n2.11.2 Plugin directory\\nThe following plugins are available for LLM. Here’show to install them.\\nLocal models\\nThese plugins all help you run LLMs directly on your own computer:\\n• llm-ggufuses llama.cpp to run models published in the GGUF format.\\n• llm-mlx (Mac only) uses Apple’s MLX framework to provide extremely high performance access to a large\\nnumber of local models.\\n• llm-ollamaadds support for local models run using Ollama.\\n• llm-llamafileadds support for local models that are running locally using llamafile.\\n• llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n• llm-gpt4all adds support for various models released by the GPT4All project that are optimized to run locally\\non your own machine. These models include versions of Vicuna, Orca, Falcon and MPT - here’s a full list of\\nmodels.\\n• llm-mpt30badds support for the MPT-30B local model.\\n78 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 84, 'page_label': '79'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nRemote APIs\\nThese plugins can be used to interact with remotely hosted models via their API:\\n• llm-mistral adds support for Mistral AI’s language and embedding models.\\n• llm-geminiadds support for Google’s Gemini models.\\n• llm-anthropicsupports Anthropic’s Claude 3 family, 3.5 Sonnet and beyond.\\n• llm-command-rsupports Cohere’s Command R and Command R Plus API models.\\n• llm-rekasupports the Reka family of models via their API.\\n• llm-perplexity by Alexandru Geana supports the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-onlinewhich can search for things online andllama-3-70b-instruct.\\n• llm-groqby Moritz Angermann provides access to fast models hosted by Groq.\\n• llm-grokby Benedikt Hiepler providing access to Grok model using the xAI API Grok.\\n• llm-anyscale-endpointssupports models hosted on the Anyscale Endpoints platform, including Llama 2 70B.\\n• llm-replicateadds support for remote models hosted on Replicate, including Llama 2 from Meta AI.\\n• llm-fireworkssupports models hosted by Fireworks AI.\\n• llm-openrouterprovides access to models hosted on OpenRouter.\\n• llm-cohere by Alistair Shepherd providescohere-generate and cohere-summarize API models, powered\\nby Cohere.\\n• llm-bedrockadds support for Nova by Amazon via Amazon Bedrock.\\n• llm-bedrock-anthropicby Sean Blakey adds support for Claude and Claude Instant by Anthropic via Amazon\\nBedrock.\\n• llm-bedrock-metaby Fabian Labat adds support for Llama 2 and Llama 3 by Meta via Amazon Bedrock.\\n• llm-togetheradds support for the Together AI extensive family of hosted openly licensed models.\\n• llm-deepseek adds support for the DeepSeek’s DeepSeek-Chat and DeepSeek-Coder models.\\n• llm-lambda-labs provides access to models hosted by Lambda Labs, including the Nous Hermes 3 series.\\n• llm-venice provides access to uncensored models hosted by privacy-focused Venice AI, including Llama 3.1\\n405B.\\nIfanAPImodelhostprovidesanOpenAI-compatibleAPIyoucanalsoconfigureLLMtotalktoitwithoutneedingan\\nextra plugin.\\nTools\\nThe following plugins add newtoolsthat can be used by models:\\n• llm-tools-simpleevalimplements simple expression support for things like mathematics.\\n• llm-tools-quickjs provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run\\nJavaScript code. The environment persists between calls so the model can set variables and build functions\\nand reuse them later on.\\n• llm-tools-sqlite can run read-only SQL queries against local SQLite databases.\\n• llm-tools-datasette can run SQL queries against a remote Datasette instance.\\n• llm-tools-exaby Dan Turkel can perform web searches and question-answering using exa.ai.\\n2.11. Plugins 79'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 85, 'page_label': '80'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n• llm-tools-rag by Dan Turkel can perform searches over your LLM embedding collections for simple RAG.\\nFragments and template loaders\\nLLM 0.24introduced support for plugins that define-f prefix:value or -t prefix:value custom loaders for\\nfragments and templates.\\n• llm-video-framesusesffmpegtoturnavideointoasequenceofJPEGframessuitableforfeedingintoavision\\nmodelthatdoesn’tsupportvideoinputs: llm -f video-frames:video.mp4 \\'describe the key scenes\\nin this video\\'.\\n• llm-templates-githubsupports loading templates shared on GitHub, e.g.llm -t gh:simonw/pelican-svg.\\n• llm-templates-fabric provides access to the Fabric collection of prompts: cat setup.py | llm -t\\nfabric:explain_code.\\n• llm-fragments-github can load entire GitHub repositories in a single operation:llm -f github:simonw/\\nfiles-to-prompt \\'explain this code\\'. It can also fetch issue threads as Markdown using llm -f\\nissue:https://github.com/simonw/llm-fragments-github/issues/3.\\n• llm-hacker-newsimports conversations from Hacker News as fragments:llm -f hn:43615912 \\'summary\\nwith illustrative direct quotes\\'.\\n• llm-fragments-pypiloadsPyPIpackages’descriptionandmetadataasfragments: llm -f pypi:ruff \"What\\nflake8 plugins does ruff re-implement?\".\\n• llm-fragments-pdfbyDanTurkelconvertsPDFstomarkdownwithPyMuPDF4LLMtouseasfragments: llm\\n-f pdf:something.pdf \"what\\'s this about?\".\\n• llm-fragments-site-textbyDanTurkelconvertswebsitestomarkdownwithTrafilaturatouseasfragments: llm\\n-f site:https://example.com \"summarize this\".\\n• llm-fragments-reader runs a URL theough the Jina Reader API: llm -f \\'reader:https://\\nsimonwillison.net/tags/jina/\\' summary.\\nEmbedding models\\nEmbedding modelsare models that can be used to generate and store embedding vectors for text.\\n• llm-sentence-transformers adds support for embeddings using the sentence-transformers library, which pro-\\nvides access to a wide range of embedding models.\\n• llm-clipprovidestheCLIPmodel,whichcanbeusedtoembedimagesandtextinthesamevectorspace,enabling\\ntext search against images. See Build an image search engine with llm-clip for more on this plugin.\\n• llm-embed-jina provides Jina AI’s 8K text embedding models.\\n• llm-embed-onnx provides seven embedding models that can be executed using the ONNX model framework.\\n80 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 86, 'page_label': '81'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nExtra commands\\n• llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so you\\ncan review it, edit it and then hit<enter>to execute orctrl+cto cancel.\\n• llm-cmd-compprovidesakeybindingforyourshellthatwilllaunchachattobuildthecommand. Whenready,\\nhit <enter>and it will go right back into your shell command line, so you can run it.\\n• llm-pythonadds allm python command for running a Python interpreter in the same virtual environment as\\nLLM. This is useful for debugging, and also provides a convenient way to interact with the LLMPython APIif\\nyou installed LLM using Homebrew orpipx.\\n• llm-cluster adds allm cluster command for calculating clusters for a collection of embeddings. Calculated\\nclusters can then be passed to a Large Language Model to generate a summary description.\\n• llm-jq lets you pipe in JSON data and a prompt describing ajq program, then executes the generated program\\nagainst the JSON.\\nJust for fun\\n• llm-markovaddsasimplemodelthatgeneratesoutputusingaMarkovchain. Thisexampleisusedinthetutorial\\nWriting a plugin to support a new model.\\n2.11.3 Plugin hooks\\nPlugins useplugin hooksto customize LLM’s behavior. These hooks are powered by the Pluggy plugin system.\\nEachplugincanimplementoneormorehooksusingthe@hookimpldecoratoragainstoneofthehookfunctionnames\\ndescribed on this page.\\nLLM imitates the Datasette plugin system. The Datasette plugin documentation describes how plugins work.\\nregister_commands(cli)\\nThis hook adds new commands to thellmCLI tool - for examplellm extra-command.\\nThis example plugin adds a newhello-worldcommand that prints “Hello world!”:\\nfrom llm import hookimpl\\nimport click\\n@hookimpl\\ndef register_commands(cli):\\n@cli.command(name=\"hello-world\")\\ndef hello_world():\\n\"Print hello world\"\\nclick.echo(\"Hello world!\")\\nThis new command will be added tollm --helpand can be run usingllm hello-world.\\n2.11. Plugins 81'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 87, 'page_label': '82'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nregister_models(register)\\nThis hook can be used to register one or more additional models.\\nimport llm\\n@llm.hookimpl\\ndef register_models(register):\\nregister(HelloWorld())\\nclass HelloWorld(llm.Model):\\nmodel_id = \"helloworld\"\\ndef execute(self, prompt, stream, response):\\nreturn [\"hello world\"]\\nIf your model includes an async version, you can register that too:\\nclass AsyncHelloWorld(llm.AsyncModel):\\nmodel_id = \"helloworld\"\\nasync def execute(self, prompt, stream, response):\\nreturn [\"hello world\"]\\n@llm.hookimpl\\ndef register_models(register):\\nregister(HelloWorld(), AsyncHelloWorld(), aliases=(\"hw\",))\\nThis demonstrates how to register a model with both sync and async versions, and how to specify an alias for that\\nmodel.\\nThe model plugin tutorialdescribes how to use this hook in detail. Asynchronous modelsare described here.\\nregister_embedding_models(register)\\nThis hook can be used to register one or more additional embedding models, as described inWriting plugins to add\\nnew embedding models.\\nimport llm\\n@llm.hookimpl\\ndef register_embedding_models(register):\\nregister(HelloWorld())\\nclass HelloWorld(llm.EmbeddingModel):\\nmodel_id = \"helloworld\"\\ndef embed_batch(self, items):\\nreturn [[1, 2, 3], [4, 5, 6]]\\n82 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 88, 'page_label': '83'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nregister_tools(register)\\nThis hook can register one or more tool functions for use with LLM. Seethe tools documentationfor more details.\\nThis example registers two tools:upperand count_character_in_word.\\nimport llm\\ndef upper(text: str) -> str:\\n\"\"\"Convert text to uppercase.\"\"\"\\nreturn text.upper()\\ndef count_char(text: str, character: str) -> int:\\n\"\"\"Count the number of occurrences of a character in a word.\"\"\"\\nreturn text.count(character)\\n@llm.hookimpl\\ndef register_tools(register):\\nregister(upper)\\n# Here the name= argument is used to specify a different name for the tool:\\nregister(count_char, name=\"count_character_in_word\")\\nTools can also be implemented as classes, as described inToolbox classesin the Python API documentation.\\nYou can register classes like theMemory example from there by passing the class (not an instance of the class) to\\nregister():\\nimport llm\\nclass Memory(llm.Toolbox):\\n...\\n@llm.hookimpl\\ndef register_tools(register):\\nregister(Memory)\\nOnce installed, this tool can be used like so:\\nllm chat -T Memory\\nIf a tool name starts with a capital letter it is assumed to be a toolbox class, not a regular tool function.\\nHere’s an example session with the Memory tool:\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> Remember my name is Henry\\nTool call: Memory_set({\\'key\\': \\'user_name\\', \\'value\\': \\'Henry\\'})\\nnull\\nGot it, Henry! I\\'ll remember your name. How can I assist you today?\\n(continues on next page)\\n2.11. Plugins 83'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 89, 'page_label': '84'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n> what keys are there?\\nTool call: Memory_keys({})\\n[\\n\"user_name\"\\n]\\nCurrently, there is one key stored: \"user_name\". Would you like to add or retrieve any␣\\n˓→information?\\n> read it\\nTool call: Memory_get({\\'key\\': \\'user_name\\'})\\nHenry\\nThe value stored under the key \"user_name\" is Henry. Is there anything else you\\'d like␣\\n˓→to do?\\n> add Barrett to it\\nTool call: Memory_append({\\'key\\': \\'user_name\\', \\'value\\': \\'Barrett\\'})\\nnull\\nI have added \"Barrett\" to the key \"user_name\". If you want, I can now show you the␣\\n˓→updated value.\\n> show value\\nTool call: Memory_get({\\'key\\': \\'user_name\\'})\\nHenry\\nBarrett\\nThe value stored under the key \"user_name\" is now:\\nHenry\\nBarrett\\nIs there anything else you would like to do?\\nregister_template_loaders(register)\\nPlugins can register newtemplate loadersusing theregister_template_loadershook.\\nTemplate loaders work with thellm -t prefix:name syntax. The prefix specifies the loader, then the registered\\nloaderfunctioniscalledwiththenameasanargument. Theloaderfunctionshouldreturnan llm.Template()object.\\nThis example plugin registersmy-prefixas a new template loader. Once installed it can be used like this:\\nllm -t my-prefix:my-template\\nHere’s the Python code:\\nimport llm\\n@llm.hookimpl\\ndef register_template_loaders(register):\\n(continues on next page)\\n84 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 90, 'page_label': '85'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nregister(\"my-prefix\", my_template_loader)\\ndef my_template_loader(template_path: str) -> llm.Template:\\n\"\"\"\\nDocumentation for the template loader goes here. It will be displayed\\nwhen users run the \\'llm templates loaders\\' command.\\n\"\"\"\\ntry:\\n# Your logic to fetch the template content\\n# This is just an example:\\nprompt = \"This is a sample prompt for {}\".format(template_path)\\nsystem = \"You are an assistant specialized in {}\".format(template_path)\\n# Return a Template object with the required fields\\nreturn llm.Template(\\nname=template_path,\\nprompt=prompt,\\nsystem=system,\\n)\\nexcept Exception as e:\\n# Raise a ValueError with a clear message if the template cannot be found\\nraise ValueError(f\"Template \\'{template_path}\\' could not be loaded: {str(e)}\")\\nThe llm.Templateclass has the following constructor:\\nclass llm.Template(*,name: str,prompt: str | None = None, system: str | None = None,attachments: List[str] |\\nNone = None, attachment_types: List[AttachmentType] | None = None,model: str | None =\\nNone,defaults: Dict[str, Any] | None = None,options: Dict[str, Any] | None = None,\\nextract: bool | None = None, extract_last: bool | None = None,schema_object: dict | None\\n= None,fragments: List[str] | None = None,system_fragments: List[str] | None = None,\\ntools: List[str] | None = None, functions: str | None = None)\\nThe loader function should raise aValueErrorif the template cannot be found or loaded correctly, providing a clear\\nerror message.\\nNote thatfunctions: provided by templates using this plugin hook will not be made available, to avoid the risk of\\nplugin hooks that load templates from remote sources introducing arbitrary code execution vulnerabilities.\\nregister_fragment_loaders(register)\\nPlugins can register new fragment loaders using theregister_template_loaders hook. These can then be used\\nwith thellm -f prefix:argumentsyntax.\\nFragment loader plugins differ from template loader plugins in that you can stack more than one fragment loader call\\ntogether in the same prompt.\\nAfragmentloadercanreturnoneormorestringfragmentsorattachments,oramixtureofthetwo. Thefragmentswill\\nbe concatenated together into the prompt string, while any attachments will be added to the list of attachments to be\\nsent to the model.\\nThe prefixspecifies the loader. Theargumentwill be passed to that registered callback..\\nThecallbackworksinaverysimilarwaytotemplateloaders,butreturnseitherasingle llm.Fragment,alistof llm.\\nFragmentobjects, a singlellm.Attachment, or a list that can mixllm.Attachmentand llm.Fragmentobjects.\\n2.11. Plugins 85'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 91, 'page_label': '86'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThellm.Fragmentconstructor takes a required string argument (the content of the fragment) and an optional second\\nsourceargument, which is a string that may be displayed as debug information. For files this is a path and for URLs\\nit is a URL. Your plugin can use anything you like for thesourcevalue.\\nSee the Python API documentation for attachmentsfor details of thellm.Attachmentclass.\\nHere is some example code:\\nimport llm\\n@llm.hookimpl\\ndef register_fragment_loaders(register):\\nregister(\"my-fragments\", my_fragment_loader)\\ndef my_fragment_loader(argument: str) -> llm.Fragment:\\n\"\"\"\\nDocumentation for the fragment loader goes here. It will be displayed\\nwhen users run the \\'llm fragments loaders\\' command.\\n\"\"\"\\ntry:\\nfragment = \"Fragment content for {}\".format(argument)\\nsource = \"my-fragments:{}\".format(argument)\\nreturn llm.Fragment(fragment, source)\\nexcept Exception as ex:\\n# Raise a ValueError with a clear message if the fragment cannot be loaded\\nraise ValueError(\\nf\"Fragment \\'my-fragments:{argument}\\' could not be loaded: {str(ex)}\"\\n)\\n# Or for the case where you want to return multiple fragments and attachments:\\ndef my_fragment_loader(argument: str) -> list[llm.Fragment]:\\n\"Docs go here.\"\\nreturn [\\nllm.Fragment(\"Fragment 1 content\", \"my-fragments:{argument}\"),\\nllm.Fragment(\"Fragment 2 content\", \"my-fragments:{argument}\"),\\nllm.Attachment(path=\"/path/to/image.png\"),\\n]\\nA plugin like this one can be called like so:\\nllm -f my-fragments:argument\\nIf multiple fragments are returned they will be used as if the user passed multiple-f Xarguments to the command.\\nMultiple fragments are particularly useful for things like plugins that return every file in a directory. If these were\\nconcatenated together by the plugin, a change to a single file would invalidate the de-duplicatino cache for that whole\\nfragment. Giving each file its own fragment means we can avoid storing multiple copies of that full collection if only\\na single file has changed.\\n86 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 92, 'page_label': '87'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.11.4 Developing a model plugin\\nThis tutorial will walk you through developing a new plugin for LLM that adds support for a new Large Language\\nModel.\\nWe will be developing a plugin that implements a simple Markov chain to generate words based on an input string.\\nMarkovchainsarenottechnicallylargelanguagemodels,buttheyprovideausefulexercisefordemonstratinghowthe\\nLLM tool can be extended through plugins.\\nThe initial structure of the plugin\\nFirst create a new directory with the name of your plugin - it should be called something likellm-markov.\\nmkdir llm-markov\\ncd llm-markov\\nIn that directory create a file calledllm_markov.pycontaining this:\\nimport llm\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\ndef execute(self, prompt, stream, response, conversation):\\nreturn [\"hello world\"]\\nThe def register_models() function here is called by the plugin system (thanks to the@hookimpl decorator). It\\nuses theregister()function passed to it to register an instance of the new model.\\nThe Markov class implements the model. It sets amodel_id - an identifier that can be passed tollm -m in order to\\nidentify the model to be executed.\\nThe logic for executing the model goes in theexecute()method. We’ll extend this to do something more useful in a\\nlater step.\\nNext, create apyproject.tomlfile. This is necessary to tell LLM how to load your plugin:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThis is the simplest possible configuration. It defines a plugin name and provides an entry point forllmtelling it how\\nto load the plugin.\\nIf you are comfortable with Python virtual environments you can create one now for your project, activate it and run\\npip install llmbefore the next step.\\nIf you aren’t familiar with virtual environments, don’t worry: you can develop plugins without them. You’ll need to\\nhave LLM installed using Homebrew orpipxor one of the other installation options.\\n2.11. Plugins 87'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 93, 'page_label': '88'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nInstalling your plugin to try it out\\nHaving created a directory with apyproject.tomlfile and anllm_markov.pyfile, you can install your plugin into\\nLLM by running this from inside yourllm-markovdirectory:\\nllm install -e .\\nThe -estands for “editable” - it means you’ll be able to make further changes to thellm_markov.pyfile that will be\\nreflected without you having to reinstall the plugin.\\nThe .means the current directory. You can also install editable plugins by passing a path to their directory this:\\nllm install -e path/to/llm-markov\\nTo confirm that your plugin has installed correctly, run this command:\\nllm plugins\\nThe output should look like this:\\n[\\n{\\n\"name\": \"llm-markov\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\": \"0.1\"\\n},\\n{\\n\"name\": \"llm.default_plugins.openai_models\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n]\\n}\\n]\\nThis command lists default plugins that are included with LLM as well as new plugins that have been installed.\\nNow let’s try the plugin by running a prompt through it:\\nllm -m markov \"the cat sat on the mat\"\\nIt outputs:\\nhello world\\nNext, we’ll make it execute and return the results of a Markov chain.\\n88 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 94, 'page_label': '89'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nBuilding the Markov chain\\nMarkov chains can be thought of as the simplest possible example of a generative language model. They work by\\nbuilding an index of words that have been seen following other words.\\nHere’s what that index looks like for the phrase “the cat sat on the mat”\\n{\\n\"the\": [\"cat\", \"mat\"],\\n\"cat\": [\"sat\"],\\n\"sat\": [\"on\"],\\n\"on\": [\"the\"]\\n}\\nHere’s a Python function that builds that data structure from a text input:\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\nWe can try that out by pasting it into the interactive Python interpreter and running this:\\n>>> transitions = build_markov_table(\"the cat sat on the mat\")\\n>>> transitions\\n{\\'the\\': [\\'cat\\', \\'mat\\'], \\'cat\\': [\\'sat\\'], \\'sat\\': [\\'on\\'], \\'on\\': [\\'the\\']}\\nExecuting the Markov chain\\nTo execute the model, we start with a word. We look at the options for words that might come next and pick one of\\nthose at random. Then we repeat that process until we have produced the desired number of output words.\\nSomewordsmightnothaveanyfollowingwordsfromourtrainingsentence. Forourimplementationwewillfallback\\non picking a random word from our collection.\\nWe will implement this as a Python generator, using the yield keyword to produce each token:\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nIf you aren’t familiar with generators, the above code could also be implemented like this - creating a Python list and\\nreturning it at the end of the function:\\n2.11. Plugins 89'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 95, 'page_label': '90'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\ndef generate_list(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\noutput = []\\nfor i in range(length):\\noutput.append(next_word)\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nreturn output\\nYou can try out thegenerate()function like this:\\nlookup = build_markov_table(\"the cat sat on the mat\")\\nfor word in generate(transitions, 20):\\nprint(word)\\nOr you can generate a full string sentence with it like this:\\nsentence = \" \".join(generate(transitions, 20))\\nAdding that to the plugin\\nOur execute()method from earlier currently returns the list[\"hello world\"].\\nUpdate that to use our new Markov chain generator instead. Here’s the full text of the newllm_markov.pyfile:\\nimport llm\\nimport random\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\n(continues on next page)\\n90 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 96, 'page_label': '91'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nfor word in generate(transitions, 20):\\nyield word + \\' \\'\\nTheexecute()methodcanaccessthetextpromptthattheuserprovidedusing prompt.prompt-promptisa Prompt\\nobject that might include other more advanced input details as well.\\nNow when you run this you should see the output of the Markov chain!\\nllm -m markov \"the cat sat on the mat\"\\nthe mat the cat sat on the cat sat on the mat cat sat on the mat cat sat on\\nUnderstanding execute()\\nThe full signature of theexecute()method is:\\ndef execute(self, prompt, stream, response, conversation):\\nThe prompt argument is aPrompt object that contains the text that the user provided, the system prompt and the\\nprovided options.\\nstreamis a boolean that says if the model is being run in streaming mode.\\nresponse is theResponse object that is being created by the model. This is provided so you can write additional\\ninformation toresponse.response_json, which may be logged to the database.\\nconversation is theConversation that the prompt is a part of - orNone if no conversation was provided. Some\\nmodels may useconversation.responses to access previous prompts and responses in the conversation and use\\nthem to construct a call to the LLM that includes previous context.\\nPrompts and responses are logged to the database\\nThe prompt and the response will be logged to a SQLite database automatically by LLM. You can see the single most\\nrecent addition to the logs using:\\nllm logs -n 1\\nThe output should look something like this:\\n[\\n{\\n\"id\": \"01h52s4yez2bd1qk2deq49wk8h\",\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {},\\n\"response\": \"on the cat sat on the cat sat on the mat cat sat on the cat sat on the␣\\n˓→cat \",\\n(continues on next page)\\n2.11. Plugins 91'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 97, 'page_label': '92'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"response_json\": null,\\n\"conversation_id\": \"01h52s4yey7zc5rjmczy3ft75g\",\\n\"duration_ms\": 0,\\n\"datetime_utc\": \"2023-07-11T15:29:34.685868\",\\n\"conversation_name\": \"the cat sat on the mat\",\\n\"conversation_model\": \"markov\"\\n}\\n]\\nPlugins can log additional information to the database by assigning a dictionary to theresponse.response_json\\nproperty during theexecute()method.\\nHere’s how to include that fulltransitionstable in theresponse_jsonin the log:\\ndef execute(self, prompt, stream, response, conversation):\\ntext = self.prompt.prompt\\ntransitions = build_markov_table(text)\\nfor word in generate(transitions, 20):\\nyield word + \\' \\'\\nresponse.response_json = {\"transitions\": transitions}\\nNow when you run the logs command you’ll see that too:\\nllm logs -n 1\\n[\\n{\\n\"id\": 623,\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {},\\n\"response\": \"on the mat the cat sat on the cat sat on the mat sat on the cat sat on␣\\n˓→the \",\\n\"response_json\": {\\n\"transitions\": {\\n\"the\": [\\n\"cat\",\\n\"mat\"\\n],\\n\"cat\": [\\n\"sat\"\\n],\\n\"sat\": [\\n\"on\"\\n],\\n\"on\": [\\n\"the\"\\n]\\n}\\n},\\n\"reply_to_id\": null,\\n(continues on next page)\\n92 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 98, 'page_label': '93'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"chat_id\": null,\\n\"duration_ms\": 0,\\n\"datetime_utc\": \"2023-07-06T01:34:45.376637\"\\n}\\n]\\nIn this particular case this isn’t a great idea here though: thetransitionstable is duplicate information, since it can\\nbe reproduced from the input data - and it can get really large for longer prompts.\\nAdding options\\nLLM models can take options. For large language models these can be things liketemperatureor top_k.\\nOptions are passed using the-o/--optioncommand line parameters, for example:\\nllm -m gpt4 \"ten pet pelican names\" -o temperature 1.5\\nWe’re going to add two options to our Markov chain model:\\n• length: Number of words to generate\\n• delay: a floating point number of Delay in between output token\\nThedelaytokenwillletussimulateastreaminglanguagemodel,wheretokenstaketimetogenerateandarereturned\\nby theexecute()function as they become ready.\\nOptions are defined using an inner class on the model, calledOptions. It should extend thellm.Optionsclass.\\nFirst, add this import to the top of yourllm_markov.pyfile:\\nfrom typing import Optional\\nThen add thisOptionsclass to your model:\\nclass Markov(Model):\\nmodel_id = \"markov\"\\nclass Options(llm.Options):\\nlength: Optional[int] = None\\ndelay: Optional[float] = None\\nLet’s add extra validation rules to our options. Length must be at least 2. Duration must be between 0 and 10.\\nThe Optionsclass uses Pydantic 2, which can support all sorts of advanced validation rules.\\nWe can also add inline documentation, which can then be displayed by thellm models --optionscommand.\\nAdd these imports to the top ofllm_markov.py:\\nfrom pydantic import field_validator, Field\\nWe can now add Pydantic field validators for our two new rules, plus inline documentation:\\nclass Options(llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\",\\ndefault=None\\n(continues on next page)\\n2.11. Plugins 93'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 99, 'page_label': '94'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\",\\ndefault=None\\n)\\n@field_validator(\"length\")\\ndef validate_length(cls, length):\\nif length is None:\\nreturn None\\nif length < 2:\\nraise ValueError(\"length must be >= 2\")\\nreturn length\\n@field_validator(\"delay\")\\ndef validate_delay(cls, delay):\\nif delay is None:\\nreturn None\\nif not 0 <= delay <= 10:\\nraise ValueError(\"delay must be between 0 and 10\")\\nreturn delay\\nLets test our options validation:\\nllm -m markov \"the cat sat on the mat\" -o length -1\\nError: length\\nValue error, length must be >= 2\\nNext, we will modify ourexecute()method to handle those options. Add this to the beginning ofllm_markov.py:\\nimport time\\nThen replace theexecute()method with this one:\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or 20\\nfor word in generate(transitions, length):\\nyield word + \\' \\'\\nif prompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nAddcan_stream = Trueto the top of theMarkovmodel class, on the line below`model_id = “markov”. This tells\\nLLM that the model is able to stream content to the console.\\nThe fullllm_markov.pyfile should now look like this:\\nimport llm\\nimport random\\nimport time\\nfrom typing import Optional\\n(continues on next page)\\n94 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 100, 'page_label': '95'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nfrom pydantic import field_validator, Field\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\ncan_stream = True\\nclass Options(llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\", default=None\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\", default=None\\n)\\n@field_validator(\"length\")\\ndef validate_length(cls, length):\\nif length is None:\\nreturn None\\nif length < 2:\\nraise ValueError(\"length must be >= 2\")\\nreturn length\\n@field_validator(\"delay\")\\ndef validate_delay(cls, delay):\\nif delay is None:\\nreturn None\\n(continues on next page)\\n2.11. Plugins 95'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 101, 'page_label': '96'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif not 0 <= delay <= 10:\\nraise ValueError(\"delay must be between 0 and 10\")\\nreturn delay\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or 20\\nfor word in generate(transitions, length):\\nyield word + \" \"\\nif prompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nNow we can request a 20 word completion with a 0.1s delay between tokens like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1\\nLLMprovidesa --no-streamoptionuserscanusetoturnoffstreaming. UsingthatoptioncausesLLMtogatherthe\\nresponse from the stream and then return it to the console in one block. You can try that like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1 --no-stream\\nIn this case it will still delay for 2s total while it gathers the tokens, then output them all at once.\\nThat --no-stream option causes thestream argument passed toexecute() to be false. Yourexecute() method\\ncan then behave differently depending on whether it is streaming or not.\\nOptions are also logged to the database. You can see those here:\\nllm logs -n 1\\n[\\n{\\n\"id\": 636,\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {\\n\"length\": 20,\\n\"delay\": 0.1\\n},\\n\"response\": \"the mat on the mat on the cat sat on the mat sat on the mat cat sat on␣\\n˓→the \",\\n\"response_json\": null,\\n\"reply_to_id\": null,\\n\"chat_id\": null,\\n\"duration_ms\": 2063,\\n\"datetime_utc\": \"2023-07-07T03:02:28.232970\"\\n}\\n]\\n96 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 102, 'page_label': '97'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nDistributing your plugin\\nThere are many different options for distributing your new plugin so other people can try it out.\\nYoucancreateadownloadablewheelor .zipor .tar.gzfiles, orsharethepluginthroughGitHubGistsorreposito-\\nries.\\nYou can also publish your plugin to PyPI, the Python Package Index.\\nWheels and sdist packages\\nTheeasiestoptionistoproduceadistributablepackageistousethe buildcommand. First,installthe buildpackage\\nby running this:\\npython -m pip install build\\nThen runbuildin your plugin directory to create the packages:\\npython -m build\\nThis will create two files:dist/llm-markov-0.1.tar.gzand dist/llm-markov-0.1-py3-none-any.whl.\\nEither of these files can be used to install the plugin:\\nllm install dist/llm_markov-0.1-py3-none-any.whl\\nIf you host this file somewhere online other people will be able to install it usingpip install against the URL to\\nyour package:\\nllm install 'https://.../llm_markov-0.1-py3-none-any.whl'\\nYou can run the following command at any time to uninstall your plugin, which is useful for testing out different\\ninstallation methods:\\nllm uninstall llm-markov -y\\nGitHub Gists\\nA neat quick option for distributing a simple plugin is to host it in a GitHub Gist. These are available for free with a\\nGitHub account, and can be public or private. Gists can contain multiple files but don’t support directory structures -\\nwhich is OK, because our plugin is just two files,pyproject.tomland llm_markov.py.\\nHere’s an example Gist I created for this tutorial:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d\\nYoucanturnaGistintoaninstallable .zipURLbyright-clickingonthe“DownloadZIP”buttonandselecting“Copy\\nLink”. Here’s that link for my example Gist:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\ncc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip\\nThe plugin can be installed using thellm installcommand like this:\\nllm install 'https://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\n˓→cc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip'\\n2.11. Plugins 97\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 103, 'page_label': '98'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nGitHub repositories\\nThesametrickworksforregularGitHubrepositoriesaswell: the“DownloadZIP”buttoncanbefoundbyclickingthe\\ngreen “Code” button at the top of the repository. The URL which that provides can then be used to install the plugin\\nthat lives in that repository.\\nPublishing plugins to PyPI\\nThe Python Package Index (PyPI) is the official repository for Python packages. You can upload your plugin to PyPI\\nand reserve a name for it - once you have done that, anyone will be able to install your plugin usingllm install\\n<name>.\\nFollow these instructions to publish a package to PyPI. The short version:\\npython -m pip install twine\\npython -m twine upload dist/*\\nYou will need an account on PyPI, then you can enter your username and password - or create a token in the PyPI\\nsettings and use__token__as the username and the token as the password.\\nAdding metadata\\nBeforeuploadingapackagetoPyPIit’sagoodideatoadddocumentationandexpand pyproject.tomlwithadditional\\nmetadata.\\nCreate aREADME.md file in the root of your plugin directory with instructions about how to install, configure and use\\nyour plugin.\\nYou can then replacepyproject.tomlwith something like this:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\ndescription = \"Plugin for LLM adding a Markov chain generating model\"\\nreadme = \"README.md\"\\nauthors = [{name = \"Simon Willison\"}]\\nlicense = {text = \"Apache-2.0\"}\\nclassifiers = [\\n\"License :: OSI Approved :: Apache Software License\"\\n]\\ndependencies = [\\n\"llm\"\\n]\\nrequires-python = \">3.7\"\\n[project.urls]\\nHomepage = \"https://github.com/simonw/llm-markov\"\\nChangelog = \"https://github.com/simonw/llm-markov/releases\"\\nIssues = \"https://github.com/simonw/llm-markov/issues\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThis will pull in your README to be displayed as part of your project’s listing page on PyPI.\\n98 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 104, 'page_label': '99'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nIt addsllmas a dependency, ensuring it will be installed if someone tries to install your plugin package without it.\\nIt adds some links to useful pages (you can drop theproject.urls section if those links are not useful for your\\nproject).\\nYoushoulddropa LICENSEfileintotheGitHubrepositoryforyourpackageaswell. IliketousetheApache2license\\nlike this.\\nWhat to do if it breaks\\nSometimes you may make a change to your plugin that causes it to break, preventingllm from starting. For example\\nyou may see an error like this one:\\n$ llm \\'hi\\'\\nTraceback (most recent call last):\\n...\\nFile llm-markov/llm_markov.py\", line 10\\nregister(Markov()):\\n^\\nSyntaxError: invalid syntax\\nYou may find that you are unable to uninstall the plugin usingllm uninstall llm-markov because the command\\nitself fails with the same error.\\nShould this happen, you can uninstall the plugin after first disabling it using theLLM_LOAD_PLUGINS environment\\nvariable like this:\\nLLM_LOAD_PLUGINS=\\'\\' llm uninstall llm-markov\\n2.11.5 Advanced model plugins\\nThe model plugin tutorialcovers the basics of developing a plugin that adds support for a new model. This document\\ncovers more advanced topics.\\nFeatures to consider for your model plugin include:\\n• Accepting API keysusing the standardmechanism that incorporatesllm keys set, environment variables and\\nsupport for passing an explicit key to the model.\\n• Including support forAsync modelsthat can be used with Python’sasynciolibrary.\\n• Support forstructured outputusing JSON schemas.\\n• Support fortools.\\n• Handlingattachments(images, audio and more) for multi-modal models.\\n• Trackingtoken usagefor models that charge by the token.\\n2.11. Plugins 99'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 105, 'page_label': '100'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nTip: lazily load expensive dependencies\\nIf your plugin depends on an expensive library such as PyTorch you should avoid importing that dependency (or a\\ndependency that uses that dependency) at the top level of your module. Expensive imports in plugins mean that even\\nsimple commands likellm --helpcan take a long time to run.\\nInstead, move those imports to inside the methods that need them. Here’s an example change to llm-sentence-\\ntransformers that shaved 1.8 seconds off the time it took to runllm --help!\\nModels that accept API keys\\nModels that call out to API providers such as OpenAI, Anthropic or Google Gemini usually require an API key.\\nLLM’s API key management mechanismis described here.\\nIfyourpluginrequiresanAPIkeyyoushouldsubclassthe llm.KeyModelclassinsteadofthe llm.Modelclass. Start\\nyour model definition like this:\\nimport llm\\nclass HostedModel(llm.KeyModel):\\nneeds_key = \"hosted\" # Required\\nkey_env_var = \"HOSTED_API_KEY\" # Optional\\nThistellsLLMthatyourmodelrequiresanAPIkey,whichmaybesavedinthekeyregistryunderthekeyname hosted\\nor might also be provided as theHOSTED_API_KEYenvironment variable.\\nThen when you define yourexecute()method it should take an extrakey=parameter like this:\\ndef execute(self, prompt, stream, response, conversation, key=None):\\n# key= here will be the API key to use\\nLLM will pass in the key from the environment variable, key registry or that has been passed to LLM as the--key\\ncommand-line option or themodel.prompt(..., key=)parameter.\\nAsync models\\nPlugins can optionally provide an asynchronous version of their model, suitable for use with Python asyncio. This is\\nparticularly useful for remote models accessible by an HTTP API.\\nThe async version of a model subclassesllm.AsyncModelinstead ofllm.Model. It must implement anasync def\\nexecute()async generator method instead ofdef execute().\\nThis example shows a subset of the OpenAI default plugin illustrating how this method might work:\\nfrom typing import AsyncGenerator\\nimport llm\\nclass MyAsyncModel(llm.AsyncModel):\\n# This can duplicate the model_id of the sync model:\\nmodel_id = \"my-model-id\"\\nasync def execute(\\nself, prompt, stream, response, conversation=None\\n) -> AsyncGenerator[str, None]:\\n(continues on next page)\\n100 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 106, 'page_label': '101'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif stream:\\ncompletion = await client.chat.completions.create(\\nmodel=self.model_id,\\nmessages=messages,\\nstream=True,\\n)\\nasync for chunk in completion:\\nyield chunk.choices[0].delta.content\\nelse:\\ncompletion = await client.chat.completions.create(\\nmodel=self.model_name or self.model_id,\\nmessages=messages,\\nstream=False,\\n)\\nif completion.choices[0].message.content is not None:\\nyield completion.choices[0].message.content\\nIfyourmodeltakesanAPIkeyyoushouldinsteadsubclass llm.AsyncKeyModelandhavea key=parameteronyour\\n.execute()method:\\nclass MyAsyncModel(llm.AsyncKeyModel):\\n...\\nasync def execute(\\nself, prompt, stream, response, conversation=None, key=None\\n) -> AsyncGenerator[str, None]:\\nThisasyncmodelinstanceshouldthenbepassedtothe register()methodinthe register_models()pluginhook:\\n@hookimpl\\ndef register_models(register):\\nregister(\\nMyModel(), MyAsyncModel(), aliases=(\"my-model-aliases\",)\\n)\\nSupporting schemas\\nIf your model supportsstructured outputagainst a defined JSON schema you can implement support by first adding\\nsupports_schema = Trueto the class:\\nclass MyModel(llm.KeyModel):\\n...\\nsupport_schema = True\\nAnd then adding code to your.execute() method that checks forprompt.schema and, if it is present, uses that to\\nprompt the model.\\nprompt.schemawillalwaysbeaPythondictionaryrepresentingaJSONschema,eveniftheuserpassedinaPydantic\\nmodel class.\\nCheck the llm-gemini and llm-anthropic plugins for example of this pattern in action.\\n2.11. Plugins 101'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 107, 'page_label': '102'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nSupporting tools\\nAddingtools supportinvolves several steps:\\n1. Addsupports_tools = Trueto your model class.\\n2. If prompt.toolsis populated, turn that list ofllm.Toolobjects into the correct format for your model.\\n3. Look out for requests to call tools in the responses from your model. Callresponse.add_tool_call(llm.\\nToolCall(...))foreachofthose. Thisshouldworkforstreamingandnon-streamingandasyncandnon-async\\ncases.\\n4. Ifyourprompthasa prompt.tool_resultslist,passtheinformationfromthose llm.ToolResultobjectsto\\nyour model.\\n5. Include prompt.tools and prompt.tool_results and tool calls from response.\\ntool_calls_or_raise()in the conversation history constructed by your plugin.\\n6. Make sure your code is OK with prompts that do not haveprompt.prompt set to a value, since they may be\\ncarrying exclusively the results of a tool call.\\nThis commit to llm-gemini implementing tools helps demonstrate what this looks like for a real plugin.\\nHere are the relevant dataclasses:\\nclass llm.Tool(name: str,description: Optional[str] = None, input_schema: Dict = <factory>,implementation:\\nOptional[Callable] = None,plugin: Optional[str] = None)\\nclass llm.ToolCall(name: str, arguments: dict,tool_call_id: str | None = None)\\nclass llm.ToolResult(name: str,output: str,attachments: List[llm.models.Attachment] = <factory>,\\ntool_call_id: Optional[str] = None, instance: Optional[llm.models.Toolbox] = None,\\nexception: Optional[Exception] = None)\\nAttachments for multi-modal models\\nModels such as GPT-4o, Claude 3.5 Sonnet and Google’s Gemini 1.5 are multi-modal: they accept input in the form\\nof images and maybe even audio, video and other formats.\\nLLM calls theseattachments. Models can specify the types of attachments they accept and then implement special\\ncode in the.execute()method to handle them.\\nSee the Python attachments documentationfor details on using attachments in the Python API.\\nSpecifying attachment types\\nA Modelsubclass can list the types of attachments it accepts by defining aattachment_typesclass attribute:\\nclass NewModel(llm.Model):\\nmodel_id = \"new-model\"\\nattachment_types = {\\n\"image/png\",\\n\"image/jpeg\",\\n\"image/webp\",\\n\"image/gif\",\\n}\\n102 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 108, 'page_label': '103'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThese content types are detected when an attachment is passed to LLM usingllm -a filename, or can be specified\\nby the user using the--attachment-type filename image/pngoption.\\nNote: MP3 files will have their attachment type detected asaudio/mpeg, notaudio/mp3.\\nLLMwillusethe attachment_typesattributetovalidatethatprovidedattachmentsshouldbeacceptedbeforepassing\\nthem to the model.\\nHandling attachments\\nThe prompt object passed to the execute() method will have an attachments attribute containing a list of\\nAttachmentobjects provided by the user.\\nAn Attachmentinstance has the following properties:\\n• url (str): The URL of the attachment, if it was provided as a URL\\n• path (str): The resolved file path of the attachment, if it was provided as a file\\n• type (str): The content type of the attachment, if it was provided\\n• content (bytes): The binary content of the attachment, if it was provided\\nGenerally only one ofurl,pathor contentwill be set.\\nYou should usually access the type and the content through one of these methods:\\n• attachment.resolve_type() -> str: Returns thetype if it is available, otherwise attempts to guess the\\ntype by looking at the first few bytes of content\\n• attachment.content_bytes() -> bytes: Returnsthebinarycontent,whichitmayneedtoreadfromafile\\nor fetch from a URL\\n• attachment.base64_content() -> str: Returns that content as a base64-encoded string\\nA id() method returns a database ID for this content, which is either a SHA256 hash of the binary content or, in the\\ncase of attachments hosted at an external URL, a hash of{\"url\": url} instead. This is an implementation detail\\nwhich you should not need to access directly.\\nNote that it’s possible for a prompt with an attachments to not include a text prompt at all, in which caseprompt.\\npromptwill beNone.\\nHere’s how the OpenAI plugin handles attachments, including the case where noprompt.promptwas provided:\\nif not prompt.attachments:\\nmessages.append({\"role\": \"user\", \"content\": prompt.prompt})\\nelse:\\nattachment_message = []\\nif prompt.prompt:\\nattachment_message.append({\"type\": \"text\", \"text\": prompt.prompt})\\nfor attachment in prompt.attachments:\\nattachment_message.append(_attachment(attachment))\\nmessages.append({\"role\": \"user\", \"content\": attachment_message})\\n# And the code for creating the attachment message\\ndef _attachment(attachment):\\nurl = attachment.url\\nbase64_content = \"\"\\nif not url or attachment.resolve_type().startswith(\"audio/\"):\\n(continues on next page)\\n2.11. Plugins 103'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 109, 'page_label': '104'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbase64_content = attachment.base64_content()\\nurl = f\"data:{attachment.resolve_type()};base64,{base64_content}\"\\nif attachment.resolve_type().startswith(\"image/\"):\\nreturn {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\\nelse:\\nformat_ = \"wav\" if attachment.resolve_type() == \"audio/wav\" else \"mp3\"\\nreturn {\\n\"type\": \"input_audio\",\\n\"input_audio\": {\\n\"data\": base64_content,\\n\"format\": format_,\\n},\\n}\\nAsyoucansee,ituses attachment.urlifthatisavailableandotherwisefallsbacktousingthe base64_content()\\nmethod to embed the image directly in the JSON sent to the API. For the OpenAI API audio attachments are always\\nincluded as base64-encoded strings.\\nAttachments from previous conversations\\nModels that implement the ability to continue a conversation can reconstruct the previous message JSON using the\\nresponse.attachmentsattribute.\\nHere’s how the OpenAI plugin does that:\\nfor prev_response in conversation.responses:\\nif prev_response.attachments:\\nattachment_message = []\\nif prev_response.prompt.prompt:\\nattachment_message.append(\\n{\"type\": \"text\", \"text\": prev_response.prompt.prompt}\\n)\\nfor attachment in prev_response.attachments:\\nattachment_message.append(_attachment(attachment))\\nmessages.append({\"role\": \"user\", \"content\": attachment_message})\\nelse:\\nmessages.append(\\n{\"role\": \"user\", \"content\": prev_response.prompt.prompt}\\n)\\nmessages.append({\"role\": \"assistant\", \"content\": prev_response.text_or_raise()})\\nThe response.text_or_raise() method used there will return the text from the response or raise aValueError\\nexception if the response is anAsyncResponseinstance that has not yet been fully resolved.\\nThis is a slightly weird hack to work around the common need to share logic for building up themessageslist across\\nboth sync and async models.\\n104 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 110, 'page_label': '105'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nTracking token usage\\nModelsthatchargebythetokenshouldtrackthenumberoftokensusedbyeachprompt. The response.set_usage()\\nmethod can be used to record the number of tokens used by a response - these will then be made available through the\\nPython API and logged to the SQLite database for command-line users.\\nresponsehere is the response object that is passed to.execute()as an argument.\\nCall response.set_usage() at the end of your.execute() method. It accepts keyword arguments input=,\\noutput= and details= - all three are optional.input and output should be integers, anddetails should be a\\ndictionary that provides additional information beyond the input and output token counts.\\nThis example logs 15 input tokens, 340 output tokens and notes that 37 tokens were cached:\\nresponse.set_usage(input=15, output=340, details={\"cached\": 37})\\nTracking resolved model names\\nIn some cases the model ID that the user requested may not be the exact model that is executed. Many providers have\\na model-latestalias which may execute different models over time.\\nIfthoseAPIsreturnthe realmodelIDthatwasused,yourplugincanrecordthatinthe resources.resolved_model\\ncolumn in the logs by calling this method and passing the string representing the resolved, final model ID:\\nresponse.set_resolved_model(resolved_model_id)\\nThis string will be recorded in the database and shown in the output ofllm logsand llm logs --json.\\nLLM_RAISE_ERRORS\\nWhile working on a plugin it can be useful to request that errors are raised instead of being caught and logged, so you\\ncan access them from the Python debugger.\\nSet theLLM_RAISE_ERRORSenvironment variable to enable this behavior, then runllmlike this:\\nLLM_RAISE_ERRORS=1 python -i -m llm ...\\nThe-ioptionmeans Pythonwill dropinto aninteractive shellif anerror occurs. You canthen opena debuggerat the\\nmost recent error using:\\nimport pdb; pdb.pm()\\n2.11.6 Utility functions for plugins\\nLLM provides some utility functions that may be useful to plugins.\\n2.11. Plugins 105'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 111, 'page_label': '106'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm.get_key()\\nThismethodcanbeusedtolookupsecretsthatusershavestoredusingthe llmkeysset command. Ifyourpluginneeds\\nto access an API key or other secret this can be a convenient way to provide that.\\nThis returns either a string containing the key orNoneif the key could not be resolved.\\nUse thealias=\"name\"option to retrieve the key set with that alias:\\ngithub_key = llm.get_key(alias=\"github\")\\nYoucanalsoadd env=\"ENV_VAR\"tofallbacktolookinginthatenvironmentvariableifthekeyhasnotbeenconfigured:\\ngithub_key = llm.get_key(alias=\"github\", env=\"GITHUB_TOKEN\")\\nIn some cases you may allow users to provide a key as input, where they could input either the key itself or specify an\\nalias to lookup inkeys.json. Use theinput=parameter for that:\\ngithub_key = llm.get_key(input=input_from_user, alias=\"github\", env=\"GITHUB_TOKEN\")\\nAn previous version of function used positional arguments in a confusing order. These are still supported but the new\\nkeyword arguments are recommended as a better way to usellm.get_key()going forward.\\nllm.user_dir()\\nLLM stores various pieces of logging and configuration data in a directory on the user’s machine.\\nOn macOS this directory is~/Library/Application Support/io.datasette.llm, but this will differ on other\\noperating systems.\\nThe llm.user_dir()function returns the path to this directory as apathlib.Pathobject, after creating that direc-\\ntory if it does not yet exist.\\nPlugins can use this to store their own data in a subdirectory of this directory.\\nimport llm\\nuser_dir = llm.user_dir()\\nplugin_dir = data_path = user_dir / \"my-plugin\"\\nplugin_dir.mkdir(exist_ok=True)\\ndata_path = plugin_dir / \"plugin-data.db\"\\nllm.ModelError\\nIf your model encounters an error that should be reported to the user you can raise this exception. For example:\\nimport llm\\nraise ModelError(\"MPT model not installed - try running \\'llm mpt30b download\\'\")\\nThis will be caught by the CLI layer and displayed to the user as an error message.\\n106 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 112, 'page_label': '107'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nResponse.fake()\\nWhen writing tests for a model it can be useful to generate fake response objects, for example in this test from llm-\\nmpt30b:\\ndef test_build_prompt_conversation():\\nmodel = llm.get_model(\"mpt\")\\nconversation = model.conversation()\\nconversation.responses = [\\nllm.Response.fake(model, \"prompt 1\", \"system 1\", \"response 1\"),\\nllm.Response.fake(model, \"prompt 2\", None, \"response 2\"),\\nllm.Response.fake(model, \"prompt 3\", None, \"response 3\"),\\n]\\nlines = model.build_prompt(llm.Prompt(\"prompt 4\", model), conversation)\\nassert lines == [\\n\"<|im_start|>system\\\\system 1<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 1<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 1<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 2<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 2<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 3<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 3<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 4<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\n\",\\n]\\nThe signature ofllm.Response.fake()is:\\ndef fake(cls, model: Model, prompt: str, system: str, response: str):\\n2.12 Python API\\nLLM provides a Python API for executing prompts, in addition to the command-line interface.\\nUnderstanding this API is also important for writingPlugins.\\n2.12.1 Basic prompt execution\\nTo run a prompt against thegpt-4o-minimodel, run this:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\n# key= is optional, you can configure the key in other ways\\nresponse = model.prompt(\\n\"Five surprising names for a pet pelican\",\\nkey=\"sk-...\"\\n)\\nprint(response.text())\\nNote that the prompt will not be evaluated until you call thatresponse.text()method - a form of lazy loading.\\nIf you inspect the response before it has been evaluated it will look like this:\\n2.12. Python API 107'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 113, 'page_label': '108'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n<Response prompt=\\'Your prompt\\' text=\\'... not yet done ...\\'>\\nThe llm.get_model() function accepts model IDs or aliases. You can also omit it to use the currently configured\\ndefault model, which isgpt-4o-miniif you have not changed the default.\\nIn this example the key is set by Python code. You can also provide the key using theOPENAI_API_KEYenvironment\\nvariable, or use thellm keys set openaicommand to store it in akeys.jsonfile, seeAPI key management.\\nThe __str__()method ofresponsealso returns the text of the response, so you can do this instead:\\nprint(llm.get_model().prompt(\"Five surprising names for a pet pelican\"))\\nYou can run this command to see a list of available models and their aliases:\\nllm models\\nIf you have set aOPENAI_API_KEYenvironment variable you can omit themodel.key = line.\\nCalling llm.get_model()with an invalid model ID will raise allm.UnknownModelErrorexception.\\nSystem prompts\\nFor models that accept a system prompt, pass it assystem=\"...\":\\nresponse = model.prompt(\\n\"Five surprising names for a pet pelican\",\\nsystem=\"Answer like GlaDOS\"\\n)\\nAttachments\\nModels that accept multi-modal input (images, audio, video etc) can be passed attachments using theattachments=\\nkeyword argument. This accepts a list ofllm.Attachment()instances.\\nThis example shows two attachments - one from a file path and one from a URL:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\\n\"Describe these images\",\\nattachments=[\\nllm.Attachment(path=\"pelican.jpg\"),\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\"),\\n]\\n)\\nUse llm.Attachment(content=b\"binary image content here\")to pass binary content directly.\\nYou can check which attachment types (if any) a model supports using themodel.attachment_typesset:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nprint(model.attachment_types)\\n# {\\'image/gif\\', \\'image/png\\', \\'image/jpeg\\', \\'image/webp\\'}\\n(continues on next page)\\n108 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 114, 'page_label': '109'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif \"image/jpeg\" in model.attachment_types:\\n# Use a JPEG attachment here\\n...\\nTools\\nToolsare functions that can be executed by the model as part of a chain of responses.\\nYou can define tools in Python code - with a docstring to describe what they do - and then pass them to themodel.\\nprompt() method using thetools= keyword argument. If the model decides to request a tool call theresponse.\\ntool_calls()method show what the model wants to execute:\\nimport llm\\ndef upper(text: str) -> str:\\n\"\"\"Convert text to uppercase.\"\"\"\\nreturn text.upper()\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.prompt(\"Convert panda to upper\", tools=[upper])\\ntool_calls = response.tool_calls()\\n# [ToolCall(name=\\'upper\\', arguments={\\'text\\': \\'panda\\'}, tool_call_id=\\'...\\')]\\nYou can callresponse.execute_tool_calls()to execute those calls and get back the results:\\ntool_results = response.execute_tool_calls()\\n# [ToolResult(name=\\'upper\\', output=\\'PANDA\\', tool_call_id=\\'...\\')]\\nYoucanusethe model.chain()topasstheresultsoftoolcallsbacktothemodelautomaticallyassubsequentprompts:\\nchain_response = model.chain(\\n\"Convert panda to upper\",\\ntools=[upper],\\n)\\nprint(chain_response.text())\\n# The word \"panda\" converted to uppercase is \"PANDA\".\\nYou can also loop through themodel.chain()response to get a stream of tokens, like this:\\nfor chunk in model.chain(\\n\"Convert panda to upper\",\\ntools=[upper],\\n):\\nprint(chunk, end=\"\", flush=True)\\nThis will stream each of the chain of responses in turn as they are generated.\\nYoucanaccesstheindividualresponsesthatmakeupthechainusing chain.responses(). Thiscanbeiteratedover\\nas the chain executes like this:\\nchain = model.chain(\\n\"Convert panda to upper\",\\n(continues on next page)\\n2.12. Python API 109'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 115, 'page_label': '110'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntools=[upper],\\n)\\nfor response in chain.responses():\\nprint(response.prompt)\\nfor chunk in response:\\nprint(chunk, end=\"\", flush=True)\\nTool debugging hooks\\nPass a function to thebefore_call= parameter ofmodel.chain() to have that called before every tool call is exe-\\ncuted. You can raisellm.CancelToolCall()to cancel that tool call.\\nThe method signature isdef before_call(tool: Optional[llm.Tool], tool_call: llm.ToolCall) -\\nthatfirst toolargumentcanbe Noneifthemodelrequestsatoolbeexecutedthathasnotbeenprovidedinthe tools=\\nlist.\\nHere’s an example:\\nimport llm\\nfrom typing import Optional\\ndef upper(text: str) -> str:\\n\"Convert text to uppercase.\"\\nreturn text.upper()\\ndef before_call(tool: Optional[llm.Tool], tool_call: llm.ToolCall):\\nprint(f\"About to call tool {tool.name} with arguments {tool_call.arguments}\")\\nif tool.name == \"upper\" and \"bad\" in repr(tool_call.arguments):\\nraise llm.CancelToolCall(\"Not allowed to call upper on text containing \\'bad\\'\")\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.chain(\\n\"Convert panda to upper and badger to upper\",\\ntools=[upper],\\nbefore_call=before_call,\\n)\\nprint(response.text())\\nIf you raisellm.CancelToolCall in thebefore_call function the model will be informed that the tool call was\\ncancelled.\\nThe after_call= parameter can be used to run a logging function after each tool call has been executed.\\nThe method signature isdef after_call(tool: llm.Tool, tool_call: llm.ToolCall, tool_result:\\nllm.ToolResult). This continues the previous example:\\ndef after_call(tool: llm.Tool, tool_call: llm.ToolCall, tool_result: llm.ToolResult):\\nprint(f\"Tool {tool.name} called with arguments {tool_call.arguments} returned {tool_\\n˓→result.output}\")\\nresponse = model.chain(\\n\"Convert panda to upper and badger to upper\",\\ntools=[upper],\\n(continues on next page)\\n110 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 116, 'page_label': '111'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nafter_call=after_call,\\n)\\nprint(response.text())\\nTools can return attachments\\nToolscanreturn attachmentsinadditiontoreturningtext. Attachmentsthatarereturnedfromatoolcallwillbepassed\\nto the model as attachments for the next prompt in the chain.\\nTo return one or more attachments, return allm.ToolOutput instance from your tool function. This can have an\\noutput=string and anattachments=list ofllm.Attachmentinstances.\\nHere’s an example:\\nimport llm\\ndef generate_image(prompt: str) -> llm.ToolOutput:\\n\"\"\"Generate an image based on the prompt.\"\"\"\\nimage_content = generate_image_from_prompt(prompt)\\nreturn llm.ToolOutput(\\noutput=\"Image generated successfully\",\\nattachments=[llm.Attachment(\\ncontent=image_content,\\nmimetype=\"image/png\"\\n)],\\n)\\nToolbox classes\\nFunctions are useful for simple tools, but some tools may have more advanced needs. You can also define tools as a\\nclass (known as a “toolbox”), which provides the following advantages:\\n• Toolbox tools can bundle multiple tools together\\n• Toolbox tools can be configured, e.g. to give filesystem tools access to a specific directory\\n• Toolbox instances can persist shared state in between tool invocations\\nToolboxes are classes that extendllm.Toolbox. Any methods that do not begin with an underscore will be exposed\\nas tool functions.\\nThis example sets up key/value memory storage that can be used by the model:\\nimport llm\\nclass Memory(llm.Toolbox):\\n_memory = None\\ndef _get_memory(self):\\nif self._memory is None:\\nself._memory = {}\\nreturn self._memory\\n(continues on next page)\\n2.12. Python API 111'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 117, 'page_label': '112'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ndef set(self, key: str, value: str):\\n\"Set something as a key\"\\nself._get_memory()[key] = value\\ndef get(self, key: str):\\n\"Get something from a key\"\\nreturn self._get_memory().get(key) or \"\"\\ndef append(self, key: str, value: str):\\n\"Append something as a key\"\\nmemory = self._get_memory()\\nmemory[key] = (memory.get(key) or \"\") + \"\\\\n\" + value\\ndef keys(self):\\n\"Return a list of keys\"\\nreturn list(self._get_memory().keys())\\nYou can then use that from Python like this:\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nmemory = Memory()\\nconversation = model.conversation(tools=[memory])\\nprint(conversation.chain(\"Set name to Simon\", after_call=print).text())\\nprint(memory._memory)\\n# Should show {\\'name\\': \\'Simon\\'}\\nprint(conversation.chain(\"Set name to Penguin\", after_call=print).text())\\n# Now it should be {\\'name\\': \\'Penguin\\'}\\nprint(conversation.chain(\"Print current name\", after_call=print).text())\\nSee theregister_tools() plugin hook documentationfor an example of this tool in action as a CLI plugin.\\nSchemas\\nAs withthe CLI toolsome models support passing a JSON schema should be used for the resulting response.\\nYoucanpassthistothe prompt(schema=)parameteraseitheraPythondictionaryoraPydantic BaseModelsubclass:\\nimport llm, json\\nfrom pydantic import BaseModel\\nclass Dog(BaseModel):\\nname: str\\nage: int\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"Describe a nice dog\", schema=Dog)\\ndog = json.loads(response.text())\\nprint(dog)\\n# {\"name\":\"Buddy\",\"age\":3}\\n112 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 118, 'page_label': '113'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nYou can also pass a schema directly, like this:\\nresponse = model.prompt(\"Describe a nice dog\", schema={\\n\"properties\": {\\n\"name\": {\"title\": \"Name\", \"type\": \"string\"},\\n\"age\": {\"title\": \"Age\", \"type\": \"integer\"},\\n},\\n\"required\": [\"name\", \"age\"],\\n\"title\": \"Dog\",\\n\"type\": \"object\",\\n})\\nYou can also use LLM’salternative schema syntaxvia thellm.schema_dsl(schema_dsl)function. This provides\\na quick way to construct a JSON schema for simple cases:\\nprint(model.prompt(\\n\"Describe a nice dog with a surprising name\",\\nschema=llm.schema_dsl(\"name, age int, bio\")\\n))\\nPassmulti=Trueto generate a schema that returns multiple items matching that specification:\\nprint(model.prompt(\\n\"Describe 3 nice dogs with surprising names\",\\nschema=llm.schema_dsl(\"name, age int, bio\", multi=True)\\n))\\nFragments\\nThe fragment systemfrom the CLI tool can also be accessed from the Python API, by passingfragments= and/or\\nsystem_fragments=lists of strings to theprompt()method:\\nresponse = model.prompt(\\n\"What do these documents say about dogs?\",\\nfragments=[\\nopen(\"dogs1.txt\").read(),\\nopen(\"dogs2.txt\").read(),\\n],\\nsystem_fragments=[\\n\"You answer questions like Snoopy\",\\n]\\n)\\nThis mechanism has limited utility in Python, as you can also assemble the contents of these strings together into the\\nprompt=and system=strings directly.\\nFragments become more interesting if you are working with LLM’s mechanisms for storing prompts to a SQLite\\ndatabase, which are not yet part of the stable, documented Python API.\\nSomemodelpluginsmayincludefeaturesthattakeadvantageoffragments,forexamplellm-anthropicaimstousethem\\nas part of a mechanism that taps into Claude’s prompt caching system.\\n2.12. Python API 113'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 119, 'page_label': '114'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nModel options\\nFor models that support options (view those withllm models --options) you can pass options as keyword argu-\\nments to the.prompt()method:\\nmodel = llm.get_model()\\nprint(model.prompt(\"Names for otters\", temperature=0.2))\\nPassing an API key\\nModels that accept API keys should take an additionalkey=parameter to theirmodel.prompt()method:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nprint(model.prompt(\"Names for beavers\", key=\"sk-...\"))\\nIf you don’t provide this argument LLM will attempt to find it from an environment variable (OPENAI_API_KEY for\\nOpenAI, others for different plugins) or from keys that have been saved using thellm keys setcommand.\\nSomemodelpluginsmaynotyethavebeenupgradedtohandlethe key=parameter,inwhichcaseyouwillneedtouse\\none of the other mechanisms.\\nModels from plugins\\nAnymodelsyouhaveinstalledaspluginswillalsobeavailablethroughthismechanism,forexampletouseAnthropic’s\\nClaude 3.5 Sonnet model with llm-anthropic:\\npip install llm-anthropic\\nThen in your Python code:\\nimport llm\\nmodel = llm.get_model(\"claude-3.5-sonnet\")\\n# Use this if you have not set the key using \\'llm keys set claude\\':\\nmodel.key = \\'YOUR_API_KEY_HERE\\'\\nresponse = model.prompt(\"Five surprising names for a pet pelican\")\\nprint(response.text())\\nSome models do not use API keys at all.\\nAccessing the underlying JSON\\nMost model plugins also make a JSON version of the prompt response available. The structure of this will differ\\nbetween model plugins, so building against this is likely to result in code that only works with that specific model\\nprovider.\\nYou can access this JSON data as a Python dictionary using theresponse.json()method:\\nimport llm\\nfrom pprint import pprint\\nmodel = llm.get_model(\"gpt-4o-mini\")\\n(continues on next page)\\n114 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 120, 'page_label': '115'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nresponse = model.prompt(\"3 names for an otter\")\\njson_data = response.json()\\npprint(json_data)\\nHere’s that example output from GPT-4o mini:\\n{\\'content\\': \\'Sure! Here are three fun names for an otter:\\\\n\\'\\n\\'\\\\n\\'\\n\\'1. **Splash**\\\\n\\'\\n\\'2. **Bubbles**\\\\n\\'\\n\\'3. **Otto** \\\\n\\'\\n\\'\\\\n\\'\\n\\'Feel free to mix and match or use these as inspiration!\\',\\n\\'created\\': 1739291215,\\n\\'finish_reason\\': \\'stop\\',\\n\\'id\\': \\'chatcmpl-AznO31yxgBjZ4zrzBOwJvHEWgdTaf\\',\\n\\'model\\': \\'gpt-4o-mini-2024-07-18\\',\\n\\'object\\': \\'chat.completion.chunk\\',\\n\\'usage\\': {\\'completion_tokens\\': 43,\\n\\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0,\\n\\'audio_tokens\\': 0,\\n\\'reasoning_tokens\\': 0,\\n\\'rejected_prediction_tokens\\': 0},\\n\\'prompt_tokens\\': 13,\\n\\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0},\\n\\'total_tokens\\': 56}}\\nToken usage\\nMany models can return a count of the number of tokens used while executing the prompt.\\nThe response.usage()method provides an abstraction over this:\\npprint(response.usage())\\nExample output:\\nUsage(input=5,\\noutput=2,\\ndetails={\\'candidatesTokensDetails\\': [{\\'modality\\': \\'TEXT\\',\\n\\'tokenCount\\': 2}],\\n\\'promptTokensDetails\\': [{\\'modality\\': \\'TEXT\\', \\'tokenCount\\': 5}]})\\nThe .inputand .outputproperties are integers representing the number of input and output tokens. The.details\\nproperty may be a dictionary with additional custom values that vary by model.\\n2.12. Python API 115'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 121, 'page_label': '116'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nStreaming responses\\nFor models that support it you can stream responses as they are generated, like this:\\nresponse = model.prompt(\"Five diabolical names for a pet goat\")\\nfor chunk in response:\\nprint(chunk, end=\"\")\\nTheresponse.text()method described earlier does this for you - it runs through the iterator and gathers the results\\ninto a string.\\nIf a response has been evaluated,response.text()will continue to return the same string.\\n2.12.2 Async models\\nSome plugins provide async versions of their supported models, suitable for use with Python asyncio.\\nTo use an async model, use thellm.get_async_model()function instead ofllm.get_model():\\nimport llm\\nmodel = llm.get_async_model(\"gpt-4o\")\\nYou can then run a prompt usingawait model.prompt(...):\\nprint(await model.prompt(\\n\"Five surprising names for a pet pelican\"\\n).text())\\nOr useasync for chunk in ...to stream the response as it is generated:\\nasync for chunk in model.prompt(\\n\"Five surprising names for a pet pelican\"\\n):\\nprint(chunk, end=\"\", flush=True)\\nThis await model.prompt() method takes the same arguments as the synchronousmodel.prompt() method, for\\noptions and attachments andkey=and suchlike.\\nTool functions can be sync or async\\nToolfunctions canbebothsynchronousorasynchronous. Thelatteraredefinedusing async def tool_name(...).\\nEither kind of function can be passed to thetools=[...]parameter.\\nIf anasync def function is used in a synchronous context LLM will automatically execute it in a thread pool using\\nasyncio.run(). This means the following will work even in non-asynchronous Python scripts:\\nasync def hello(name: str) -> str:\\n\"Say hello to name\"\\nreturn \"Hello there \" + name\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nchain_response = model.chain(\\n\"Say hello to Percival\", tools=[hello]\\n)\\nprint(chain_response.text())\\n116 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 122, 'page_label': '117'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nThis also works forasync def methods ofllm.Toolboxsubclasses.\\nTool use for async models\\nTool use is also supported for async models, using either synchronous or asynchronous tool functions. Synchronous\\nfunctionswillblocktheeventloopsoonlyusethoseinasynchronouscontextifyouarecertaintheyareextremelyfast.\\nThe response.execute_tool_calls() and chain_response.text() and chain_response.responses()\\nmethods must all be awaited when run against asynchronous models:\\nimport llm\\nmodel = llm.get_async_model(\"gpt-4.1\")\\ndef upper(string):\\n\"Converts string to uppercase\"\\nreturn string.upper()\\nchain = model.chain(\\n\"Convert panda to uppercase then pelican to uppercase\",\\ntools=[upper],\\nafter_call=print\\n)\\nprint(await chain.text())\\nTo iterate over the chained response output as it arrives useasync for:\\nasync for chunk in model.chain(\\n\"Convert panda to uppercase then pelican to uppercase\",\\ntools=[upper]\\n):\\nprint(chunk, end=\"\", flush=True)\\nThe before_calland after_callhooks can be async functions when used with async models.\\n2.12.3 Conversations\\nLLM supportsconversations, where you ask follow-up questions of a model as part of an ongoing conversation.\\nTo start a new conversation, use themodel.conversation()method:\\nmodel = llm.get_model()\\nconversation = model.conversation()\\nYou can then use theconversation.prompt()method to execute prompts against this conversation:\\nresponse = conversation.prompt(\"Five fun facts about pelicans\")\\nprint(response.text())\\nThis works exactly the same as themodel.prompt() method, except that the conversation will be maintained across\\nmultiple prompts. So if you run this next:\\nresponse2 = conversation.prompt(\"Now do skunks\")\\nprint(response2.text())\\n2.12. Python API 117'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 123, 'page_label': '118'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nYou will get back five fun facts about skunks.\\nThe conversation.prompt()method supports attachments as well:\\nresponse = conversation.prompt(\\n\"Describe these birds\",\\nattachments=[\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\")\\n]\\n)\\nAccessconversation.responses for a list of all of the responses that have so far been returned during the conver-\\nsation.\\nConversations using tools\\nYou can pass a list of tool functions to thetools=[]argument when you start a new conversation:\\nimport llm\\ndef upper(text: str) -> str:\\n\"convert text to upper case\"\\nreturn text.upper()\\ndef reverse(text: str) -> str:\\n\"reverse text\"\\nreturn text[::-1]\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nconversation = model.conversation(tools=[upper, reverse])\\nYou can then call theconversation.chain()method multiple times to have a conversation that uses those tools:\\nprint(conversation.chain(\\n\"Convert panda to uppercase and reverse it\"\\n).text())\\nprint(conversation.chain(\\n\"Same with pangolin\"\\n).text())\\nThe before_call= and after_call= parameters described above can be passed directly to the model.\\nconversation()method to set those options for all chained prompts in that conversation.\\n2.12.4 Listing models\\nThe llm.get_models()list returns a list of all available models, including those from plugins.\\nimport llm\\nfor model in llm.get_models():\\nprint(model.model_id)\\nUse llm.get_async_models()to list async models:\\n118 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 124, 'page_label': '119'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nfor model in llm.get_async_models():\\nprint(model.model_id)\\n2.12.5 Running code when a response has completed\\nFor some applications, such as tracking the tokens used by an application, it may be useful to execute code as soon as\\na response has finished being executed\\nYou can do this using theresponse.on_done(callback)method, which causes your callback function to be called\\nas soon as the response has finished (all tokens have been returned).\\nThe signature of the method you provide isdef callback(response)- it can be optionally anasync def method\\nwhen working with asynchronous models.\\nExample usage:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"a poem about a hippo\")\\nresponse.on_done(lambda response: print(response.usage()))\\nprint(response.text())\\nWhich outputs:\\nUsage(input=20, output=494, details={})\\nIn a sunlit glade by a bubbling brook,\\nLived a hefty hippo, with a curious look.\\n...\\nOr using anasynciomodel, where you need toawait response.on_done(done)to queue up the callback:\\nimport asyncio, llm\\nasync def run():\\nmodel = llm.get_async_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"a short poem about a brick\")\\nasync def done(response):\\nprint(await response.usage())\\nprint(await response.text())\\nawait response.on_done(done)\\nprint(await response.text())\\nasyncio.run(run())\\n2.12. Python API 119'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 125, 'page_label': '120'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.12.6 Other functions\\nThe llmtop level package includes some useful utility functions.\\nset_alias(alias, model_id)\\nThe llm.set_alias()function can be used to define a new alias:\\nimport llm\\nllm.set_alias(\"mini\", \"gpt-4o-mini\")\\nThe second argument can be a model identifier or another alias, in which case that alias will be resolved.\\nIf thealiases.jsonfile does not exist or contains invalid JSON it will be created or overwritten.\\nremove_alias(alias)\\nRemoves the alias with the given name from thealiases.jsonfile.\\nRaises KeyErrorif the alias does not exist.\\nimport llm\\nllm.remove_alias(\"turbo\")\\nset_default_model(alias)\\nThis sets the default model to the given model ID or alias. Any changes to defaults will be persisted in the LLM\\nconfiguration folder, and will affect all programs using LLM on the system, including thellmCLI tool.\\nimport llm\\nllm.set_default_model(\"claude-3.5-sonnet\")\\nget_default_model()\\nThis returns the currently configured default model, orgpt-4o-miniif no default has been set.\\nimport llm\\nmodel_id = llm.get_default_model()\\nTo detect if no default has been set you can use this pattern:\\nif llm.get_default_model(default=None) is None:\\nprint(\"No default has been set\")\\nHere thedefault=parameter specifies the value that should be returned if there is no configured default.\\n120 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 126, 'page_label': '121'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nset_default_embedding_model(alias) and get_default_embedding_model()\\nThese two methods work the same asset_default_model() and get_default_model() but for the defaultem-\\nbedding modelinstead.\\n2.13 Logging to SQLite\\nllmdefaults to logging all prompts and responses to a SQLite database.\\nYou can find the location of that database using thellm logs pathcommand:\\nllm logs path\\nOn my Mac that outputs:\\n/Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nThis will differ for other operating systems.\\nTo avoid logging an individual prompt, pass--no-logor -nto the command:\\nllm 'Ten names for cheesecakes' -n\\nTo turn logging by default off:\\nllm logs off\\nIf you’ve turned off logging you can still log an individual prompt and response by adding--log:\\nllm 'Five ambitious names for a pet pterodactyl' --log\\nTo turn logging by default back on again:\\nllm logs on\\nTo see the status of the logs database, run this:\\nllm logs status\\nExample output:\\nLogging is ON for all prompts\\nFound log database at /Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nNumber of conversations logged: 33\\nNumber of responses logged: 48\\nDatabase file size: 19.96MB\\n2.13. Logging to SQLite 121\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 127, 'page_label': '122'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.13.1 Viewing the logs\\nYou can view the logs using thellm logscommand:\\nllm logs\\nThis will output the three most recent logged items in Markdown format, showing both the prompt and the response\\nformatted using Markdown.\\nTo get back just the most recent prompt response as plain text, add-r/--response:\\nllm logs -r\\nUse -x/--extractto extract and return the first fenced code block from the selected log entries:\\nllm logs --extract\\nOr --xl/--extract-lastfor the last fenced code block:\\nllm logs --extract-last\\nAdd--jsonto get the log messages in JSON instead:\\nllm logs --json\\nAdd-n 10to see the ten most recent items:\\nllm logs -n 10\\nOr -n 0to see everything that has ever been logged:\\nllm logs -n 0\\nYou can truncate the display of the prompts and responses using the-t/--truncateoption. This can help make the\\nJSON output more readable - though the--shortoption is usually better.\\nllm logs -n 1 -t --json\\nExample output:\\n[\\n{\\n\"id\": \"01jm8ec74wxsdatyn5pq1fp0s5\",\\n\"model\": \"anthropic/claude-3-haiku-20240307\",\\n\"prompt\": \"hi\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"response\": \"Hello! How can I assist you today?\",\\n\"conversation_id\": \"01jm8ec74taftdgj2t4zra9z0j\",\\n\"duration_ms\": 560,\\n\"datetime_utc\": \"2025-02-16T22:34:30.374882+00:00\",\\n\"input_tokens\": 8,\\n\"output_tokens\": 12,\\n\"token_details\": null,\\n\"conversation_name\": \"hi\",\\n(continues on next page)\\n122 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 128, 'page_label': '123'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"conversation_model\": \"anthropic/claude-3-haiku-20240307\",\\n\"attachments\": []\\n}\\n]\\n-s/–short mode\\nUse -s/--shortto see a shortened YAML log with truncated prompts and no responses:\\nllm logs -n 2 --short\\nExample output:\\n- model: deepseek-reasoner\\ndatetime: \\'2025-02-02T06:39:53\\'\\nconversation: 01jk2pk05xq3d0vgk0202zrsg1\\nprompt: H01 There are five huts. H02 The Scotsman lives in the purple hut. H03 The␣\\n˓→Welshman owns the parrot. H04 Kombucha is...\\n- model: o3-mini\\ndatetime: \\'2025-02-02T19:03:05\\'\\nconversation: 01jk40qkxetedzpf1zd8k9bgww\\nsystem: Formatting re-enabled. Write a detailed README with extensive usage examples.\\nprompt: <documents> <document index=\"1\"> <source>./Cargo.toml</source> <document_\\n˓→content> [package] name = \"py-limbo\" version...\\nInclude -u/--usageto include token usage information:\\nllm logs -n 1 --short --usage\\nExample output:\\n- model: o3-mini\\ndatetime: \\'2025-02-16T23:00:56\\'\\nconversation: 01jm8fxxnef92n1663c6ays8xt\\nsystem: Produce Python code that demonstrates every possible usage of yaml.dump\\nwith all of the arguments it can take, especi...\\nprompt: <documents> <document index=\"1\"> <source>./setup.py</source> <document_content>\\nNAME = \\'PyYAML\\' VERSION = \\'7.0.0.dev0...\\nusage:\\ninput: 74793\\noutput: 3550\\ndetails:\\ncompletion_tokens_details:\\nreasoning_tokens: 2240\\n2.13. Logging to SQLite 123'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 129, 'page_label': '124'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nLogs for a conversation\\nTo view the logs for the most recentconversationyou have had with a model, use-c:\\nllm logs -c\\nTo see logs for a specific conversation based on its ID, use--cid IDor --conversation ID:\\nllm logs --cid 01h82n0q9crqtnzmf13gkyxawg\\nSearching the logs\\nYou can search the logs for a search term in thepromptor theresponsecolumns.\\nllm logs -q 'cheesecake'\\nThe most relevant results will be shown first.\\nTo switch to sorting with most recent first, add-l/--latest. This can be combined with-n to limit the number of\\nresults shown:\\nllm logs -q 'cheesecake' -l -n 3\\nFiltering past a specific ID\\nIfyouwanttoretrieveallofthelogsthatwererecordedsinceaspecificresponseIDyoucandosousingtheseoptions:\\n• --id-gt $ID- every record with an ID greater than $ID\\n• --id-gte $ID- every record with an ID greater than or equal to $ID\\nIDs are always issued in ascending order by time, so this provides a useful way to see everything that has happened\\nsince a particular record.\\nThis can be particularly useful whenworking with schema data, where you might want to access every record that you\\nhave created using a specific--schemabut exclude records you have previously processed.\\nFiltering by model\\nYou can filter to logs just for a specific model (or model alias) using-m/--model:\\nllm logs -m chatgpt\\nFiltering by prompts that used specific fragments\\nThe-f/--fragment Xoptionwillfilterforjustresponsesthatwerecreatedusingthespecified fragmenthashoralias\\nor URL or filename.\\nFragmentsaredisplayedinthelogsastheirhashID.Add -e/--expandtodisplayfragmentsastheirfullcontent-this\\noption works for both the default Markdown and the--jsonmode:\\nllm logs -f https://llm.datasette.io/robots.txt --expand\\nYoucandisplayjustthecontentforaspecificfragmenthashID(oralias)usingthe llm fragments showcommand:\\n124 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 130, 'page_label': '125'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm fragments show 993fd38d898d2b59fd2d16c811da5bdac658faa34f0f4d411edde7c17ebb0680\\nIf you provide multiple fragments you will get back responses that usedall of those fragments.\\nFiltering by prompts that used specific tools\\nYou can filter for responses that used tools from specific fragments with the--tool/-Toption:\\nllm logs -T simple_eval\\nThis will match responses that involved aresult from that tool. If the tool was not executed it will not be included in\\nthe filtered responses.\\nPass--tool/-Tmultiple times for responses that used all of the specified tools.\\nUse the llm logs --tools flag to see all responses that involved at least one tool result, including from\\n--functions:\\nllm logs --tools\\nBrowsing data collected using schemas\\nThe--schema Xoptioncanbeusedtoviewresponsesthatusedthespecifiedschema,usinganyofthe waystospecify\\na schema:\\nllm logs --schema \\'name, age int, bio\\'\\nThis can be combined with--data and --data-array and --data-key to extract just the returned JSON data -\\nconsult theschemas documentationfor details.\\n2.13.2 Browsing logs using Datasette\\nYou can also use Datasette to browse your logs like this:\\ndatasette \"$(llm logs path)\"\\n2.13.3 Backing up your database\\nYou can backup your logs to another file using thellm logs backupcommand:\\nllm logs backup /tmp/backup.db\\nThis uses SQLite VACUUM INTO under the hood.\\n2.13. Logging to SQLite 125'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 131, 'page_label': '126'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.13.4 SQL schema\\nHere’s the SQL schema used by thelogs.dbdatabase:\\nCREATE TABLE [conversations] (\\n[id] TEXT PRIMARY KEY,\\n[name] TEXT,\\n[model] TEXT\\n);\\nCREATE TABLE [schemas] (\\n[id] TEXT PRIMARY KEY,\\n[content] TEXT\\n);\\nCREATE TABLE \"responses\" (\\n[id] TEXT PRIMARY KEY,\\n[model] TEXT,\\n[prompt] TEXT,\\n[system] TEXT,\\n[prompt_json] TEXT,\\n[options_json] TEXT,\\n[response] TEXT,\\n[response_json] TEXT,\\n[conversation_id] TEXT REFERENCES [conversations]([id]),\\n[duration_ms] INTEGER,\\n[datetime_utc] TEXT,\\n[input_tokens] INTEGER,\\n[output_tokens] INTEGER,\\n[token_details] TEXT,\\n[schema_id] TEXT REFERENCES [schemas]([id]),\\n[resolved_model] TEXT\\n);\\nCREATE VIRTUAL TABLE [responses_fts] USING FTS5 (\\n[prompt],\\n[response],\\ncontent=[responses]\\n);\\nCREATE TABLE [attachments] (\\n[id] TEXT PRIMARY KEY,\\n[type] TEXT,\\n[path] TEXT,\\n[url] TEXT,\\n[content] BLOB\\n);\\nCREATE TABLE [prompt_attachments] (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[attachment_id] TEXT REFERENCES [attachments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[attachment_id])\\n);\\nCREATE TABLE [fragments] (\\n[id] INTEGER PRIMARY KEY,\\n[hash] TEXT,\\n(continues on next page)\\n126 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 132, 'page_label': '127'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n[content] TEXT,\\n[datetime_utc] TEXT,\\n[source] TEXT\\n);\\nCREATE TABLE [fragment_aliases] (\\n[alias] TEXT PRIMARY KEY,\\n[fragment_id] INTEGER REFERENCES [fragments]([id])\\n);\\nCREATE TABLE \"prompt_fragments\" (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[fragment_id] INTEGER REFERENCES [fragments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[fragment_id],\\n[order])\\n);\\nCREATE TABLE \"system_fragments\" (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[fragment_id] INTEGER REFERENCES [fragments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[fragment_id],\\n[order])\\n);\\nCREATE TABLE [tools] (\\n[id] INTEGER PRIMARY KEY,\\n[hash] TEXT,\\n[name] TEXT,\\n[description] TEXT,\\n[input_schema] TEXT,\\n[plugin] TEXT\\n);\\nCREATE TABLE [tool_responses] (\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[response_id] TEXT REFERENCES [responses]([id]),\\nPRIMARY KEY ([tool_id],\\n[response_id])\\n);\\nCREATE TABLE [tool_calls] (\\n[id] INTEGER PRIMARY KEY,\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[name] TEXT,\\n[arguments] TEXT,\\n[tool_call_id] TEXT\\n);\\nCREATE TABLE \"tool_results\" (\\n[id] INTEGER PRIMARY KEY,\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[name] TEXT,\\n[output] TEXT,\\n(continues on next page)\\n2.13. Logging to SQLite 127'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 133, 'page_label': '128'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n[tool_call_id] TEXT,\\n[instance_id] INTEGER REFERENCES [tool_instances]([id]),\\n[exception] TEXT\\n);\\nCREATE TABLE [tool_instances] (\\n[id] INTEGER PRIMARY KEY,\\n[plugin] TEXT,\\n[name] TEXT,\\n[arguments] TEXT\\n);\\nresponses_fts configures SQLite full-text search against theprompt and response columns in theresponses\\ntable.\\n2.14 Related tools\\nThe following tools are designed to be used with LLM:\\n2.14.1 strip-tags\\nstrip-tags is a command for stripping tags from HTML. This is useful when working with LLMs because HTML tags\\ncan use up a lot of your token budget.\\nHere’showtosummarizethefrontpageoftheNewYorkTimes,bybothstrippingtagsandfilteringtojusttheelements\\nwith class=\"story-wrapper\":\\ncurl -s https://www.nytimes.com/ \\\\\\n| strip-tags .story-wrapper \\\\\\n| llm -s \\'summarize the news\\'\\nllm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs describes ways to usestrip-tagsin\\nmore detail.\\n2.14.2 ttok\\nttok is a command-line tool for counting OpenAI tokens. You can use it to check if input is likely to fit in the token\\nlimit for GPT 3.5 or GPT4:\\ncat my-file.txt | ttok\\n125\\nIt can also truncate input down to a desired number of tokens:\\nttok This is too many tokens -t 3\\nThis is too\\nThis is useful for truncating a large document down to a size where it can be processed by an LLM.\\n128 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 134, 'page_label': '129'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.14.3 Symbex\\nSymbex is a tool for searching for symbols in Python codebases. It’s useful for extracting just the code for a specific\\nproblem and then piping that into LLM for explanation, refactoring or other tasks.\\nHere’s how to use it to find all functions that matchtest*csv* and use those to guess what the software under test\\ndoes:\\nsymbex 'test*csv*' | \\\\\\nllm --system 'based on these tests guess what this tool does'\\nIt can also be used to export symbols in a format that can be piped tollm embed-multiin order to create embeddings:\\nsymbex '*' '*:*' --nl | \\\\\\nllm embed-multi symbols - \\\\\\n--format nl --database embeddings.db --store\\nFor more examples see Symbex: search Python code for functions and classes, then pipe them into a LLM.\\n2.15 CLI reference\\nThis page lists the--helpoutput for all of thellmcommands.\\n2.15.1 llm –help\\nUsage: llm [OPTIONS] COMMAND [ARGS]...\\nAccess Large Language Models from the command-line\\nDocumentation: https://llm.datasette.io/\\nLLM can run models from many different providers. Consult the plugin directory\\nfor a list of available models:\\nhttps://llm.datasette.io/en/stable/plugins/directory.html\\nTo get started with OpenAI, obtain an API key from them and:\\n$ llm keys set openai\\nEnter key: ...\\nThen execute a prompt like this:\\nllm 'Five outrageous names for a pet pelican'\\nFor a full list of prompting options run:\\nllm prompt --help\\nOptions:\\n--version Show the version and exit.\\n(continues on next page)\\n2.15. CLI reference 129\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 135, 'page_label': '130'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n-h, --help Show this message and exit.\\nCommands:\\nprompt* Execute a prompt\\naliases Manage model aliases\\nchat Hold an ongoing chat with a model.\\ncollections View and manage collections of embeddings\\nembed Embed text and store or return the result\\nembed-models Manage available embedding models\\nembed-multi Store embeddings for multiple strings at once in the...\\nfragments Manage fragments that are stored in the database\\ninstall Install packages from PyPI into the same environment as LLM\\nkeys Manage stored API keys for different models\\nlogs Tools for exploring logged prompts and responses\\nmodels Manage available models\\nopenai Commands for working directly with the OpenAI API\\nplugins List installed plugins\\nschemas Manage stored schemas\\nsimilar Return top N similar IDs from a collection using cosine...\\ntemplates Manage stored prompt templates\\ntools Manage tools that can be made available to LLMs\\nuninstall Uninstall Python packages from the LLM environment\\nllm prompt –help\\nUsage: llm prompt [OPTIONS] [PROMPT]\\nExecute a prompt\\nDocumentation: https://llm.datasette.io/en/stable/usage.html\\nExamples:\\nllm 'Capital of France?'\\nllm 'Capital of France?' -m gpt-4o\\nllm 'Capital of France?' -s 'answer in Spanish'\\nMulti-modal models can be called with attachments like this:\\nllm 'Extract text from this image' -a image.jpg\\nllm 'Describe' -a https://static.simonwillison.net/static/2024/pelicans.jpg\\ncat image | llm 'describe image' -a -\\n# With an explicit mimetype:\\ncat image | llm 'describe image' --at - image/jpeg\\nThe -x/--extract option returns just the content of the first ``` fenced code\\nblock, if one is present. If none are present it returns the full response.\\nllm 'JavaScript function for reversing a string' -x\\n(continues on next page)\\n130 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 136, 'page_label': '131'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-d, --database FILE Path to log database\\n-q, --query TEXT Use first model matching these strings\\n-a, --attachment ATTACHMENT Attachment path or URL or -\\n--at, --attachment-type <TEXT TEXT>...\\nAttachment with explicit mimetype,\\n--at image.jpg image/jpeg\\n-T, --tool TEXT Name of a tool to make available to the model\\n--functions TEXT Python code block or file path defining\\nfunctions to register as tools\\n--td, --tools-debug Show full details of tool executions\\n--ta, --tools-approve Manually approve every tool execution\\n--cl, --chain-limit INTEGER How many chained tool responses to allow,\\ndefault 5, set 0 for unlimited\\n-o, --option <TEXT TEXT>... key/value options for the model\\n--schema TEXT JSON schema, filepath or ID\\n--schema-multi TEXT JSON schema to use for multiple results\\n-f, --fragment TEXT Fragment (alias, URL, hash or file path) to\\nadd to the prompt\\n--sf, --system-fragment TEXT Fragment to add to system prompt\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters for template\\n--no-stream Do not stream output\\n-n, --no-log Don 't log to database\\n--log Log prompt and response to the database\\n-c, --continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation with the given ID.\\n--key TEXT API key to use\\n--save TEXT Save prompt with this template name\\n--async Run prompt asynchronously\\n-u, --usage Show token usage\\n-x, --extract Extract first fenced code block\\n--xl, --extract-last Extract last fenced code block\\n-h, --help Show this message and exit.\\nllm chat –help\\nUsage: llm chat [OPTIONS]\\nHold an ongoing chat with a model.\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-c, --continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation with the given ID.\\n-f, --fragment TEXT Fragment (alias, URL, hash or file path) to add\\nto the prompt\\n(continues on next page)\\n2.15. CLI reference 131\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 137, 'page_label': '132'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n--sf, --system-fragment TEXT Fragment to add to system prompt\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters for template\\n-o, --option <TEXT TEXT>... key/value options for the model\\n-d, --database FILE Path to log database\\n--no-stream Do not stream output\\n--key TEXT API key to use\\n-T, --tool TEXT Name of a tool to make available to the model\\n--functions TEXT Python code block or file path defining\\nfunctions to register as tools\\n--td, --tools-debug Show full details of tool executions\\n--ta, --tools-approve Manually approve every tool execution\\n--cl, --chain-limit INTEGER How many chained tool responses to allow,\\ndefault 5, set 0 for unlimited\\n-h, --help Show this message and exit.\\nllm keys –help\\nUsage: llm keys [OPTIONS] COMMAND [ARGS]...\\nManage stored API keys for different models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List names of all stored keys\\nget Return the value of a stored key\\npath Output the path to the keys.json file\\nset Save a key in the keys.json file\\nllm keys list –help\\nUsage: llm keys list [OPTIONS]\\nList names of all stored keys\\nOptions:\\n-h, --help Show this message and exit.\\n132 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 138, 'page_label': '133'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm keys path –help\\nUsage: llm keys path [OPTIONS]\\nOutput the path to the keys.json file\\nOptions:\\n-h, --help Show this message and exit.\\nllm keys get –help\\nUsage: llm keys get [OPTIONS] NAME\\nReturn the value of a stored key\\nExample usage:\\nexport OPENAI_API_KEY=$(llm keys get openai)\\nOptions:\\n-h, --help Show this message and exit.\\nllm keys set –help\\nUsage: llm keys set [OPTIONS] NAME\\nSave a key in the keys.json file\\nExample usage:\\n$ llm keys set openai\\nEnter key: ...\\nOptions:\\n--value TEXT Value to set\\n-h, --help Show this message and exit.\\nllm logs –help\\nUsage: llm logs [OPTIONS] COMMAND [ARGS]...\\nTools for exploring logged prompts and responses\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\n(continues on next page)\\n2.15. CLI reference 133'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 139, 'page_label': '134'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nlist* Show logged prompts and their responses\\nbackup Backup your logs database to this file\\noff Turn off logging for all prompts\\non Turn on logging for all prompts\\npath Output the path to the logs.db file\\nstatus Show current status of database logging\\nllm logs path –help\\nUsage: llm logs path [OPTIONS]\\nOutput the path to the logs.db file\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs status –help\\nUsage: llm logs status [OPTIONS]\\nShow current status of database logging\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs backup –help\\nUsage: llm logs backup [OPTIONS] PATH\\nBackup your logs database to this file\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs on –help\\nUsage: llm logs on [OPTIONS]\\nTurn on logging for all prompts\\nOptions:\\n-h, --help Show this message and exit.\\n134 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 140, 'page_label': '135'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm logs off –help\\nUsage: llm logs off [OPTIONS]\\nTurn off logging for all prompts\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs list –help\\nUsage: llm logs list [OPTIONS]\\nShow logged prompts and their responses\\nOptions:\\n-n, --count INTEGER Number of entries to show - defaults to 3, use 0\\nfor all\\n-d, --database FILE Path to log database\\n-m, --model TEXT Filter by model or model alias\\n-q, --query TEXT Search for logs matching this string\\n-f, --fragment TEXT Filter for prompts using these fragments\\n-T, --tool TEXT Filter for prompts with results from these tools\\n--tools Filter for prompts with results from any tools\\n--schema TEXT JSON schema, filepath or ID\\n--schema-multi TEXT JSON schema used for multiple results\\n-l, --latest Return latest results matching search query\\n--data Output newline-delimited JSON data for schema\\n--data-array Output JSON array of data for schema\\n--data-key TEXT Return JSON objects from array in this key\\n--data-ids Attach corresponding IDs to JSON objects\\n-t, --truncate Truncate long strings in output\\n-s, --short Shorter YAML output with truncated prompts\\n-u, --usage Include token usage\\n-r, --response Just output the last response\\n-x, --extract Extract first fenced code block\\n--xl, --extract-last Extract last fenced code block\\n-c, --current Show logs from the current conversation\\n--cid, --conversation TEXT Show logs for this conversation ID\\n--id-gt TEXT Return responses with ID > this\\n--id-gte TEXT Return responses with ID >= this\\n--json Output logs as JSON\\n-e, --expand Expand fragments to show their content\\n-h, --help Show this message and exit.\\n2.15. CLI reference 135'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 141, 'page_label': '136'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm models –help\\nUsage: llm models [OPTIONS] COMMAND [ARGS]...\\nManage available models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available models\\ndefault Show or set the default model\\noptions Manage default options for models\\nllm models list –help\\nUsage: llm models list [OPTIONS]\\nList available models\\nOptions:\\n--options Show options for each model, if available\\n--async List async models\\n--schemas List models that support schemas\\n--tools List models that support tools\\n-q, --query TEXT Search for models matching these strings\\n-m, --model TEXT Specific model IDs\\n-h, --help Show this message and exit.\\nllm models default –help\\nUsage: llm models default [OPTIONS] [MODEL]\\nShow or set the default model\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options –help\\nUsage: llm models options [OPTIONS] COMMAND [ARGS]...\\nManage default options for models\\nOptions:\\n-h, --help Show this message and exit.\\n(continues on next page)\\n136 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 142, 'page_label': '137'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nCommands:\\nlist* List default options for all models\\nclear Clear default option(s) for a model\\nset Set a default option for a model\\nshow List default options set for a specific model\\nllm models options list –help\\nUsage: llm models options list [OPTIONS]\\nList default options for all models\\nExample usage:\\nllm models options list\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options show –help\\nUsage: llm models options show [OPTIONS] MODEL\\nList default options set for a specific model\\nExample usage:\\nllm models options show gpt-4o\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options set –help\\nUsage: llm models options set [OPTIONS] MODEL KEY VALUE\\nSet a default option for a model\\nExample usage:\\nllm models options set gpt-4o temperature 0.5\\nOptions:\\n-h, --help Show this message and exit.\\n2.15. CLI reference 137'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 143, 'page_label': '138'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm models options clear –help\\nUsage: llm models options clear [OPTIONS] MODEL [KEY]\\nClear default option(s) for a model\\nExample usage:\\nllm models options clear gpt-4o\\n# Or for a single option\\nllm models options clear gpt-4o temperature\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates –help\\nUsage: llm templates [OPTIONS] COMMAND [ARGS]...\\nManage stored prompt templates\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available prompt templates\\nedit Edit the specified prompt template using the default $EDITOR\\nloaders Show template loaders registered by plugins\\npath Output the path to the templates directory\\nshow Show the specified prompt template\\nllm templates list –help\\nUsage: llm templates list [OPTIONS]\\nList available prompt templates\\nOptions:\\n-h, --help Show this message and exit.\\n138 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 144, 'page_label': '139'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm templates show –help\\nUsage: llm templates show [OPTIONS] NAME\\nShow the specified prompt template\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates edit –help\\nUsage: llm templates edit [OPTIONS] NAME\\nEdit the specified prompt template using the default $EDITOR\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates path –help\\nUsage: llm templates path [OPTIONS]\\nOutput the path to the templates directory\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates loaders –help\\nUsage: llm templates loaders [OPTIONS]\\nShow template loaders registered by plugins\\nOptions:\\n-h, --help Show this message and exit.\\nllm schemas –help\\nUsage: llm schemas [OPTIONS] COMMAND [ARGS]...\\nManage stored schemas\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\n(continues on next page)\\n2.15. CLI reference 139'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 145, 'page_label': '140'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nlist* List stored schemas\\ndsl Convert LLM 's schema DSL to a JSON schema\\nshow Show a stored schema\\nllm schemas list –help\\nUsage: llm schemas list [OPTIONS]\\nList stored schemas\\nOptions:\\n-d, --database FILE Path to log database\\n-q, --query TEXT Search for schemas matching this string\\n--full Output full schema contents\\n--json Output as JSON\\n--nl Output as newline-delimited JSON\\n-h, --help Show this message and exit.\\nllm schemas show –help\\nUsage: llm schemas show [OPTIONS] SCHEMA_ID\\nShow a stored schema\\nOptions:\\n-d, --database FILE Path to log database\\n-h, --help Show this message and exit.\\nllm schemas dsl –help\\nUsage: llm schemas dsl [OPTIONS] INPUT\\nConvert LLM's schema DSL to a JSON schema\\nllm schema dsl 'name, age int, bio: their bio'\\nOptions:\\n--multi Wrap in an array\\n-h, --help Show this message and exit.\\n140 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 146, 'page_label': '141'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm tools –help\\nUsage: llm tools [OPTIONS] COMMAND [ARGS]...\\nManage tools that can be made available to LLMs\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available tools that have been provided by plugins\\nllm tools list –help\\nUsage: llm tools list [OPTIONS]\\nList available tools that have been provided by plugins\\nOptions:\\n--json Output as JSON\\n--functions TEXT Python code block or file path defining functions to\\nregister as tools\\n-h, --help Show this message and exit.\\nllm aliases –help\\nUsage: llm aliases [OPTIONS] COMMAND [ARGS]...\\nManage model aliases\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List current aliases\\npath Output the path to the aliases.json file\\nremove Remove an alias\\nset Set an alias for a model\\nllm aliases list –help\\nUsage: llm aliases list [OPTIONS]\\nList current aliases\\nOptions:\\n--json Output as JSON\\n-h, --help Show this message and exit.\\n2.15. CLI reference 141'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 147, 'page_label': '142'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm aliases set –help\\nUsage: llm aliases set [OPTIONS] ALIAS [MODEL_ID]\\nSet an alias for a model\\nExample usage:\\nllm aliases set mini gpt-4o-mini\\nAlternatively you can omit the model ID and specify one or more -q options.\\nThe first model matching all of those query strings will be used.\\nllm aliases set mini -q 4o -q mini\\nOptions:\\n-q, --query TEXT Set alias for model matching these strings\\n-h, --help Show this message and exit.\\nllm aliases remove –help\\nUsage: llm aliases remove [OPTIONS] ALIAS\\nRemove an alias\\nExample usage:\\n$ llm aliases remove turbo\\nOptions:\\n-h, --help Show this message and exit.\\nllm aliases path –help\\nUsage: llm aliases path [OPTIONS]\\nOutput the path to the aliases.json file\\nOptions:\\n-h, --help Show this message and exit.\\n142 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 148, 'page_label': '143'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm fragments –help\\nUsage: llm fragments [OPTIONS] COMMAND [ARGS]...\\nManage fragments that are stored in the database\\nFragments are reusable snippets of text that are shared across multiple\\nprompts.\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List current fragments\\nloaders Show fragment loaders registered by plugins\\nremove Remove a fragment alias\\nset Set an alias for a fragment\\nshow Display the fragment stored under an alias or hash\\nllm fragments list –help\\nUsage: llm fragments list [OPTIONS]\\nList current fragments\\nOptions:\\n-q, --query TEXT Search for fragments matching these strings\\n--aliases Show only fragments with aliases\\n--json Output as JSON\\n-h, --help Show this message and exit.\\nllm fragments set –help\\nUsage: llm fragments set [OPTIONS] ALIAS FRAGMENT\\nSet an alias for a fragment\\nAccepts an alias and a file path, URL, hash or '-' for stdin\\nExample usage:\\nllm fragments set mydocs ./docs.md\\nOptions:\\n-h, --help Show this message and exit.\\n2.15. CLI reference 143\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 149, 'page_label': '144'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm fragments show –help\\nUsage: llm fragments show [OPTIONS] ALIAS_OR_HASH\\nDisplay the fragment stored under an alias or hash\\nllm fragments show mydocs\\nOptions:\\n-h, --help Show this message and exit.\\nllm fragments remove –help\\nUsage: llm fragments remove [OPTIONS] ALIAS\\nRemove a fragment alias\\nExample usage:\\nllm fragments remove docs\\nOptions:\\n-h, --help Show this message and exit.\\nllm fragments loaders –help\\nUsage: llm fragments loaders [OPTIONS]\\nShow fragment loaders registered by plugins\\nOptions:\\n-h, --help Show this message and exit.\\nllm plugins –help\\nUsage: llm plugins [OPTIONS]\\nList installed plugins\\nOptions:\\n--all Include built- in default plugins\\n--hook TEXT Filter for plugins that implement this hook\\n-h, --help Show this message and exit.\\n144 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 150, 'page_label': '145'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm install –help\\nUsage: llm install [OPTIONS] [PACKAGES]...\\nInstall packages from PyPI into the same environment as LLM\\nOptions:\\n-U, --upgrade Upgrade packages to latest version\\n-e, --editable TEXT Install a project in editable mode from this path\\n--force-reinstall Reinstall all packages even if they are already up-to-\\ndate\\n--no-cache-dir Disable the cache\\n--pre Include pre-release and development versions\\n-h, --help Show this message and exit.\\nllm uninstall –help\\nUsage: llm uninstall [OPTIONS] PACKAGES...\\nUninstall Python packages from the LLM environment\\nOptions:\\n-y, --yes Don 't ask for confirmation\\n-h, --help Show this message and exit.\\nllm embed –help\\nUsage: llm embed [OPTIONS] [COLLECTION] [ID]\\nEmbed text and store or return the result\\nOptions:\\n-i, --input PATH File to embed\\n-m, --model TEXT Embedding model to use\\n--store Store the text itself in the database\\n-d, --database FILE\\n-c, --content TEXT Content to embed\\n--binary Treat input as binary data\\n--metadata TEXT JSON object metadata to store\\n-f, --format [json|blob|base64|hex]\\nOutput format\\n-h, --help Show this message and exit.\\n2.15. CLI reference 145\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 151, 'page_label': '146'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm embed-multi –help\\nUsage: llm embed-multi [OPTIONS] COLLECTION [INPUT_PATH]\\nStore embeddings for multiple strings at once in the specified collection.\\nInput data can come from one of three sources:\\n1. A CSV, TSV, JSON or JSONL file:\\n- CSV/TSV: First column is ID, remaining columns concatenated as content\\n- JSON: Array of objects with \"id\" field and content fields\\n- JSONL: Newline-delimited JSON objects\\nExamples:\\nllm embed-multi docs input.csv\\ncat data.json | llm embed-multi docs -\\nllm embed-multi docs input.json --format json\\n2. A SQL query against a SQLite database:\\n- First column returned is used as ID\\n- Other columns concatenated to form content\\nExamples:\\nllm embed-multi docs --sql \"SELECT id, title, body FROM posts\"\\nllm embed-multi docs --attach blog blog.db --sql \"SELECT id, content FROM blog.\\n˓→posts\"\\n3. Files in directories matching glob patterns:\\n- Each file becomes one embedding\\n- Relative file paths become IDs\\nExamples:\\nllm embed-multi docs --files docs \\'**/*.md\\'\\nllm embed-multi images --files photos \\'*.jpg\\' --binary\\nllm embed-multi texts --files texts \\'*.txt\\' --encoding utf-8 --encoding latin-1\\nOptions:\\n--format [json|csv|tsv|nl] Format of input file - defaults to auto-detect\\n--files <DIRECTORY TEXT>... Embed files in this directory - specify directory\\nand glob pattern\\n--encoding TEXT Encodings to try when reading --files\\n--binary Treat --files as binary data\\n--sql TEXT Read input using this SQL query\\n--attach <TEXT FILE>... Additional databases to attach - specify alias\\nand file path\\n--batch-size INTEGER Batch size to use when running embeddings\\n--prefix TEXT Prefix to add to the IDs\\n-m, --model TEXT Embedding model to use\\n--prepend TEXT Prepend this string to all content before\\nembedding\\n--store Store the text itself in the database\\n-d, --database FILE\\n-h, --help Show this message and exit.\\n146 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 152, 'page_label': '147'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm similar –help\\nUsage: llm similar [OPTIONS] COLLECTION [ID]\\nReturn top N similar IDs from a collection using cosine similarity.\\nExample usage:\\nllm similar my-collection -c \"I like cats\"\\nOr to find content similar to a specific stored ID:\\nllm similar my-collection 1234\\nOptions:\\n-i, --input PATH File to embed for comparison\\n-c, --content TEXT Content to embed for comparison\\n--binary Treat input as binary data\\n-n, --number INTEGER Number of results to return\\n-p, --plain Output in plain text format\\n-d, --database FILE\\n--prefix TEXT Just IDs with this prefix\\n-h, --help Show this message and exit.\\nllm embed-models –help\\nUsage: llm embed-models [OPTIONS] COMMAND [ARGS]...\\nManage available embedding models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available embedding models\\ndefault Show or set the default embedding model\\nllm embed-models list –help\\nUsage: llm embed-models list [OPTIONS]\\nList available embedding models\\nOptions:\\n-q, --query TEXT Search for embedding models matching these strings\\n-h, --help Show this message and exit.\\n2.15. CLI reference 147'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 153, 'page_label': '148'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm embed-models default –help\\nUsage: llm embed-models default [OPTIONS] [MODEL]\\nShow or set the default embedding model\\nOptions:\\n--remove-default Reset to specifying no default model\\n-h, --help Show this message and exit.\\nllm collections –help\\nUsage: llm collections [OPTIONS] COMMAND [ARGS]...\\nView and manage collections of embeddings\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* View a list of collections\\ndelete Delete the specified collection\\npath Output the path to the embeddings database\\nllm collections path –help\\nUsage: llm collections path [OPTIONS]\\nOutput the path to the embeddings database\\nOptions:\\n-h, --help Show this message and exit.\\nllm collections list –help\\nUsage: llm collections list [OPTIONS]\\nView a list of collections\\nOptions:\\n-d, --database FILE Path to embeddings database\\n--json Output as JSON\\n-h, --help Show this message and exit.\\n148 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 154, 'page_label': '149'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nllm collections delete –help\\nUsage: llm collections delete [OPTIONS] COLLECTION\\nDelete the specified collection\\nExample usage:\\nllm collections delete my-collection\\nOptions:\\n-d, --database FILE Path to embeddings database\\n-h, --help Show this message and exit.\\nllm openai –help\\nUsage: llm openai [OPTIONS] COMMAND [ARGS]...\\nCommands for working directly with the OpenAI API\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nmodels List models available to you from the OpenAI API\\nllm openai models –help\\nUsage: llm openai models [OPTIONS]\\nList models available to you from the OpenAI API\\nOptions:\\n--json Output as JSON\\n--key TEXT OpenAI API key\\n-h, --help Show this message and exit.\\n2.16 Contributing\\nTo contribute to this tool, first checkout the code. Then create a new virtual environment:\\ncd llm\\npython -m venv venv\\nsource venv/bin/activate\\nOr if you are usingpipenv:\\n2.16. Contributing 149'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 155, 'page_label': '150'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\npipenv shell\\nNow install the dependencies and test dependencies:\\npip install -e \\'.[test]\\'\\nTo run the tests:\\npytest\\n2.16.1 Updating recorded HTTP API interactions and associated snapshots\\nThisprojectusespytest-recordingtorecordOpenAIAPIresponsesforsomeofthetests,andsyrupytocapturesnapshots\\nof their results.\\nIf you add a new test that calls the API you can capture the API response and snapshot like this:\\nPYTEST_OPENAI_API_KEY=\"$(llm keys get openai)\" pytest --record-mode once --snapshot-\\n˓→update\\nThen review the new snapshots intests/__snapshots__/to make sure they look correct.\\n2.16.2 Debugging tricks\\nThedefaultOpenAIpluginhasadebuggingmechanismforshowingtheexactrequestsandresponsesthatweresentto\\nthe OpenAI API.\\nSet theLLM_OPENAI_SHOW_RESPONSESenvironment variable like this:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt \\'three word slogan for an an otter-run bakery\\'\\nThis will output details of the API requests and responses to the console.\\nUse --no-streamto see a more readable version of the body that avoids streaming the response:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt --no-stream \\\\\\n\\'three word slogan for an an otter-run bakery\\'\\n2.16.3 Documentation\\nDocumentation for this project uses MyST - it is written in Markdown and rendered using Sphinx.\\nTo build the documentation locally, run the following:\\ncd docs\\npip install -r requirements.txt\\nmake livehtml\\nThis will start a live preview server, using sphinx-autobuild.\\nThe CLI--helpexamples in the documentation are managed using Cog. Update those files like this:\\njust cog\\n150 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 156, 'page_label': '151'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nYou’ll need Just installed to run this command.\\n2.16.4 Release process\\nTo release a new version:\\n1. Updatedocs/changelog.mdwith the new changes.\\n2. Update the version number inpyproject.toml\\n3. Create a GitHub release for the new version.\\n4. Wait for the package to push to PyPI and then...\\n5. Run the regenerate.yaml workflow to update the Homebrew tap to the latest version.\\n2.17 Changelog\\n2.17.1 0.26 (2025-05-27)\\nTool supportis finally here! This release adds support exposingtools to LLMs, previously described in the release\\nnotes for0.26a0 and 0.26a1.\\nRead Large Language Models can run tools in your terminal with LLM 0.26for a detailed overview of the new\\nfeatures.\\nAlso in this release:\\n• Two newdefault tools: llm_version()and llm_time(). #1096, #1103\\n• Documentation onhow to add tool supports to a model plugin. #1000\\n• Added aprominent warningabout the risk of prompt injection when using tools. #1097\\n• SwitchedtousingmonotonicULIDsfortheresponseIDsinthelogs,fixingsomeintermittenttestfailures. #1099\\n• Newtool_instancestable records details of Toolbox instances created while executing a prompt. #1089\\n• llm.get_key()is now adocumented utility function. #1094\\n2.17.2 0.26a1 (2025-05-25)\\nHopefully the last alpha before a stable release that includes tool support.\\nFeatures\\n• Plugin-provided tools can now be grouped into “Toolboxes”.\\n– Toolboxes(llm.Toolboxclasses)allowpluginstoexposemultiplerelatedtoolsthatsharestateorconfig-\\nuration (e.g., aMemorytool orFilesystemtool). (#1059, #1086)\\n• Tool support forllm chat.\\n– The llm chat command now accepts--tool and --functions arguments, allowing interactive chat\\nsessions to use tools. (#1004, #1062)\\n• Tools can now execute asynchronously.\\n2.17. Changelog 151'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 157, 'page_label': '152'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n– Models that implementAsyncModel can now run tools, including tool functions defined asasync def.\\n(#1063)\\n• llm chat now supports adding fragments during a session.\\n– Usethenew !fragment <id>commandwhilechattingtoinsertcontentfromafragment. Initialfragments\\ncan also be passed tollm chatusing -f or --sf. Thanks, Dan Turkel. (#1044, #1048)\\n• Filter llm logs by tools.\\n– New--tool <name>optiontofilterlogstoshowonlyresponsesthatinvolvedaspecifictool(e.g., --tool\\nsimple_eval).\\n– The --toolsflag shows all responses that used any tool. (#1013, #1072)\\n• llm schemas list can output JSON.\\n– Added--jsonand --nl(newline-delimitedJSON)optionsto llm schemas listforprogrammaticac-\\ncess to saved schema definitions. (#1070)\\n• Filter llm similar results by ID prefix.\\n– The new--prefix option forllm similar allows searching for similar items only within IDs that start\\nwithaspecifiedstring(e.g., llm similar my-collection --prefix 'docs/'). Thanks,DanTurkel.\\n(#1052)\\n• Control chained tool execution limit.\\n– New --chain-limit <N> (or --cl) option forllm prompt and llm chat to specify the maximum\\nnumberofconsecutivetoolcallsallowedforasingleprompt. Defaultsto5; setto0forunlimited. (#1025)\\n• llm plugins --hook <NAME> option.\\n– Filter the list of installed plugins to only show those that implement a specific plugin hook. (#1047)\\n• llm tools listnow shows toolboxes and their methods. (#1013)\\n• llm promptandllm chatnowautomaticallyre-enableplugin-providedtoolswhencontinuingaconversation\\n(-cor --cid). (#1020)\\n• The --tools-debugoption now pretty-prints JSON tool results for improved readability. (#1083)\\n• NewLLM_TOOLS_DEBUGenvironment variable to permanently enable--tools-debug. (#1045)\\n• llm chat sessions now correctly respect default model options configured withllm models set-options.\\nThanks, André Arko. (#985)\\n• New--preoption forllm installto allow installing pre-release packages. (#1060)\\n• OpenAI models (gpt-4o, gpt-4o-mini) now explicitly declare support for tools and vision. (#1037)\\n• The supports_toolsparameter is now supported inextra-openai-models.yaml. Thanks, Mahesh Hegde\\n. (#1068)\\n152 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 158, 'page_label': '153'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nBug fixes\\n• Fixedabugwherethe nameparameterin register(function, name=\"name\")wasignoredfortoolplugins.\\n(#1032)\\n• Ensure pathlib.Path objects are cast to str before passing to click.edit in llm templates edit.\\nThanks, Abizer Lokhandwala. (#1031)\\n2.17.3 0.26a0 (2025-05-13)\\nThis is the first alpha to introducesupport for tools! Models with tool capability (which includes the default OpenAI\\nmodel family) can now be granted access to execute Python functions as part of responding to a prompt.\\nTools are supported bythe command-line interface:\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\'\\nAnd inthe Python API, using a newmodel.chain()method for executing multiple prompts in a sequence:\\nimport llm\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.chain(\\n\"What is 34234 * 213345?\",\\ntools=[multiply]\\n)\\nprint(response.text())\\nNew tools can also be defined using theregister_tools() plugin hook. They can then be called by name from the\\ncommand-line like this:\\nllm -T multiply \\'What is 34234 * 213345?\\'\\nTool support is currently underactive development. Consult this milestone for the latest status.\\n2.17.4 0.25 (2025-05-04)\\n• New plugin feature:register_fragment_loaders(register)plugins can now return a mixture of fragments and\\nattachments. The llm-video-frames plugin is the first to take advantage of this mechanism. #972\\n• New OpenAI models:gpt-4.1,gpt-4.1-mini, gpt-41-nano,o3,o4-mini. #945, #965, #976.\\n• Newenvironmentvariables: LLM_MODELandLLM_EMBEDDING_MODELforsettingthemodeltousewithoutneed-\\ning to specify-m model_idevery time. #932\\n• New command:llm fragments loaders, to list all currently available fragment loader prefixes provided by\\nplugins. #941\\n2.17. Changelog 153'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 159, 'page_label': '154'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n• llm fragmentscommand now shows fragments ordered by the date they were first used. #973\\n• llm chatnowincludesa !editcommandforeditingapromptusingyourdefaultterminaltexteditor. Thanks,\\nBenedikt Willi. #969\\n• Allow-tand --systemto be used at the same time. #916\\n• Fixed a bug where accessing a model via its alias would fail to respect any default options set for that model.\\n#968\\n• Improved documentation forextra-openai-models.yaml. Thanks, Rahim Nathwani and Dan Guido. #950, #957\\n• llm -c/--continue now works correctly with the-d/--database option. llm chat now accepts that-d/\\n--databaseoption. Thanks, Sukhbinder Singh. #933\\n2.17.5 0.25a0 (2025-04-10)\\n• llm models --options now shows keys and environment variables for models that use API keys. Thanks,\\nSteve Morin. #903\\n• Added py.typed marker file so LLM can now be used as a dependency in projects that usemypy without a\\nwarning. #887\\n• $characters can now be used in templates by escaping them as$$. Thanks, @guspix. #904\\n• LLM now usespyproject.tomlinstead ofsetup.py. #908\\n2.17.6 0.24.2 (2025-04-08)\\n• Fixed a bug on Windows with the newllm -t path/to/file.yamlfeature. #901\\n2.17.7 0.24.1 (2025-04-08)\\n• Templates can now be specified as a path to a file on disk, usingllm -t path/to/file.yaml. This makes\\nthem consistent with how-f fragments are loaded. #897\\n• llm logs backup /tmp/backup.dbcommand forbacking up yourlogs.dbdatabase. #879\\n2.17.8 0.24 (2025-04-07)\\nSupport forfragmentsto help assemble prompts for long context models. Improved support fortemplatesto support\\nattachments and fragments. New plugin hooks for providing custom loaders for both templates and fragments. See\\nLong context support in LLM 0.24 using fragments and template plugins for more on this release.\\nThe new llm-docs plugin demonstrates these new features. Install it like this:\\nllm install llm-docs\\nNow you can ask questions of the LLM documentation like this:\\nllm -f docs: 'How do I save a new template?'\\nThe docs: prefix is registered by the plugin. The plugin fetches the LLM documentation for your installed version\\n(from the docs-for-llms repository) and uses that as a prompt fragment to help answer your question.\\nTwo more new plugins are llm-templates-github and llm-templates-fabric.\\n154 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 160, 'page_label': '155'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\nllm-templates-githubletsyoushareandusetemplatesonGitHub. YoucanrunmyPelicanridingabicyclebench-\\nmark against a model like this:\\nllm install llm-templates-github\\nllm -t gh:simonw/pelican-svg -m o3-mini\\nThis executes this pelican-svg.yaml template stored in my simonw/llm-templates repository, using a new repository\\nnaming convention.\\nTo share your own templates, create a repository on GitHub under your user account calledllm-templatesand start\\nsaving.yamlfiles to it.\\nllm-templates-fabric provides a similar mechanism for loading templates from Daniel Miessler’s fabric collection:\\nllm install llm-templates-fabric\\ncurl https://simonwillison.net/2025/Apr/6/only-miffy/ | \\\\\\nllm -t f:extract_main_idea\\nMajor new features:\\n• Newfragmentsfeature. Fragmentscanbeusedtoassemblelongpromptsfrommultipleexistingpieces-URLs,\\nfile paths or previously used fragments. These will be stored de-duplicated in the database avoiding wasting\\nspace storing multiple long context pieces. Example usage:llm -f https://llm.datasette.io/robots.\\ntxt 'explain this file'. #617\\n• The llm logs file now accepts-f fragment references too, and will show just logged prompts that used those\\nfragments.\\n• register_template_loaders()pluginhook allowingpluginstoregisternew prefix:valuecustomtemplateload-\\ners. #809\\n• register_fragment_loaders()pluginhook allowingpluginstoregisternew prefix:valuecustomfragmentload-\\ners. #886\\n• llm fragmentsfamily of commands for browsing fragments that have been previously logged to the database.\\n• Thenewllm-openaipluginprovidessupportfor o1-pro(whichisnotsupportedbytheOpenAImechanismused\\nby LLM core). Future OpenAI features will migrate to this plugin instead of LLM core itself.\\nImprovements to templates:\\n• llm -t $URLoption can now take a URL to a YAML template. #856\\n• Templates can now store default model options. #845\\n• Executing a template that does not use the$inputvariable no longer blocks LLM waiting for input, so prompt\\ntemplates can now be used to try different models usingllm -t pelican-svg -m model_id. #835\\n• llm templatescommand no longer crashes if one of the listed template files contains invalid YAML. #880\\n• Attachments can now be stored in templates. #826\\nOther changes:\\n• Newllm models optionsfamily of commands for setting default options for particular models. #829\\n• llm logs list,llm schemas listandllm schemas showallnowtakea -d/--databaseoptionwithan\\noptionalpathtoaSQLitedatabase. Theyusedtotake -p/--pathbutthatwasinconsistentwithothercommands.\\n-p/--pathstill works but is excluded from--helpand will be removed in a future LLM release. #857\\n• llm logs -e/--expandoption for expanding fragments. #881\\n• llm prompt -d path-to-sqlite.db option can now be used to write logs to a custom SQLite database.\\n#858\\n2.17. Changelog 155\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 161, 'page_label': '156'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n• llm similar -p/--plainoption providing more human-readable output than the default JSON. #853\\n• llm logs -s/--shortnow truncates to include the end of the prompt too. Thanks, Sukhbinder Singh. #759\\n• Setthe LLM_RAISE_ERRORS=1environmentvariabletoraiseerrorsduringpromptsratherthansuppressingthem,\\nwhichmeansyoucanrun python -i -m llm 'prompt' andthendropintoadebuggeronerrorswith import\\npdb; pdb.pm(). #817\\n• Improved –help output forllm embed-multi. #824\\n• llm models -m X option which can be passed multiple times with model IDs to see the details of just those\\nmodels. #825\\n• OpenAI models now accept PDF attachments. #834\\n• llm prompt -q gpt -q 4ooption - pass-q searchtermone or more times to execute a prompt against the\\nfirst model that matches all of those strings - useful for if you can’t remember the full model ID. #841\\n• OpenAIcompatible modelsconfiguredusing extra-openai-models.yamlnowsupport supports_schema:\\ntrue, vision: true and audio: true options. Thanks @adaitche and @giuli007. #819, #843\\n2.17.9 0.24a1 (2025-04-06)\\n• New Fragments feature. #617\\n• register_fragment_loaders()plugin hook. #809\\n2.17.10 0.24a0 (2025-02-28)\\n• Alpha release with experimentalregister_template_loaders()plugin hook. #809\\n2.17.11 0.23 (2025-02-28)\\nSupport forschemas, for getting supported models to output JSON that matches a specified JSON schema. See also\\nStructured data extraction from unstructured content using LLM schemas for background on this feature. #776\\n• New llm prompt --schema '{JSON schema goes here} option for specifying a schema that should be\\nused for the output from the model. Theschemas documentationhas more details and a tutorial.\\n• Schemascanalsobedefinedusinga conciseschemaspecification ,forexample llm prompt --schema 'name,\\nbio, age int'. #790\\n• Schemas can also be specified by passing a filename and throughseveral other methods. #780\\n• Newllmschemasfamilyofcommands : llm schemas list,llm schemas show,and llm schemas dslfor\\ndebugging the new concise schema language. #781\\n• Schemas can now be saved to templates usingllm --schema X --save template-name or through modi-\\nfying thetemplate YAML. #778\\n• Thellmlogs commandnowhasnewoptionsforextractingdatacollectedusingschemas: --data,--data-key,\\n--data-array, --data-ids. #782\\n• Newllm logs --id-gt Xand --id-gte Xoptions. #801\\n• Newllm models --schemasoption for listing models that support schemas. #797\\n• model.prompt(..., schema={...}) parameter for specifying a schema from Python. This accepts either a\\ndictionary JSON schema definition or a PydanticBaseModelsubclass, seeschemas in the Python API docs.\\n156 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 162, 'page_label': '157'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n• The default OpenAI plugin now enables schemas across all supported models. Runllm models --schemas\\nfor a list of these.\\n• The llm-anthropic and llm-gemini plugins have been upgraded to add schema support for those models. Here’s\\ndocumentation on how toadd schema support to a model plugin.\\nOther smaller changes:\\n• GPT-4.5 preview is now a supported model:llm -m gpt-4.5 \\'a joke about a pelican and a wolf\\'\\n#795\\n• The prompt string is now optional when calling model.prompt() from the Python API, so model.\\nprompt(attachments=llm.Attachment(url=url)))now works. #784\\n• extra-openai-models.yamlnowsupportsa reasoning: true option. Thanks,KasperPrimdalLauritzen.\\n#766\\n• LLM now depends on Pydantic v2 or higher. Pydantic v1 is no longer supported. #520\\n2.17.12 0.22 (2025-02-16)\\nSee also LLM 0.22, the annotated release notes.\\n• Plugins that provide models that use API keys can now subclass the new llm.KeyModel and llm.\\nAsyncKeyModelclasses. ThisresultsintheAPIkeybeingpassedasanew keyparametertotheir .execute()\\nmethods, and means that Python users can pass a key as themodel.prompt(..., key=)- seePassing an API\\nkey. Plugin developers should consult the new documentation on writingModels that accept API keys. #744\\n• New OpenAI model:chatgpt-4o-latest. This model ID accesses the current model being used to power\\nChatGPT, which can change without warning. #752\\n• New llm logs -s/--short flag, which returns a greatly shortened version of the matching log entries in\\nYAML format with a truncated prompt and without including the response. #737\\n• Bothllm modelsandllm embed-modelsnowtakemultiple -qsearchfragments. Youcannowsearchforall\\nmodels matching “gemini” and “exp” usingllm models -q gemini -q exp. #748\\n• New llm embed-multi --prepend X option for prepending a string to each value before it is embed-\\nded - useful for models such as nomic-embed-text-v2-moe that require passages to start with a string like\\n\"search_document: \" . #745\\n• The response.json()and response.usage()methods arenow documented.\\n• Fixed a bug where conversations that were loaded from the database could not be continued usingasyncio\\nprompts. #742\\n• New plugin for macOS users: llm-mlx, which provides extremely high performance access to a wide range of\\nlocal models using Apple’s MLX framework.\\n• The llm-claude-3plugin has been renamed to llm-anthropic.\\n2.17. Changelog 157'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 163, 'page_label': '158'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.17.13 0.21 (2025-01-31)\\n• New model:o3-mini. #728\\n• The o3-mini and o1 models now support areasoning_effort option which can be set tolow, medium or\\nhigh.\\n• llm promptandllm logsnowhavea --xl/--extract-lastoptionforextractingthelastfencedcodeblock\\nin the response - a complement to the existing--x/--extractoption. #717\\n2.17.14 0.20 (2025-01-22)\\n• New model,o1. This model does not yet support streaming. #676\\n• o1-previewand o1-minimodels now support streaming.\\n• New models,gpt-4o-audio-previewand gpt-4o-mini-audio-preview. #677\\n• llm prompt -x/--extractoption,whichreturnsjustthecontentofthefirstfencedcodeblockintheresponse.\\nTryllm prompt -x 'Python function to reverse a string'. #681\\n– Creatingatemplateusing llm ... --save xnowsupportsthe -x/--extractoption,whichissavedto\\nthe template. YAML templates can set this option usingextract: true .\\n– New llm logs -x/--extract option extracts the first fenced code block from matching logged re-\\nsponses.\\n• Newllm models -q 'search' option returning models that case-insensitively match the search query. #700\\n• Installation documentation now also includesuv. Thanks, Ariel Marcus. #690 and #702\\n• llm models command now shows the current default model at the bottom of the listing. Thanks, Amjith Ra-\\nmanujam. #688\\n• Plugin directorynow includesllm-venice,llm-bedrock, llm-deepseekand llm-cmd-comp.\\n• Fixed bug where some dependency version combinations could cause a Client.__init__() got an\\nunexpected keyword argument 'proxies' error. #709\\n• OpenAI embedding models are now available using their full names of text-embedding-ada-002,\\ntext-embedding-3-smallandtext-embedding-3-large-thepreviousnamesarestillsupportedasaliases.\\nThanks, web-sst. #654\\n2.17.15 0.19.1 (2024-12-05)\\n• FIxedbugwhere llm.get_models()andllm.get_async_models()returnedthesamemodelmultipletimes.\\n#667\\n2.17.16 0.19 (2024-12-01)\\n• Tokens used by a response are now logged to newinput_tokens and output_tokens integer columns and\\na token_details JSON string column, for the default OpenAI models and models from other plugins that\\nimplement this feature. #610\\n• llm promptnow takes a-u/--usageflag to display token usage at the end of the response.\\n• llm logs -u/--usageshows token usage information for logged responses.\\n• llm prompt ... --asyncresponses are now logged to the database. #641\\n158 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 164, 'page_label': '159'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n• llm.get_models()and llm.get_async_models()functions,documented here. #640\\n• response.usage() and async responseawait response.usage() methods, returning aUsage(input=2,\\noutput=1, details=None)dataclass. #644\\n• response.on_done(callback)and await response.on_done(callback)methodsforspecifying acall-\\nback to be executed when a response has completed,documented here. #653\\n• Fix for bug runningllm chaton Windows 11. Thanks, Sukhbinder Singh. #495\\n2.17.17 0.19a2 (2024-11-20)\\n• llm.get_models()and llm.get_async_models()functions,documented here. #640\\n2.17.18 0.19a1 (2024-11-19)\\n• response.usage() and async responseawait response.usage() methods, returning aUsage(input=2,\\noutput=1, details=None)dataclass. #644\\n2.17.19 0.19a0 (2024-11-19)\\n• Tokens used by a response are now logged to newinput_tokens and output_tokens integer columns and\\na token_details JSON string column, for the default OpenAI models and models from other plugins that\\nimplement this feature. #610\\n• llm promptnow takes a-u/--usageflag to display token usage at the end of the response.\\n• llm logs -u/--usageshows token usage information for logged responses.\\n• llm prompt ... --asyncresponses are now logged to the database. #641\\n2.17.20 0.18 (2024-11-17)\\n• Initial support for async models. Plugins can now provide anAsyncModelsubclass that can be accessed in the\\nPython API using the newllm.get_async_model(model_id) method. Seeasync models in the Python API\\ndocs andimplementing async models in plugins. #507\\n• OpenAI models all now include async models, so function calls such as llm.\\nget_async_model(\"gpt-4o-mini\")will return an async model.\\n• gpt-4o-audio-previewmodel can be used to send audio attachments to the GPT-4o audio model. #608\\n• Attachments can now be sent without requiring a prompt. #611\\n• llm models --optionsnow includes information on whether a model supports attachments. #612\\n• llm models --asyncshows available async models.\\n• Custom OpenAI-compatible models can now be marked ascan_stream: false in the YAML if they do not\\nsupport streaming. Thanks, Chris Mungall. #600\\n• Fixed bug where OpenAI usage data was incorrectly serialized to JSON. #614\\n• Standardized onaudio/wavMIME type for audio attachments rather thanaudio/wave. #603\\n2.17. Changelog 159'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 165, 'page_label': '160'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.21 0.18a1 (2024-11-14)\\n• Fixed bug where conversations did not work for async OpenAI models. #632\\n• __repr__methods forResponseand AsyncResponse.\\n2.17.22 0.18a0 (2024-11-13)\\nAlpha support forasync models. #507\\nMultiple smaller changes.\\n2.17.23 0.17 (2024-10-29)\\nSupport forattachments, allowing multi-modal models to accept images, audio, video and other formats. #578\\nThe default OpenAIgpt-4oand gpt-4o-minimodels can both now be prompted with JPEG, GIF, PNG and WEBP\\nimages.\\nAttachmentsin the CLIcan be URLs:\\nllm -m gpt-4o \"describe this image\" \\\\\\n-a https://static.simonwillison.net/static/2024/pelicans.jpg\\nOr file paths:\\nllm -m gpt-4o-mini \"extract text\" -a image1.jpg -a image2.jpg\\nOr binary data, which may need to use--attachment-typeto specify the MIME type:\\ncat image | llm -m gpt-4o-mini \"extract text\" --attachment-type - image/jpeg\\nAttachments are also availablein the Python API:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\\n\"Describe these images\",\\nattachments=[\\nllm.Attachment(path=\"pelican.jpg\"),\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\"),\\n]\\n)\\nPlugins that provide alternative models can support attachments, seeAttachments for multi-modal modelsfor details.\\nThe latestllm-claude-3 plugin now supports attachments for Anthropic’s Claude 3 and 3.5 models. Thellm-gemini\\nplugin supports attachments for Google’s Gemini 1.5 models.\\nAlso in this release: OpenAI models now record their\"usage\" data in the database even when the response was\\nstreamed. These records can be viewed usingllm logs --json. #591\\n160 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 166, 'page_label': '161'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.24 0.17a0 (2024-10-28)\\nAlpha support forattachments. #578\\n2.17.25 0.16 (2024-09-12)\\n• OpenAImodelsnowusetheinternal self.get_key()mechanism,whichmeanstheycanbeusedfromPython\\ncode in a way that will pick up keys that have been configured usingllm keys set or theOPENAI_API_KEY\\nenvironment variable. #552. This code now works correctly:\\nimport llm\\nprint(llm.get_model(\"gpt-4o-mini\").prompt(\"hi\"))\\n• New documented API methods: llm.get_default_model(), llm.set_default_model(alias), llm.\\nget_default_embedding_model(alias), llm.set_default_embedding_model(). #553\\n• Support for OpenAI’s new o1 family of preview models, llm -m o1-preview \"prompt\" and llm -m\\no1-mini \"prompt\". These models are currently only available to tier 5 OpenAI API users, though this may\\nchange in the future. #570\\n2.17.26 0.15 (2024-07-18)\\n• Support for OpenAI’s new GPT-4o mini model: llm -m gpt-4o-mini \\'rave about pelicans in\\nFrench\\' #536\\n• gpt-4o-miniisnowthedefaultmodelifyoudonot specifyyourowndefault ,replacingGPT-3.5Turbo. GPT-4o\\nmini is both cheaper and better than GPT-3.5 Turbo.\\n• Fixedabugwhere llm logs -q \\'flourish\\' -m haikucouldnotcombineboththe -qsearchqueryandthe\\n-mmodel specifier. #515\\n2.17.27 0.14 (2024-05-13)\\n• Support for OpenAI’s new GPT-4o model:llm -m gpt-4o \\'say hi in Spanish\\' #490\\n• The gpt-4-turbo alias is now a model ID, which indicates the latest version of OpenAI’s GPT-4 Turbo\\ntext and image model. Your existinglogs.db database may contain records under the previous model ID of\\ngpt-4-turbo-preview. #493\\n• New llm logs -r/--response option for outputting just the last captured response, without wrapping it in\\nMarkdown and accompanying it with the prompt. #431\\n• Nine newplugins since version 0.13:\\n– llm-claude-3 supporting Anthropic’s Claude 3 family of models.\\n– llm-command-r supporting Cohere’s Command R and Command R Plus API models.\\n– llm-rekasupports the Reka family of models via their API.\\n– llm-perplexity by Alexandru Geana supporting the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-onlinewhichcansearchforthingsonlineand llama-3-70b-instruct.\\n– llm-groqby Moritz Angermann providing access to fast models hosted by Groq.\\n– llm-fireworkssupporting models hosted by Fireworks AI.\\n– llm-togetheradds support for the Together AI extensive family of hosted openly licensed models.\\n2.17. Changelog 161'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 167, 'page_label': '162'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n– llm-embed-onnx provides seven embedding models that can be executed using the ONNX model frame-\\nwork.\\n– llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so\\nyou can review it, edit it and then hit<enter>to execute orctrl+cto cancel, see this post for details.\\n2.17.28 0.13.1 (2024-01-26)\\n• Fix forNo module named 'readline' error on Windows. #407\\n2.17.29 0.13 (2024-01-26)\\nSee also LLM 0.13: The annotated release notes.\\n• Added support for new OpenAI embedding models:3-small and 3-large and three variants of those with\\ndifferent dimension sizes,3-small-512, 3-large-256 and 3-large-1024. See OpenAI embedding models\\nfor details. #394\\n• Thedefault gpt-4-turbomodelaliasnowpointsto gpt-4-turbo-preview,whichusesthemostrecentOpe-\\nnAI GPT-4 turbo model (currentlygpt-4-0125-preview). #396\\n• New OpenAI model aliasesgpt-4-1106-previewand gpt-4-0125-preview.\\n• OpenAI models now support a-o json_object 1 option which will cause their output to be returned as a\\nvalid JSON object. #373\\n• Newplugins since the last release include llm-mistral, llm-gemini, llm-ollama and llm-bedrock-meta.\\n• The keys.jsonfile for storing API keys is now created with600file permissions. #351\\n• Documentedapattern forinstallingpluginsthatdependonPyTorchusingtheHomebrewversionofLLM,despite\\nHomebrewusingPython3.12whenPyTorchhavenotyetreleasedastablepackageforthatPythonversion. #397\\n• Underlying OpenAI Python library has been upgraded to>1.0. It is possible this could cause compatibility\\nissues with LLM plugins that also depend on that library. #325\\n• Arrow keys now work inside thellm chatcommand. #376\\n• LLM_OPENAI_SHOW_RESPONSES=1 environment variable now outputs much more detailed information about\\nthe HTTP request and response made to OpenAI (and OpenAI-compatible) APIs. #404\\n• Dropped support for Python 3.7.\\n2.17.30 0.12 (2023-11-06)\\n• SupportforthenewGPT-4TurbomodelfromOpenAI.Tryitusing llm chat -m gpt-4-turboor llm chat\\n-m 4t. #323\\n• New-o seed 1optionforOpenAImodelswhichsetsaseedthatcanattempttoevaluatethepromptdetermin-\\nistically. #324\\n162 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 168, 'page_label': '163'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.31 0.11.2 (2023-11-06)\\n• Pin to version of OpenAI Python library prior to 1.0 to avoid breaking. #327\\n2.17.32 0.11.1 (2023-10-31)\\n• Fixed a bug wherellm embed -c \"text\" did not correctly pick up the configureddefault embedding model.\\n#317\\n• New plugins: llm-python, llm-bedrock-anthropic and llm-embed-jina (described in Execute Jina embeddings\\nwith a CLI using llm-embed-jina).\\n• llm-gpt4all now uses the new GGUF model format. simonw/llm-gpt4all#16\\n2.17.33 0.11 (2023-09-18)\\nLLMnowsupportsthenewOpenAI gpt-3.5-turbo-instructmodel,andOpenAIcompletion(asopposedtochat\\ncompletion) models in general. #284\\nllm -m gpt-3.5-turbo-instruct \\'Reasons to tame a wild beaver:\\'\\nOpenAI completion models like this support a-o logprobs 3option, which accepts a number between 1 and 5 and\\nwill include the log probabilities (for each produced token, what were the top 3 options considered by the model) in\\nthe logged response.\\nllm -m gpt-3.5-turbo-instruct \\'Say hello succinctly\\' -o logprobs 3\\nYou can then view thelogprobsthat were recorded in the SQLite logs database like this:\\nsqlite-utils \"$(llm logs path)\" \\\\\\n\\'select * from responses order by id desc limit 1\\' | \\\\\\njq \\'.[0].response_json\\' -r | jq\\nTruncated output looks like this:\\n[\\n{\\n\"text\": \"Hi\",\\n\"top_logprobs\": [\\n{\\n\"Hi\": -0.13706253,\\n\"Hello\": -2.3714375,\\n\"Hey\": -3.3714373\\n}\\n]\\n},\\n{\\n\"text\": \" there\",\\n\"top_logprobs\": [\\n{\\n\" there\": -0.96057636,\\n\"!\\\\\"\": -0.5855763,\\n\".\\\\\"\": -3.2574513\\n(continues on next page)\\n2.17. Changelog 163'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 169, 'page_label': '164'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n}\\n]\\n}\\n]\\nAlso in this release:\\n• The llm.user_dir()function, used by plugins, now ensures the directory exists before returning it. #275\\n• New LLM_OPENAI_SHOW_RESPONSES=1 environment variable for displaying the full HTTP response returned\\nby OpenAI compatible APIs. #286\\n• The llm embed-multi command now has a--batch-size X option for setting the batch size to use when\\nprocessing embeddings - useful if you have limited memory available. #273\\n• The collection.embed_multi()method also now accepts an optionalbatch_size=intargument.\\n• Fixed two bugs withllm embed-multi --filesrelating to handling of directories. Thanks, ealvar3z. #274,\\n#280\\n2.17.34 0.10 (2023-09-12)\\nThe two major features in this release are thellm chat commandand support forembedding binary data.\\nSeeBuildanimagesearchenginewithllm-clip,chatwithmodelswithllmchatformorebackgroundonthesefeatures.\\nllm chat\\nThe newllm chat command starts an ongoing chat conversation with a model in your terminal. It works with all\\nmodels supported by LLM and itsplugins, including locally installed models such as Llama 2. #231\\nThis offers a big performance boost for local models, since they don’t need to be freshly loaded into memory for each\\nprompt.\\nHere’s an example chat with Llama 2 13B, provided by the llm-mlc plugin.\\nllm chat -m mlc-chat-Llama-2-13b-chat-hf-q4f16_1\\nThis starts a chat session:\\nType 'exit' or 'quit' to exit\\nType '!multi' to enter multiple lines, then '!end' to finish\\nType '!edit' to open your default editor and modify the prompt.\\n> Who are you?\\nHello! I'm just an AI, here to assist you with any questions you may have.\\nMy name is LLaMA, and I'm a large language model trained to provide helpful\\nand informative responses to a wide range of questions and topics. I'm here\\nto help you find the information you need, while ensuring a safe and\\nrespectful conversation. Please feel free to ask me anything, and I'll do my\\nbest to provide a helpful and accurate response.\\n> Tell me a joke about otters\\nSure, here's a joke about otters:\\nWhy did the otter go to the party?\\n(continues on next page)\\n164 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 170, 'page_label': '165'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nBecause he heard it was a \"whale\" of a time!\\n(Get it? Whale, like a big sea mammal, but also a \"wild\" or \"fun\" time.\\nOtters are known for their playful and social nature, so it\\'s a lighthearted\\nand silly joke.)\\nI hope that brought a smile to your face! Do you have any other questions or\\ntopics you\\'d like to discuss?\\n> exit\\nChat sessions arelogged to SQLite- usellm logs to view them. They can accept system prompts, templates and\\nmodel options - consultthe chat documentationfor details.\\nBinary embedding support\\nLLM’sembeddings featurehas been expanded to provide support for embedding binary data, in addition to text. #254\\nThis enables models like CLIP, supported by the newllm-clipplugin.\\nCLIP is a multi-modal embedding model which can embed images and text into the same vector space. This means\\nyoucanuseittocreateanembeddingindexofphotos,andthensearchfortheembeddingvectorfor“ahappydog”and\\nget back images that are semantically closest to that string.\\nTo create embeddings for every JPEG in a directory stored in aphotoscollection, run:\\nllm install llm-clip\\nllm embed-multi photos --files photos/ \\'*.jpg\\' --binary -m clip\\nNow you can search for photos of raccoons using:\\nllm similar photos -c \\'raccoon\\'\\nThis spits out a list of images, ranked by how similar they are to the string “raccoon”:\\n{\"id\": \"IMG_4801.jpeg\", \"score\": 0.28125139257127457, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_4656.jpeg\", \"score\": 0.26626441704164294, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_2944.jpeg\", \"score\": 0.2647445926996852, \"content\": null, \"metadata\": null}\\n...\\nAlso in this release\\n• The LLM_LOAD_PLUGINS environment variablecan be used to control which plugins are loaded whenllm\\nstarts running. #256\\n• The llm plugins --alloption includes builtin plugins in the list of plugins. #259\\n• The llm embed-dbfamily of commands has been renamed tollm collections. #229\\n• llm embed-multi --files now has an--encoding option and defaults to falling back tolatin-1 if a file\\ncannot be processed asutf-8. #225\\n2.17. Changelog 165'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 171, 'page_label': '166'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.35 0.10a1 (2023-09-11)\\n• Support for embedding binary data. #254\\n• llm chatnow works for models with API keys. #247\\n• llm chat -ofor passing options to a model. #244\\n• llm chat --no-streamoption. #248\\n• LLM_LOAD_PLUGINSenvironment variable. #256\\n• llm plugins --alloption for including builtin plugins. #259\\n• llm embed-dbhas been renamed tollm collections. #229\\n• Fixed bug wherellm embed -coption was treated as a filepath, not a string. Thanks, mhalle. #263\\n2.17.36 0.10a0 (2023-09-04)\\n• Newllm chatcommand for starting an interactive terminal chat with a model. #231\\n• llm embed-multi --files now has an--encoding option and defaults to falling back tolatin-1 if a file\\ncannot be processed asutf-8. #225\\n2.17.37 0.9 (2023-09-03)\\nThe big new feature in this release is support forembeddings. See LLM now provides tools for working with embed-\\ndings for additional details.\\nEmbedding modelstake a piece of text - a word, sentence, paragraph or even a whole article, and convert that into an\\narray of floating point numbers. #185\\nThis embedding vector can be thought of as representing a position in many-dimensional-space, where the distance\\nbetweentwovectorsrepresentshowsemanticallysimilartheyaretoeachotherwithinthecontentofalanguagemodel.\\nEmbeddingscanbeusedtofind relateddocuments,andalsotoimplement semanticsearch -whereausercansearch\\nforaphraseandgetbackresultsthataresemanticallysimilartothatphraseeveniftheydonotshareanyexactkeywords.\\nLLM now provides both CLI and Python APIs for working with embeddings. Embedding models are defined by\\nplugins, so you can install additional models using theplugins mechanism.\\nThe first two embedding models supported by LLM are:\\n• OpenAI’sada-002embeddingmodel,availableviaaninexpensiveAPIifyousetanOpenAIkeyusing llm keys\\nset openai.\\n• The sentence-transformers family of models, available via the new llm-sentence-transformers plugin.\\nSee Embedding with the CLIfor detailed instructions on working with embeddings using LLM.\\nThe new commands for working with embeddings are:\\n• llm embed-calculateembeddingsforcontentandreturnthemtotheconsoleorstoretheminaSQLitedatabase.\\n• llm embed-multi- run bulk embeddings for multiple strings, using input from a CSV, TSV or JSON file, data\\nfrom a SQLite database or data found by scanning the filesystem. #215\\n• llm similar- run similarity searches against your stored embeddings - starting with a search phrase or finding\\ncontent related to a previously stored vector. #190\\n• llm embed-models- list available embedding models.\\n166 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 172, 'page_label': '167'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n• llm embed-db- commands for inspecting and working with the default embeddings SQLite database.\\nThere’s also a newllm.Collection class for creating and searching collections of embedding from Python code, and a\\nllm.get_embedding_model()interface for embedding strings directly. #191\\n2.17.38 0.8.1 (2023-08-31)\\n• Fixedbugwherefirstpromptwouldshowanerrorifthe io.datasette.llmdirectoryhadnotyetbeencreated.\\n#193\\n• Updateddocumentationtorecommendadifferent llm-gpt4allmodelsincetheonewewereusingisnolonger\\navailable. #195\\n2.17.39 0.8 (2023-08-20)\\n• The output format forllm logs has changed. Previously it was JSON - it’s now a much more readable Mark-\\ndown format suitable for pasting into other documents. #160\\n– The newllm logs --jsonoption can be used to get the old JSON format.\\n– Passllm logs --conversation IDor --cid IDto see the full logs for a specific conversation.\\n• You can now combine piped input and a prompt in a single command:cat script.py | llm 'explain\\nthis code'. This works even for models that do not supportsystem prompts. #153\\n• Additional OpenAI-compatible modelscan now be configured with custom HTTP headers. This enables plat-\\nforms such as openrouter.ai to be used with LLM, which can provide Claude access even without an Anthropic\\nAPI key.\\n• Keys set inkeys.jsonare now used in preference to environment variables. #158\\n• The documentation now includes aplugin directorylisting all available plugins for LLM. #173\\n• Newrelated toolssection in the documentation describingttok,strip-tagsand symbex. #111\\n• The llm models, llm aliases and llm templates commands now default to running the same command\\nasllm models listand llm aliases listand llm templates list. #167\\n• Newllm keys(aka llm keys list) command for listing the names of all configured keys. #174\\n• Two new Python API functions,llm.set_alias(alias, model_id)and llm.remove_alias(alias)can\\nbe used to configure aliases from within Python code. #154\\n• LLM is now compatible with both Pydantic 1 and Pydantic 2. This means you can installllm as a Python\\ndependency in a project that depends on Pydantic 1 without running into dependency conflicts. Thanks, Chris\\nMungall. #147\\n• llm.get_model(model_id)is now documented as raisingllm.UnknownModelErrorif the requested model\\ndoes not exist. #155\\n2.17. Changelog 167\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 173, 'page_label': '168'}, page_content=\"LLM documentation, Release 0.26-31-g0bf655a\\n2.17.40 0.7.1 (2023-08-19)\\n• Fixedabugwheresomeuserswouldseean AlterError: No such column: log.id errorwhenattempt-\\ning to use this tool, after upgrading to the latest sqlite-utils 3.35 release. #162\\n2.17.41 0.7 (2023-08-12)\\nThe newModel aliasescommands can be used to configure additional aliases for models, for example:\\nllm aliases set turbo gpt-3.5-turbo-16k\\nNow you can run the 16,000 tokengpt-3.5-turbo-16kmodel like this:\\nllm -m turbo 'An epic Greek-style saga about a cheesecake that builds a SQL database␣\\n˓→from scratch'\\nUse llm aliases listto see a list of aliases andllm aliases remove turboto remove one again. #151\\nNotable new plugins\\n• llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n• llm-llama-cpp uses llama.cpp to run models published in the GGML format. See Run Llama 2 on your own\\nMac using LLM and Homebrew for more details.\\nAlso in this release\\n• OpenAI models now have min and max validation on their floating point options. Thanks, Pavel Král. #115\\n• Fix for bug wherellm templates listraised an error if a template had an empty prompt. Thanks, Sherwin\\nDaganato. #132\\n• Fixed bug inllm install --editableoption which prevented installation of.[test]. #136\\n• llm install --no-cache-dirand --force-reinstalloptions. #146\\n2.17.42 0.6.1 (2023-07-24)\\n• LLM can now be installed directly from Homebrew core:brew install llm. #124\\n• Python API documentation now coversSystem prompts.\\n• Fixed incorrect example in theTemplatesdocumentation. Thanks, Jorge Cabello. #125\\n168 Chapter 2. Contents\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 174, 'page_label': '169'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.43 0.6 (2023-07-18)\\n• ModelshostedonReplicatecannowbeaccessedusingthellm-replicateplugin,includingthenewLlama2model\\nfrom Meta AI. More details here: Accessing Llama 2 from the command-line with the llm-replicate plugin.\\n• ModelprovidersthatexposeanAPIthatiscompatiblewiththeOpenAPIAPIformat,includingself-hostedmodel\\nserverssuchasLocalAI,cannowbeaccessedusing additionalconfiguration forthedefaultOpenAIplugin. #106\\n• OpenAI models that are not yet supported by LLM can also be configured using the new\\nextra-openai-models.yamlconfiguration file. #107\\n• Thellmlogscommand nowacceptsa -m model_idoptiontofilterlogstoaspecificmodel. Aliasescanbeused\\nhere in addition to model IDs. #108\\n• Logs now have a SQLite full-text search index against their prompts and responses, and thellm logs -q\\nSEARCHoption can be used to return logs that match a search term. #109\\n2.17.44 0.5 (2023-07-12)\\nLLMnowsupports additionallanguagemodels ,thankstoanew pluginsmechanism forinstallingadditionalmodels.\\nPlugins are available for 19 models in addition to the default OpenAI ones:\\n• llm-gpt4alladdssupportfor17modelsthatcandownloadandrunonyourowndevice,includingVicuna,Falcon\\nand wizardLM.\\n• llm-mpt30b adds support for the MPT-30B model, a 19GB download.\\n• llm-palm adds support for Google’s PaLM 2 via the Google API.\\nA comprehensive tutorial,writing a plugin to support a new modeldescribes how to add new models by building\\nplugins in detail.\\nNew features\\n• Python APIdocumentation for using LLM models, including models from plugins, directly from Python. #75\\n• Messages are now logged to the database by default - no need to run thellm init-db command any more,\\nwhich has been removed. Instead, you can toggle this behavior off usingllm logs off or turn it on again\\nusingllm logs on. Thellm logs statuscommandshowsthecurrentstatusofthelogdatabase. Iflogging\\nis turned off, passing--logto thellm promptcommand will cause that prompt to be logged anyway. #98\\n• Newdatabaseschemaforloggedmessages,with conversationsandresponsestables. Ifyouhavepreviously\\nused the oldlogstable it will continue to exist but will no longer be written to. #91\\n• New-o/--option name valuesyntax for setting options for models, such as temperature. Available options\\ndiffer for different models. #63\\n• llm models list --optionscommand for viewing all available model options. #82\\n• llm \"prompt\" --save templateoption for saving a prompt directly to a template. #55\\n• Prompt templates can now specifydefault valuesfor parameters. Thanks, Chris Mungall. #57\\n• llm openai modelscommand to list all available OpenAI models from their API. #70\\n• llm models default MODEL_IDtosetadifferentmodelasthedefaulttobeusedwhen llmisrunwithoutthe\\n-m/--modeloption. #31\\n2.17. Changelog 169'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 175, 'page_label': '170'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nSmaller improvements\\n• llm -sis now a shortcut forllm --system. #69\\n• llm -m 4-32kalias forgpt-4-32k.\\n• llm install -e directorycommand for installing a plugin from a local directory.\\n• The LLM_USER_PATH environment variable now controls the location of the directory in which LLM stores its\\ndata. This replaces the oldLLM_KEYS_PATHand LLM_LOG_PATHand LLM_TEMPLATES_PATHvariables. #76\\n• Documentation coveringUtility functions for plugins.\\n• Documentation site now uses Plausible for analytics. #79\\n2.17.45 0.4.1 (2023-06-17)\\n• LLM can now be installed using Homebrew:brew install simonw/llm/llm. #50\\n• llmis now styled LLM in the documentation. #45\\n• Examples in documentation now include a copy button. #43\\n• llm templatescommand no longer has its display disrupted by newlines. #42\\n• llm templatescommand now includes system prompt, if set. #44\\n2.17.46 0.4 (2023-06-17)\\nThis release includes some backwards-incompatible changes:\\n• The -4option for GPT-4 is now-m 4.\\n• The --codeoption has been removed.\\n• The -soption has been removed as streaming is now the default. Use--no-streamto opt out of streaming.\\nPrompt templates\\nTemplatesis a new feature that allows prompts to be saved as templates and re-used with different variables.\\nTemplates can be created using thellm templates editcommand:\\nllm templates edit summarize\\nTemplates are YAML - the following template defines summarization using a system prompt:\\nsystem: Summarize this text\\nThe template can then be executed like this:\\ncat myfile.txt | llm -t summarize\\nTemplatescanincludebothsystemprompts,regularpromptsandindicatethemodeltheyshoulduse. Theycanreference\\nvariables such as$input for content piped to the tool, or other variables that are passed using the new-p/--param\\noption.\\nThis example adds avoiceparameter:\\n170 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 176, 'page_label': '171'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nsystem: Summarize this text in the voice of $voice\\nThen to run it (via strip-tags to remove HTML tags from the input):\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nExample output:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\nThe Templatesdocumentation provides more detailed examples.\\nContinue previous chat\\nYoucannowuse llmtocontinueapreviousconversationwiththeOpenAIchatmodels( gpt-3.5-turboandgpt-4).\\nThis will include your previous prompts and responses in the prompt sent to the API, allowing the model to continue\\nwithin the same context.\\nUse the new-c/--continueoption to continue from the previous message thread:\\nllm \"Pretend to be a witty gerbil, say hi briefly\"\\nGreetings, dear human! Iam aclever gerbil, ready toentertain you withmy quick witand endless energy.\\nllm \"What do you think of snacks?\" -c\\nOh, how I adore snacks, dear human! Crunchy carrot sticks, sweet apple slices, and chewy yogurt drops\\nare some of my favorite treats. I could nibble on them all day long!\\nThe -coption will continue from the most recent logged message.\\nTo continue a different chat, pass an integer ID to the--chat option. This should be the ID of a previously logged\\nmessage. You can find these IDs using thellm logscommand.\\nThanks Amjith Ramanujam for contributing to this feature. #6\\nNew mechanism for storing API keys\\nAPIkeysforlanguagemodelssuchasthosebyOpenAIcannowbesavedusingthenew llm keysfamilyofcommands.\\nTo set the default key to be used for the OpenAI APIs, run this:\\nllm keys set openai\\nThen paste in your API key.\\nKeyscanalsobepassedusingthenew --keycommandlineoption-thiscanbeafullkeyorthealiasofakeythathas\\nbeen previously stored.\\nSee API key managementfor more. #13\\n2.17. Changelog 171'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 177, 'page_label': '172'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\nNew location for the logs.db database\\nThe logs.db database that stores a history of executed prompts no longer lives at~/.llm/log.db - it can now be\\nfound in a location that better fits the host operating system, which can be seen using:\\nllm logs path\\nOn macOS this is~/Library/Application Support/io.datasette.llm/logs.db.\\nTo open that database using Datasette, run this:\\ndatasette \"$(llm logs path)\"\\nYou can upgrade your existing installation by copying your database to the new location like this:\\ncp ~/.llm/log.db \"$(llm logs path)\"\\nrm -rf ~/.llm # To tidy up the now obsolete directory\\nThe database schema has changed, and will be updated automatically the first time you run the command.\\nThat schema is included in the documentation. #35\\nOther changes\\n• Newllm logs --truncateoption(shortcut -t)whichtruncatesthedisplayedpromptstomakethelogoutput\\neasier to read. #16\\n• Documentation now spans multiple pages and lives at https://llm.datasette.io/ #21\\n• Default llm chatgptcommand has been renamed tollm prompt. #17\\n• Removed--codeoption in favour of new prompt templates mechanism. #24\\n• Responses are now streamed by default, if the model supports streaming. The-s/--stream option has been\\nremoved. A new--no-streamoption can be used to opt-out of streaming. #25\\n• The -4/--gpt4 option has been removed in favour of-m 4 or -m gpt4, using a new mechanism that allows\\nmodels to have additional short names.\\n• The newgpt-3.5-turbo-16k model with a 16,000 token context length can now also be accessed using-m\\nchatgpt-16kor -m 3.5-16k. Thanks, Benjamin Kirkbride. #37\\n• Improved display of error messages from OpenAI. #15\\n2.17.47 0.3 (2023-05-17)\\n• llm logscommand for browsing logs of previously executed completions. #3\\n• llm \"Python code to output factorial 10\" --code option which sets a system prompt designed to\\nencourage code to be output without any additional explanatory text. #5\\n• Tool can now accept a prompt piped directly to standard input. #11\\n172 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 178, 'page_label': '173'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n2.17.48 0.2 (2023-04-01)\\n• If a SQLite database exists in~/.llm/log.db all prompts and responses are logged to that file. Thellm\\ninit-dbcommand can be used to create this file. #2\\n2.17.49 0.1 (2023-04-01)\\n• Initial prototype release. #1\\n2.17. Changelog 173'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 179, 'page_label': '174'}, page_content='LLM documentation, Release 0.26-31-g0bf655a\\n174 Chapter 2. Contents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-20T17:12:46+00:00', 'author': 'Simon Willison', 'title': 'LLM documentation', 'subject': '', 'keywords': '', 'moddate': '2025-06-20T17:12:46+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': 'llm.pdf', 'total_pages': 181, 'page': 180, 'page_label': '175'}, page_content='INDEX\\nT\\nTemplate(class in llm), 85\\nTool(class in llm), 102\\nToolCall(class in llm), 102\\nToolResult(class in llm), 102\\n175')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095fe659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfadb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "     -------------------------------------- 187.3/187.3 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.0)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fe03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = WebBaseLoader(web_path=\"https://www.geeksforgeeks.org/artificial-intelligence/large-language-model-llm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5abe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/artificial-intelligence/large-language-model-llm/', 'title': 'What is a Large Language Model (LLM) - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a Large Language Model (LLM) - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA to DevelopmentGATE 2026 PrepGet 3 IBM CertificationsFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)Data Analytics TrainingDevOps Engineering (LIVE)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation with DSAData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive ProgrammingFull Stack Development with React & Node JS (Live)(NEW) Digital Marketing ProgramFull Stack DevelopmentData Science & ML ProgramAll CoursesTutorialsPythonJavaData Structures & AlgorithmsML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOps And LinuxSchool LearningSoftware and ToolsPracticePractice Coding ProblemsGfG 160: Free DSA PracticeProblem of the DayETS TOEFL: Scholarship ContestJobsBecome a MentorApply Now!Post JobsJob-A-Thon: Hiring Challenge\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                                        You're all caught up!!\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPythonR LanguagePython for Data ScienceNumPyPandasOpenCVData AnalysisML MathMachine LearningNLPDeep LearningDeep Learning Interview QuestionsMachine LearningML ProjectsML Interview Questions \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExplore GfG CoursesShare Your ExperiencesWhat is LLMOps (Large Language Model Operations)?Multiturn Deviation in Large Language ModelExploring Multimodal Large Language ModelsLarge Concept Models (LCMs)What are LLM Parameters?Llama 3: Meta's New AI ModelNation SkillUp Explore \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a Large Language Model (LLM)\\n\\n\\n\\nLast Updated : \\n22 Jan, 2025\\n\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLarge Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing.This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP).What are Large Language Models(LLMs)?A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Language Model. Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.There are many techniques that were tried to perform natural language-related tasks but the LLM is purely based on the deep learning methodologies. LLM (Large language model) models are highly efficient in capturing the complex entity relationships in the text at hand and can generate the text using the semantic and syntactic of that particular language in which we wish to do so. If we talk about the size of the advancements in the GPT (Generative Pre-trained Transformer) model only then:GPT-1 which was released in 2018 contains 117 million parameters having 985 million words.GPT-2 which was released in 2019 contains 1.5 billion parameters.GPT-3 which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.GPT-4 model is released in the early 2023 and it is likely to contain trillions of parameters.GPT-4 Turbo was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.How do Large Language Models work?Large Language Models (LLMs) operate on the principles of deep learning, leveraging neural network architectures to process and understand human languages. These models, are trained on vast datasets using self-supervised learning techniques. The core of their functionality lies in the intricate patterns and relationships they learn from diverse language data during training. LLMs consist of multiple layers, including feedforward layers, embedding layers, and attention layers. They employ attention mechanisms, like self-attention, to weigh the importance of different tokens in a sequence, allowing the model to capture dependencies and relationships. Architecture of LLMLarge Language Model's (LLM) architecture is determined by a number of factors, like the objective of\\xa0the specific model design, the available computational resources, and the kind of language processing tasks that are to be carried out by the LLM. The general architecture of LLM consists of many layers such as the feed forward layers, embedding layers, attention layers. A text which is embedded inside is collaborated together to generate predictions.Important components to influence Large Language Model architecture:Model Size and Parameter Countinput representationsSelf-Attention MechanismsTraining ObjectivesComputational EfficiencyDecoding and Output GenerationTransformer-Based LLM Model ArchitecturesTransformer-based models, which have revolutionized natural language processing tasks, typically follow a general architecture that includes the following components:Input Embeddings: The input text is tokenized into smaller units, such as words or sub-words, and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.Positional Encoding: Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.Encoder: Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.Self-Attention Mechanism: Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.Feed-Forward Neural Network: After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.Decoder Layers: In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.Multi-Head Attention: Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.Layer Normalization: Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model's ability to generalize across different inputs.Output Layers: The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.It's important to keep in mind that the actual architecture of transformer-based models can change and be enhanced based on particular research and model creations. To fulfill different tasks and objectives, several models like GPT, BERT, and T5 may integrate more components or modifications.Popular Large Language Models Now let's look at some of the famous LLMs which has been developed and are up for inference.GPT-3: GPT 3 is developed by OpenAI, stands for Generative Pre-trained Transformer 3. This model powers ChatGPT and is widely recognized for its ability to generate human-like text across a variety of applications.BERT: It is created by Google, is commonly used for natural language processing tasks and generating text embeddings, which can also be utilized for training other models.RoBERTa: RoBERTa is an advanced version of BERT, stands for Robustly Optimized BERT Pretraining Approach. Developed by Facebook AI Research, it enhances the performance of the transformer architecture.BLOOM: It is the first multilingual LLM, designed collaboratively by multiple organizations and researchers. It follows an architecture similar to GPT-3, enabling diverse language-based tasks.For implementation details, these models are available on open-source platforms like Hugging Face and OpenAI for Python-based applications.Large Language Models Use CasesCode Generation: LLMs can generate accurate code based on user instructions for specific tasks.Debugging and Documentation: They assist in identifying code errors, suggesting fixes, and even automating project documentation.Question Answering: Users can ask both casual and complex questions, receiving detailed, context-aware responses.Language Translation and Correction: LLMs can translate text between over 50 languages and correct grammatical errors.Prompt-Based Versatility: By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios.Use cases of LLM are not limited to the above-mentioned one has to be just creative enough to write better prompts and you can make these models do a variety of tasks as they are trained to perform tasks on one-shot learning and zero-shot learning methodologies as well. Due to this only Prompt Engineering is a totally new and hot topic in academics for people who are looking forward to using ChatGPT-type models extensively.Applications of Large Language Models LLMs, such as GPT-3, have a wide range of applications across various domains. Few of them are:Natural Language Understanding (NLU): Large language models power advanced chatbots capable of engaging in natural conversations.They can be used to create intelligent virtual assistants for tasks like scheduling, reminders, and information retrieval.Content Generation: Creating human-like text for various purposes, including content creation, creative writing, and storytelling.Writing code snippets based on natural language descriptions or commands.Language Translation: Large language models can aid in translating text between different languages with improved accuracy and fluency.Text Summarization: Generating concise summaries of longer texts or articles.Sentiment Analysis: Analyzing and understanding sentiments expressed in social media posts, reviews, and comments.Difference Between NLP and LLM\\xa0NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-Automotive routine taskImprove search\\xa0Search engine optimizationAnalyzing and organizing large documentsSocial Media Analytics.while on the other hand, LLM is a Large Language Model, and is more specific to human- like text, providing content generation, and personalized recommendations.\\xa0What are the Advantages of Large Language Models?Large Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications:LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.LLMs efficiently handle vast amounts of data, making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.LLMs can be fine-tuned on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.LLMs enable the automation of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.Challenges in Training of Large Language ModelsHigh Costs: Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.Time-Intensive: Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.Data Challenges: Obtaining large text datasets is difficult, and concerns about the legality of data scraping for commercial purposes have arisen.Environmental Impact: Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns.ConclusionDue to the challenges faced in training LLM transfer learning is promoted heavily to get rid of all of the challenges discussed above. LLM has the capability to bring revolution in the AI-powered application but the advancements in this field seem a bit difficult because just increasing the size of the model may increase its performance but after a particular time a saturation in the performance will come and the challenges to handle these models will be bigger than the performance boost achieved by further increasing the size of the models. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    More infoAdvertise with us \\n\\nNext Article\\n\\n\\n\\n\\nWhat is LLMOps (Large Language Model Operations)?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA\\n\\n\\n\\n\\n \\n\\nabhishekm482g \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nArtificial Intelligence\\n\\n\\ndata-science\\n\\n\\nChatGPT\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2k+ interested Geeks \\n\\n\\n\\nGATE CSE 2028 Online Course [Live Classes] \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n137k+ interested Geeks \\n\\n\\n\\nCore Computer Science Subject for Interview Preparation \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2k+ interested Geeks \\n\\n\\n\\nGATE CS/IT 2027 Complete Course [with Placement Preparation] \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdvertise with us\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersIn MediaContact UsCorporate SolutionCampus Training ProgramExploreJob-A-ThonOffline Classroom ProgramDSA in JAVA/C++Master System DesignMaster CPVideosTutorialsPythonJavaC++PHPGoLangSQLR LanguageAndroidDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessAptitude PreparationPuzzlesCompany-Wise PreparationMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsCoursesIBM Certification CoursesDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudProgramming LanguagesC Programming with Data StructuresC++ Programming CourseJava Programming CoursePython Full CourseClouds/DevopsDevOps EngineeringAWS Solutions Architect CertificationSalesforce Certified Administrator CourseGATE 2026GATE CS Rank BoosterGATE DA Rank BoosterGATE CS & IT Course - 2026GATE DA Course 2026GATE Rank Predictor \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\r\\n        acknowledge that you have read and understood our\\r\\n        Cookie Policy &\\r\\n        Privacy Policy\\n\\n\\r\\n        Got It !\\r\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53304e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ac67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Collecting feedparser~=6.0.10\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "     -------------------------------------- 81.3/81.3 kB 652.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from arxiv) (2.32.4)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.7.14)\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "  Running setup.py install for sgmllib3k: started\n",
      "  Running setup.py install for sgmllib3k: finished with status 'done'\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd9bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "     ---------------------------------------- 18.7/18.7 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe18edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test4 = ArxivLoader(query = \"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4543f395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4078393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41078350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.14.0)\n",
      "Installing collected packages: wikipedia\n",
      "  Running setup.py install for wikipedia: started\n",
      "  Running setup.py install for wikipedia: finished with status 'done'\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: wikipedia is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db2ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test5 = WikipediaLoader(query= \"Gen AI\", load_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06b61c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nSince inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.\\nThe new generative models i'),\n",
       " Document(metadata={'title': 'OpenAI', 'summary': 'OpenAI, Inc. is an American artificial intelligence (AI) organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization has a complex corporate structure. As of April 2025, it is led by the non-profit OpenAI, Inc., registered in Delaware, and has multiple for-profit subsidiaries including OpenAI Holdings, LLC and OpenAI Global, LLC. Microsoft has invested US$13 billion in OpenAI, and is entitled to 49% of OpenAI Global, LLC\\'s profits, capped at an estimated 10x their investment. Microsoft also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.', 'source': 'https://en.wikipedia.org/wiki/OpenAI'}, page_content='OpenAI, Inc. is an American artificial intelligence (AI) organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization has a complex corporate structure. As of April 2025, it is led by the non-profit OpenAI, Inc., registered in Delaware, and has multiple for-profit subsidiaries including OpenAI Holdings, LLC and OpenAI Global, LLC. Microsoft has invested US$13 billion in OpenAI, and is entitled to 49% of OpenAI Global, LLC\\'s profits, capped at an estimated 10x their investment. Microsoft also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.\\n\\n\\n== History ==\\n\\n\\n=== 2015: founding and initial motivations ===\\n\\nIn December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, while YC Research never contributed any funds, Open Philanthropy contributed $30 million and another $15 million in verifiable donations were traced back to Musk. OpenAI later stated that Musk\\'s contributions totaled less than $45 million. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI was initially run from Brockman\\'s living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\\nAccording to OpenAI\\'s charter, its founding mission is \"to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.\"\\nMusk and Altman stated in 2015 that they were partly motivated by concerns about AI safety and existential risk from artificial general intelligence. OpenAI stated that \"it\\'s hard to fathom how much human-level AI could benefit society\", and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". The startup also wrote that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible\", and that \"because of AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach. When it does, it\\'ll be important to have a leading research institution which can prioritize a good outcome for all over its own self-interest.\" Co-chair Sam Altman expected a decades-long project that eventually surpasses human intelligence.\\nVishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce results general')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test5.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ec066f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40ef28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splittext = PyPDFLoader(\"llm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e9b09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = splittext.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad5061e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\\n\".join([doc.page_content for doc in fulltext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41062771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c6818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size =100, chunk_overlap = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea24f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "     -------------------------------------- 893.9/893.9 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "     -------------------------------------- 274.1/274.1 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d14b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = text_splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f84e41c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LLM documentation\\nRelease 0.26-31-g0bf655a\\nSimon Willison\\nJun 20, 2025',\n",
       " 'CONTENTS\\n1 Quick start 3\\n2 Contents 5\\n2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Upgrading to the latest version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.3 Using uvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.4 A note about Homebrew and PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.5 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.6 API key management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.1.7 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.2 Starting an interactive chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2.3 Listing available models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.2.4 Setting default options for models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3 OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.3.2 OpenAI language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.3.3 Model features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.3.4 OpenAI embedding models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3.5 OpenAI completion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.3.6 Adding more OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.4.1 Installing and using a local model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.4.2 OpenAI-compatible models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n2.5 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.1 How tools work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.2 Trying out tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.5.3 LLM’s implementation of tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.5.4 Default tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.5.5 Tips for implementing tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6 Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6.1 Schemas tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6.2 Using JSON schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.6.3 Ways to specify a schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n2.6.4 Concise LLM schema syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n2.6.5 Saving reusable schemas in templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.6.6 Browsing logged JSON objects created using schemas . . . . . . . . . . . . . . . . . . . . 48\\n2.7 Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\ni\\n2.7.1 Getting started with –save . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n2.7.2 Using a template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.3 Listing available templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.4 Templates as YAML files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.5 Template loaders from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.8 Fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.8.1 Using fragments in a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.8.2 Using fragments in chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.8.3 Browsing fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.8.4 Setting aliases for fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.8.5 Viewing fragments in your logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.8.6 Using fragments from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.8.7 Listing available fragment prefixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9 Model aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9.1 Listing aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9.2 Adding a new alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n2.9.3 Removing an alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.9.4 Viewing the aliases file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.10.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n2.10.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . . 74\\n2.10.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.11.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.11.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n2.11.4 Developing a model plugin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n2.11.5 Advanced model plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n2.11.6 Utility functions for plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n2.12 Python API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n2.12.1 Basic prompt execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n2.12.2 Async models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n2.12.3 Conversations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n2.12.4 Listing models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n2.12.5 Running code when a response has completed . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n2.12.6 Other functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n2.13 Logging to SQLite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n2.13.1 Viewing the logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n2.13.2 Browsing logs using Datasette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n2.13.3 Backing up your database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n2.13.4 SQL schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n2.14 Related tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.1 strip-tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.2 ttok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n2.14.3 Symbex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.15 CLI reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.15.1 llm –help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n2.16 Contributing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n2.16.1 Updating recorded HTTP API interactions and associated snapshots . . . . . . . . . . . . . 150\\n2.16.2 Debugging tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n2.16.3 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n2.16.4 Release process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17 Changelog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\nii\\n2.17.1 0.26 (2025-05-27) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17.2 0.26a1 (2025-05-25) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n2.17.3 0.26a0 (2025-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n2.17.4 0.25 (2025-05-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n2.17.5 0.25a0 (2025-04-10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.6 0.24.2 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.7 0.24.1 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.8 0.24 (2025-04-07) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n2.17.9 0.24a1 (2025-04-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.10 0.24a0 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.11 0.23 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\\n2.17.12 0.22 (2025-02-16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n2.17.13 0.21 (2025-01-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.14 0.20 (2025-01-22) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.15 0.19.1 (2024-12-05) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.16 0.19 (2024-12-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n2.17.17 0.19a2 (2024-11-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.18 0.19a1 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.19 0.19a0 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.20 0.18 (2024-11-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n2.17.21 0.18a1 (2024-11-14) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.22 0.18a0 (2024-11-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.23 0.17 (2024-10-29) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n2.17.24 0.17a0 (2024-10-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.25 0.16 (2024-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.26 0.15 (2024-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.27 0.14 (2024-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n2.17.28 0.13.1 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.29 0.13 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.30 0.12 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\\n2.17.31 0.11.2 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.32 0.11.1 (2023-10-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.33 0.11 (2023-09-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n2.17.34 0.10 (2023-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n2.17.35 0.10a1 (2023-09-11) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.36 0.10a0 (2023-09-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.37 0.9 (2023-09-03) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n2.17.38 0.8.1 (2023-08-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n2.17.39 0.8 (2023-08-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n2.17.40 0.7.1 (2023-08-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.41 0.7 (2023-08-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.42 0.6.1 (2023-07-24) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n2.17.43 0.6 (2023-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n2.17.44 0.5 (2023-07-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n2.17.45 0.4.1 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n2.17.46 0.4 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n2.17.47 0.3 (2023-05-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n2.17.48 0.2 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n2.17.49 0.1 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\nIndex 175\\niii\\niv\\nLLM documentation, Release 0.26-31-g0bf655a\\nA CLI tool and Python library for interacting withOpenAI, Anthropic’s Claude, Google’s Gemini, Meta’s Llama\\nand dozens of other Large Language Models, both via remote APIs and with models that can be installed and run on\\nyour own machine.\\nWatchLanguage models on the command-lineon YouTube for a demo or read the accompanying detailed notes.\\nWith LLM you can:\\n• Run prompts from the command-line\\n• Store prompts and responses in SQLite\\n• Generate and store embeddings\\n• Extract structured content from text and images\\n• Grant models the ability to execute tools\\n• ... and much, much more\\nCONTENTS 1\\nLLM documentation, Release 0.26-31-g0bf655a\\n2 CONTENTS\\nCHAPTER\\nONE\\nQUICK START\\nFirst, install LLM usingpipor Homebrew orpipxor uv:\\npip install llm\\nOr with Homebrew (seewarning note):\\nbrew install llm\\nOr with pipx:\\npipx install llm\\nOr with uv\\nuv tool install llm\\nIf you have an OpenAI API key key you can run this:\\n# Paste your OpenAI API key into this\\nllm keys set openai\\n# Run a prompt (with the default gpt-4o-mini model)\\nllm \"Ten fun names for a pet pelican\"\\n# Extract text from an image\\nllm \"extract text\" -a scanned-document.jpg\\n# Use a system prompt against a file\\ncat myfile.py | llm -s \"Explain this code\"\\nRun prompts against Gemini or Anthropic with their respective plugins:\\nllm install llm-gemini\\nllm keys set gemini\\n# Paste Gemini API key here\\nllm -m gemini-2.0-flash \\'Tell me fun facts about Mountain View\\'\\nllm install llm-anthropic\\nllm keys set anthropic\\n# Paste Anthropic API key here\\nllm -m claude-4-opus \\'Impress me with wild facts about turnips\\'\\n3\\nLLM documentation, Release 0.26-31-g0bf655a\\nYou can alsoinstall a pluginto access models that can run on your local device. If you use Ollama:\\n# Install the plugin\\nllm install llm-ollama\\n# Download and run a prompt against the Orca Mini 7B model\\nollama pull llama3.2:latest\\nllm -m llama3.2:latest \\'What is the capital of France?\\'\\nTo startan interactive chatwith a model, usellm chat:\\nllm chat -m gpt-4.1\\nChatting with gpt-4.1\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> Tell me a joke about a pelican\\nWhy don\\'t pelicans like to tip waiters?\\nBecause they always have a big bill!\\nMore background on this project:\\n• llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs\\n• The LLM CLI tool now supports self-hosted language models via plugins\\n• LLM now provides tools for working with embeddings\\n• Build an image search engine with llm-clip, chat with models with llm chat\\n• You can now run prompts against images, audio and video in your terminal using LLM\\n• Structured data extraction from unstructured content using LLM schemas\\n• Long context support in LLM 0.24 using fragments and template plugins\\nSee also the llm tag on my blog.\\n4 Chapter 1. Quick start\\nCHAPTER\\nTWO\\nCONTENTS\\n2.1 Setup\\n2.1.1 Installation\\nInstall this tool usingpip:\\npip install llm\\nOr using pipx:\\npipx install llm\\nOr using uv (more tips below):\\nuv tool install llm\\nOr using Homebrew (seewarning note):\\nbrew install llm\\n2.1.2 Upgrading to the latest version\\nIf you installed usingpip:\\npip install -U llm\\nFor pipx:\\npipx upgrade llm\\nFor uv:\\nuv tool upgrade llm\\nFor Homebrew:\\nbrew upgrade llm\\nIf the latest version is not yet available on Homebrew you can upgrade like this instead:\\n5\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm install -U llm\\n2.1.3 Using uvx\\nIf you have uv installed you can also use theuvxcommand to try LLM without first installing it like this:\\nexport OPENAI_API_KEY=\\'sx-...\\'\\nuvx llm \\'fun facts about skunks\\'\\nThis will install and run LLM using a temporary virtual environment.\\nYou can use the--withoption to add extra plugins. To use Anthropic’s models, for example:\\nexport ANTHROPIC_API_KEY=\\'...\\'\\nuvx --with llm-anthropic llm -m claude-3.5-haiku \\'fun facts about skunks\\'\\nAll of the usual LLM commands will work withuvx llm. Here’s how to set your OpenAI key without needing an\\nenvironment variable for example:\\nuvx llm keys set openai\\n# Paste key here\\n2.1.4 A note about Homebrew and PyTorch\\nTheversionofLLMpackagedforHomebrewcurrentlyusesPython3.12. ThePyTorchprojectdonotyethaveastable\\nrelease of PyTorch for that version of Python.\\nThis means that LLM plugins that depend on PyTorch such as llm-sentence-transformers may not install cleanly with\\nthe Homebrew version of LLM.\\nYou can workaround this by manually installing PyTorch before installingllm-sentence-transformers:\\nllm install llm-python\\nllm python -m pip install \\\\\\n--pre torch torchvision \\\\\\n--index-url https://download.pytorch.org/whl/nightly/cpu\\nllm install llm-sentence-transformers\\nThis should produce a working installation of that plugin.\\n2.1.5 Installing plugins\\nPlugins can be used to add support for other language models, including models that can run on your own device.\\nFor example, the llm-gpt4all plugin adds support for 17 new models that can be installed on your own machine. You\\ncan install that like so:\\nllm install llm-gpt4all\\n6 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.1.6 API key management\\nManyLLMmodelsrequireanAPIkey. TheseAPIkeyscanbeprovidedtothistoolusingseveraldifferentmechanisms.\\nYou can obtain an API key for OpenAI’s language models from the API keys page on their site.\\nSaving and using stored keys\\nThe easiest way to store an API key is to use thellm keys setcommand:\\nllm keys set openai\\nYou will be prompted to enter the key like this:\\n% llm keys set openai\\nEnter key:\\nOnce stored, this key will be automatically used for subsequent calls to the API:\\nllm \"Five ludicrous names for a pet lobster\"\\nYou can list the names of keys that have been set using this command:\\nllm keys\\nKeysthatarestoredinthiswayliveinafilecalled keys.json. Thisfileislocatedatthepathshownwhenyourunthe\\nfollowing command:\\nllm keys path\\nOnmacOSthiswillbe ~/Library/Application Support/io.datasette.llm/keys.json. OnLinuxitmaybe\\nsomething like~/.config/io.datasette.llm/keys.json.\\nPassing keys using the –key option\\nKeys can be passed directly using the--keyoption, like this:\\nllm \"Five names for pet weasels\" --key sk-my-key-goes-here\\nYoucanalsopass thealiasofakeystored inthe keys.jsonfile. Forexample, ifyouwanttomaintainapersonal API\\nkey you could add that like this:\\nllm keys set personal\\nAnd then use it for prompts like so:\\nllm \"Five friendly names for a pet skunk\" --key personal\\n2.1. Setup 7\\nLLM documentation, Release 0.26-31-g0bf655a\\nKeys in environment variables\\nKeys can also be set using an environment variable. These are different for different models.\\nFor OpenAI models the key will be read from theOPENAI_API_KEYenvironment variable.\\nThe environment variable will be used if no--keyoption is passed to the command and there is not a key configured\\nin keys.json\\nTo use an environment variable in place of thekeys.jsonkey run the prompt like this:\\nllm \\'my prompt\\' --key $OPENAI_API_KEY\\n2.1.7 Configuration\\nYou can configure LLM in a number of different ways.\\nSetting a custom default model\\nThe model used when callingllm without the-m/--model option defaults togpt-4o-mini - the fastest and least\\nexpensive OpenAI model.\\nYou can use thellm models default command to set a different default model. For GPT-4o (slower and more\\nexpensive, but more capable) run this:\\nllm models default gpt-4o\\nYou can view the current model by running this:\\nllm models default\\nAny of the supported aliases for a model can be passed to this command.\\nSetting a custom directory location\\nThis tool stores various files - prompt templates, stored keys, preferences, a database of logs - in a directory on your\\ncomputer.\\nOn macOS this is~/Library/Application Support/io.datasette.llm/.\\nOn Linux it may be something like~/.config/io.datasette.llm/.\\nYou can set a custom location for this directory by setting theLLM_USER_PATHenvironment variable:\\nexport LLM_USER_PATH=/path/to/my/custom/directory\\n8 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nTurning SQLite logging on and off\\nBy default, LLM will log every prompt and response you make to a SQLite database - seeLogging to SQLitefor more\\ndetails.\\nYou can turn this behavior off by default by running:\\nllm logs off\\nOr turn it back on again with:\\nllm logs on\\nRunllm logs statusto see the current states of the setting.\\n2.2 Usage\\nThe command to run a prompt isllm prompt \\'your prompt\\'. This is the default command, so you can usellm\\n\\'your prompt\\' as a shortcut.\\n2.2.1 Executing a prompt\\nThese examples use the default OpenAIgpt-4o-minimodel, which requires you to firstset an OpenAI API key.\\nYoucan installLLMplugins tousemodelsfromotherproviders,includingopenlylicensedmodelsyoucanrundirectly\\non your own computer.\\nTo run a prompt, streaming tokens as they come in:\\nllm \\'Ten names for cheesecakes\\'\\nTo disable streaming and only return the response once it has completed:\\nllm \\'Ten names for cheesecakes\\' --no-stream\\nTo switch from ChatGPT 4o-mini (the default) to GPT-4o:\\nllm \\'Ten names for cheesecakes\\' -m gpt-4o\\nYou can use-m 4oas an even shorter shortcut.\\nPass--model <model name>to use a different model. Runllm modelsto see a list of available models.\\nOrifyouknowthenameistoolongtotype, use -qonceormoretoprovidesearchterms-themodelwiththeshortest\\nmodel ID that matches all of those terms (as a lowercase substring) will be used:\\nllm \\'Ten names for cheesecakes\\' -q 4o -q mini\\nTo change the default model for the current session, set theLLM_MODELenvironment variable:\\nexport LLM_MODEL=gpt-4.1-mini\\nllm \\'Ten names for cheesecakes\\' # Uses gpt-4.1-mini\\nYou can send a prompt directly to standard input like this:\\n2.2. Usage 9\\nLLM documentation, Release 0.26-31-g0bf655a\\necho \\'Ten names for cheesecakes\\' | llm\\nIfyousendtexttostandardinputandprovidearguments,theresultingpromptwillconsistofthepipedcontentfollowed\\nby the arguments:\\ncat myscript.py | llm \\'explain this code\\'\\nWill run a prompt of:\\n<contents of myscript.py> explain this code\\nFor models that support them,system promptsare a better tool for this kind of prompting.\\nModel options\\nSomemodelssupportoptions. Youcanpasstheseusing -o/--option name value-forexample, tosetthetemper-\\nature to 1.5 run this:\\nllm \\'Ten names for cheesecakes\\' -o temperature 1.5\\nUse thellm models --optionscommand to see which options are supported by each model.\\nYou can alsoconfigure default optionsfor a model using thellm models optionscommands.\\nAttachments\\nSome models are multi-modal, which means they can accept input in more than just text. GPT-4o and GPT-4o mini\\ncan accept images, and models such as Google Gemini 1.5 can accept audio and video as well.\\nLLM calls theseattachments. You can pass attachments using the-aoption like this:\\nllm \"describe this image\" -a https://static.simonwillison.net/static/2024/pelicans.jpg\\nAttachments can be passed using URLs or file paths, and you can attach more than one attachment to a single prompt:\\nllm \"extract text\" -a image1.jpg -a image2.jpg\\nYou can also pipe an attachment to LLM by using-as the filename:\\ncat image.jpg | llm \"describe this image\" -a -\\nLLM will attempt to automatically detect the content type of the image. If this doesn’t work you can instead use the\\n--attachment-typeoption (--atfor short) which takes the URL/path plus an explicit content type:\\ncat myfile | llm \"describe this image\" --at - image/jpeg\\n10 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nSystem prompts\\nYou can use-s/--system \\'...\\' to set a system prompt.\\nllm \\'SQL to calculate total sales by month\\' \\\\\\n--system \\'You are an exaggerated sentient cheesecake that knows SQL and talks about␣\\n˓→cheesecake a lot\\'\\nThis is useful for piping content to standard input, for example:\\ncurl -s \\'https://simonwillison.net/2023/May/15/per-interpreter-gils/\\' | \\\\\\nllm -s \\'Suggest topics for this post as a JSON array\\'\\nOr to generate a description of changes made to a Git repository since the last commit:\\ngit diff | llm -s \\'Describe these changes\\'\\nDifferent models support system prompts in different ways.\\nTheOpenAImodelsareparticularlygoodatusingsystempromptsasinstructionsforhowtheyshouldprocessadditional\\ninput sent as part of the regular prompt.\\nOther models might use system prompts change the default voice and attitude of the model.\\nSystem prompts can be saved astemplates to create reusable tools. For example, you can create a template called\\npytestlike this:\\nllm -s \\'write pytest tests for this code\\' --save pytest\\nAnd then use the new template like this:\\ncat llm/utils.py | llm -t pytest\\nSee prompt templatesfor more.\\nTools\\nManymodelssupporttheabilitytocall externaltools. Toolscanbeprovided byplugins oryoucanpassa --functions\\nCODEoption to LLM to define one or more Python functions that the model can then call.\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\'\\nAdd --td/--tools-debug to see full details of the tools that are being executed. You can also set the\\nLLM_TOOLS_DEBUGenvironment variable to1to enable this for all prompts.\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\' --td\\nOutput:\\n2.2. Usage 11\\nLLM documentation, Release 0.26-31-g0bf655a\\nTool call: multiply({\\'x\\': 34234, \\'y\\': 213345})\\n7303652730\\n34234 multiplied by 213345 is 7,303,652,730.\\nOr add--ta/--tools-approveto approve each tool call interactively before it is executed:\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\' --ta\\nOutput:\\nTool call: multiply({\\'x\\': 34234, \\'y\\': 213345})\\nApprove tool call? [y/N]:\\nThe --functions option can be passed more than once, and can also point to the filename of a.py file containing\\none or more functions.\\nIf you have any tools that have been made available via plugins you can add them to the prompt using--tool/-T\\noption. For example, using llm-tools-simpleeval like this:\\nllm install llm-tools-simpleeval\\nllm --tool simple_eval \"4444 * 233423\" --td\\nRun this command to see a list of available tools from plugins:\\nllm tools\\nIf you run a prompt that uses tools from plugins (as opposed to tools provided using the--functions option) con-\\ntinuing that conversation usingllm -c will reuse the tools from the first prompt. Runningllm chat -c will start a\\nchat that continues using those same tools. For example:\\nllm -T simple_eval \"12345 * 12345\" --td\\nTool call: simple_eval({\\'expression\\': \\'12345 * 12345\\'})\\n152399025\\n12345 multiplied by 12345 equals 152,399,025.\\nllm -c \"that * 6\" --td\\nTool call: simple_eval({\\'expression\\': \\'152399025 * 6\\'})\\n914394150\\n152,399,025 multiplied by 6 equals 914,394,150.\\nllm chat -c --td\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\n> / 123\\nTool call: simple_eval({\\'expression\\': \\'914394150 / 123\\'})\\n7434098.780487805\\n914,394,150 divided by 123 is approximately 7,434,098.78.\\nSome tools are bundled in a configurable collection of tools called atoolbox. This means a single--tooloption can\\nload multiple related tools.\\nllm-tools-datasette is one example. Using a toolbox looks like this:\\n12 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm install llm-tools-datasette\\nllm -T \\'Datasette(\"https://datasette.io/content\")\\' \"Show tables\" --td\\nToolboxesalwaysstartwithacapitalletter. Theycanbeconfiguredbypassingatoolspecification,whichshouldfitthe\\nfollowing patterns:\\n• Empty: ToolboxNameor ToolboxName()- has no configuration arguments\\n• JSON object:ToolboxName({\"key\": \"value\", \"other\": 42})\\n• Single JSON value:ToolboxName(\"hello\")or ToolboxName([1,2,3])\\n• Key-value pairs:ToolboxName(name=\"test\", count=5, items=[1,2]) - treated the same as{\"name\":\\n\"test\", \"count\": 5, \"items\": [1, 2]} , all values must be valid JSON\\nToolboxes are not currently supported with thellm -coption, but they work well withllm chat. Try chatting with\\nthe Datasette content database like this:\\nllm chat -T \\'Datasette(\"https://datasette.io/content\")\\' --td\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\n...\\n> show tables\\nExtracting fenced code blocks\\nIf you are using an LLM to generate code it can be useful to retrieve just the code it produces without any of the\\nsurrounding explanatory text.\\nThe -x/--extractoption will scan the response for the first instance of a Markdown fenced code block - something\\nthat looks like this:\\n```python\\ndef my_function():\\n# ...\\n```\\nIt will extract and returns just the content of that block, excluding the fenced coded delimiters. If there are no fenced\\ncode blocks it will return the full response.\\nUse --xl/--extract-lastto return the last fenced code block instead of the first.\\nThe entire response including explanatory text is still logged to the database, and can be viewed usingllm logs -c.\\nSchemas\\nSome models include the ability to return JSON that matches a provided JSON schema. Models from OpenAI, An-\\nthropic and Google Gemini all include this capability.\\nTake a look at theschemas documentationfor a detailed guide to using this feature.\\nYou can pass JSON schemas directly to the--schemaoption:\\n2.2. Usage 13\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm --schema \\'{\\n\"type\": \"object\",\\n\"properties\": {\\n\"dogs\": {\\n\"type\": \"array\",\\n\"items\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"bio\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n}\\n}\\n}\\' -m gpt-4o-mini \\'invent two dogs\\'\\nOr use LLM’s customconcise schema syntaxlike this:\\nllm --schema \\'name,bio\\' \\'invent a dog\\'\\nTwo use the same concise schema for multiple items use--schema-multi:\\nllm --schema-multi \\'name,bio\\' \\'invent two dogs\\'\\nYou can also save the JSON schema to a file and reference the filename using--schema:\\nllm --schema dogs.schema.json \\'invent two dogs\\'\\nOr save your schemato a templatelike this:\\nllm --schema dogs.schema.json --save dogs\\n# Then to use it:\\nllm -t dogs \\'invent two dogs\\'\\nBe warned that different models may support different dialects of the JSON schema specification.\\nSee Browsing logged JSON objects created using schemasfor tips on using thellm logs --schema Xcommand to\\naccess JSON objects you have previously logged using this option.\\nFragments\\nYoucanusethe -f/--fragmentoptiontoreferencefragmentsofcontextthatyouwouldliketoloadintoyourprompt.\\nFragments can be specified as URLs, file paths or as aliases to previously saved fragments.\\nFragmentsaredesignedforrunninglongerprompts. LLM storespromptsinadatabase ,andthesamepromptrepeated\\nmanytimescanendupstoredasmultiplecopies,wastingdiskspace. Afragmentwillbestoredjustonceandreferenced\\nby all of the prompts that use it.\\nThe -f option can accept a path to a file on disk, a URL or the hash or alias of a previous fragment.\\nFor example, to ask a question about therobots.txtfile onllm.datasette.io:\\n14 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm -f https://llm.datasette.io/robots.txt \\'explain this\\'\\nFor a poem inspired by some Python code on disk:\\nllm -f cli.py \\'a short snappy poem inspired by this code\\'\\nYou can use as many-f options as you like - the fragments will be concatenated together in the order you provided,\\nwith any additional prompt added at the end.\\nFragments can also be used for the system prompt using the--sf/--system-fragment option. If you have a file\\ncalled explain_code.txtcontaining this:\\nExplain this code in detail. Include copies of the code quoted in the explanation.\\nYou can run it as the system prompt like this:\\nllm -f cli.py --sf explain_code.txt\\nYou can use thellm fragments setcommand to load a fragment and give it an alias for use in future queries:\\nllm fragments set cli cli.py\\n# Then\\nllm -f cli \\'explain this code\\'\\nUse llm fragmentsto list all fragments that have been stored:\\nllm fragments\\nYoucansearchbypassingoneormore -q Xsearchstrings. Thiswillreturnresultsmatchingallofthosestrings,across\\nthe source, hash, aliases and content:\\nllm fragments -q pytest -q asyncio\\nThe llm fragments remove command removes an alias. It does not delete the fragment record itself as those are\\nlinked to previous prompts and responses and cannot be deleted independently of them.\\nllm fragments remove cli\\nContinuing a conversation\\nBy default, the tool will start a new conversation each time you run it.\\nYou can opt to continue the previous conversation by passing the-c/--continueoption:\\nllm \\'More names\\' -c\\nThis will re-send the prompts and responses for the previous conversation as part of the call to the language model.\\nNote that this can add up quickly in terms of tokens, especially if you are using expensive models.\\n--continue will automatically use the same model as the conversation that you are continuing, even if you omit the\\n-m/--modeloption.\\nTo continue a conversation that is not the most recent one, use the--cid/--conversation <id>option:\\nllm \\'More names\\' --cid 01h53zma5txeby33t1kbe3xk8q\\nYou can find these conversation IDs using thellm logscommand.\\n2.2. Usage 15\\nLLM documentation, Release 0.26-31-g0bf655a\\nTips for using LLM with Bash or Zsh\\nTo learn more about your computer’s operating system based on the output ofuname -a, run this:\\nllm \"Tell me about my operating system: $(uname -a)\"\\nThis pattern of using$(command)inside a double quoted string is a useful way to quickly assemble prompts.\\nCompletion prompts\\nSome models are completion models - rather than being tuned to respond to chat style prompts, they are designed to\\ncomplete a sentence or paragraph.\\nAn example of this is thegpt-3.5-turbo-instructOpenAI model.\\nYou can prompt that model the same way as the chat models, but be aware that the prompt format that works best is\\nlikely to differ.\\nllm -m gpt-3.5-turbo-instruct \\'Reasons to tame a wild beaver:\\'\\n2.2.2 Starting an interactive chat\\nThe llm chatcommand starts an ongoing interactive chat with a model.\\nThis is particularly useful for models that run on your own machine, since it saves them from having to be loaded into\\nmemory each time a new prompt is added to a conversation.\\nRunllm chat, optionally with a-m model_id, to start a chat conversation:\\nllm chat -m chatgpt\\nEach chat starts a new conversation. A record of each conversation can be accessed throughthe logs.\\nYou can pass-c to start a conversation as a continuation of your most recent prompt. This will automatically use the\\nmost recently used model:\\nllm chat -c\\nFor models that support them, you can pass options using-o/--option:\\nllm chat -m gpt-4 -o temperature 0.5\\nYou can pass a system prompt to be used for your chat conversation:\\nllm chat -m gpt-4 -s \\'You are a sentient cheesecake\\'\\nYou can also passa template- useful for creating chat personas that you wish to return to.\\nHere’s how to create a template for your GPT-4 powered cheesecake:\\nllm --system \\'You are a sentient cheesecake\\' -m gpt-4 --save cheesecake\\nNow you can start a new chat with your cheesecake any time you like using this:\\nllm chat -t cheesecake\\n16 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> who are you?\\nI am a sentient cheesecake, meaning I am an artificial\\nintelligence embodied in a dessert form, specifically a\\ncheesecake. However, I don\\'t consume or prepare foods\\nlike humans do, I communicate, learn and help answer\\nyour queries.\\nTypequitor exitfollowed by<enter>to end a chat session.\\nSometimes you may want to paste multiple lines of text into a chat at once - for example when debugging an error\\nmessage.\\nTo do that, type!multito start a multi-line input. Type or paste your text, then type!endand hit<enter>to finish.\\nIfyourpastedtextmightitselfcontaina !endline,youcansetacustomdelimiterusing !multi abcfollowedby !end\\nabcat the end:\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !multi custom-end\\nExplain this error:\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1391, in https_open\\nreturn self.do_open(http.client.HTTPSConnection, req,\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1351, in do_open\\nraise URLError(err)\\nurllib.error.URLError: <urlopen error [Errno 8] nodename nor servname provided, or not␣\\n˓→known>\\n!end custom-end\\nYou can also use!editto open your default editor and modify the prompt before sending it to the model.\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !edit\\nllm chattakesthesame --tool/-Tand--functionsoptionsas llm prompt. Youcanusethistostartachatwith\\nthe specifiedtoolsenabled.\\n2.2. Usage 17\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.2.3 Listing available models\\nThe llm models command lists every model that can be used with LLM, along with their aliases. This includes\\nmodels that have been installed usingplugins.\\nllm models\\nExample output:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nGeminiPro: gemini-1.5-pro-002\\nGeminiPro: gemini-1.5-flash-002\\n...\\nAdd one or more-q termoptions to search for models matching all of those search terms:\\nllm models -q gpt-4o\\nllm models -q 4o -q mini\\nUse one or more-moptions to indicate specific models, either by their model ID or one of their aliases:\\nllm models -m gpt-4o -m gemini-1.5-pro-002\\nAdd--optionsto also see documentation for the options supported by each model:\\nllm models --options\\nOutput:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOptions:\\ntemperature: float\\nWhat sampling temperature to use, between 0 and 2. Higher values like\\n0.8 will make the output more random, while lower values like 0.2 will\\nmake it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,\\nwhere the model considers the results of the tokens with top_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but not both.\\nfrequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non their existing frequency in the text so far, decreasing the model\\'s\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non whether they appear in the text so far, increasing the model\\'s\\nlikelihood to talk about new topics.\\n(continues on next page)\\n18 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing in the completion.\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\njson_object: boolean\\nOutput a valid JSON object {...}. Prompt must mention JSON.\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n(continues on next page)\\n2.2. Usage 19\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\n(continues on next page)\\n20 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOptions:\\n(continues on next page)\\n2.2. Usage 21\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\n(continues on next page)\\n22 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\n(continues on next page)\\n2.2. Usage 23\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-1106-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-0125-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\n(continues on next page)\\n24 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.5-preview-2025-02-27\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\n(continues on next page)\\n2.2. Usage 25\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\n(continues on next page)\\n26 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nOpenAI Chat: o1-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\n(continues on next page)\\n2.2. Usage 27\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o3-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o3\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o4-mini\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\n(continues on next page)\\n28 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str\\nAttachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nOptions:\\ntemperature: float\\nWhat sampling temperature to use, between 0 and 2. Higher values like\\n0.8 will make the output more random, while lower values like 0.2 will\\nmake it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,\\nwhere the model considers the results of the tokens with top_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but not both.\\nfrequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non their existing frequency in the text so far, decreasing the model\\'s\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based\\non whether they appear in the text so far, increasing the model\\'s\\nlikelihood to talk about new topics.\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing in the completion.\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\nlogprobs: int\\nInclude the log probabilities of most likely N per token\\nFeatures:\\n- streaming\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\n2.2. Usage 29\\nLLM documentation, Release 0.26-31-g0bf655a\\nWhen running a prompt you can pass the full model name or any of the aliases to the-m/--modeloption:\\nllm -m 4o \\\\\\n\\'As many names for cheesecakes as you can think of, with detailed descriptions\\'\\n2.2.4 Setting default options for models\\nTo configure a default option for a specific model, use thellm models options setcommand:\\nllm models options set gpt-4o temperature 0.5\\nThis option will then be applied automatically any time you run a prompt through thegpt-4omodel.\\nDefault options are stored in themodel_options.jsonfile in the LLM configuration directory.\\nYou can list all default options across all models using thellm models options listcommand:\\nllm models options list\\nOr show them for an individual model withllm models options show <model_id>:\\nllm models options show gpt-4o\\nTo clear a default option, use thellm models options clearcommand:\\nllm models options clear gpt-4o temperature\\nOr clear all default options for a model like this:\\nllm models options clear gpt-4o\\nDefaultmodel optionsare respectedby boththe llm promptandthe llm chatcommands. Theywill notbe applied\\nwhen you use LLM as aPython library.\\n2.3 OpenAI models\\nLLM ships with a default plugin for talking to OpenAI’s API. OpenAI offer both language models and embedding\\nmodels, and LLM can access both types.\\n2.3.1 Configuration\\nAll OpenAI models are accessed using an API key. You can obtain one from the API keys page on their site.\\nOnce you have created a key, configure LLM to use it by running:\\nllm keys set openai\\nThen paste in the API key.\\n30 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.3.2 OpenAI language models\\nRunllm modelsfor a full list of available models. The OpenAI models supported by LLM are:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: gpt-4o-audio-preview\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOpenAI Chat: gpt-4.5-preview-2025-02-27\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOpenAI Chat: o1\\nOpenAI Chat: o1-2024-12-17\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nOpenAI Chat: o3-mini\\nOpenAI Chat: o3\\nOpenAI Chat: o4-mini\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nSee the OpenAI models documentation for details of each of these.\\ngpt-4o-mini(aliased to4o-mini) is the least expensive model, and is the default for if you don’t specify a model at\\nall. Consult OpenAI’s model documentation for details of the other models.\\no1-pro is not available through the Chat Completions API used by LLM’s default OpenAI plugin. You can install the\\nnew llm-openai-plugin plugin to access that model.\\n2.3.3 Model features\\nThe following features work with OpenAI models:\\n• System promptscan be used to provide instructions that have a higher weight than the prompt itself.\\n• Attachments. Many OpenAI models support image inputs - check which ones usingllm models --options.\\nAny model that accepts images can also accept PDFs.\\n• Schemascan be used to influence the JSON structure of the model output.\\n• Model optionscan be used to set parameters liketemperature. Usellm models --optionsfor a full list of\\nsupported options.\\n2.3. OpenAI models 31\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.3.4 OpenAI embedding models\\nRunllm embed-modelsfor a list ofembedding models. The following OpenAI embedding models are supported by\\nLLM:\\nada-002 (aliases: ada, oai)\\n3-small\\n3-large\\n3-small-512\\n3-large-256\\n3-large-1024\\nThe3-smallmodeliscurrentlythemostinexpensive. 3-largecostsmorebutismorecapable-seeNewembedding\\nmodels and API updates on the OpenAI blog for details and benchmarks.\\nAn important characteristic of any embedding model is the size of the vector it returns. Smaller vectors cost less to\\nstore and query, but may be less accurate.\\nOpenAI3-smalland3-largevectorscanbesafelytruncatedtolowerdimensionswithoutlosingtoomuchaccuracy.\\nThe -int models provided by LLM are pre-configured to do this, so3-large-256 is the3-large model truncated\\nto 256 dimensions.\\nThe vector size of the supported OpenAI embedding models are as follows:\\nModel Size\\nada-002 1536\\n3-small 1536\\n3-large 3072\\n3-small-512 512\\n3-large-256 256\\n3-large-1024 1024\\n2.3.5 OpenAI completion models\\nThegpt-3.5-turbo-instructmodelisalittledifferent-itisacompletionmodelratherthanachatmodel,described\\nin the OpenAI completions documentation.\\nCompletion models can be called with the-o logprobs 3 option (not supported by chat models) which will cause\\nLLM to store 3 log probabilities for each returned token in the SQLite database. Consult this issue for details on how\\nto read these values.\\n2.3.6 Adding more OpenAI models\\nOpenAI occasionally release new models with new names. LLM aims to ship new releases to support these, but you\\ncan also configure them directly, by adding them to aextra-openai-models.yamlconfiguration file.\\nRun this command to find the directory in which this file should be created:\\ndirname \"$(llm logs path)\"\\nOn my Mac laptop I get this:\\n~/Library/Application Support/io.datasette.llm\\n32 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nCreate a file in that directory calledextra-openai-models.yaml.\\nLet’s say OpenAI have just released thegpt-3.5-turbo-0613 model and you want to use it, despite LLM not yet\\nshipping support. You could configure that by adding this to the file:\\n- model_id: gpt-3.5-turbo-0613\\nmodel_name: gpt-3.5-turbo-0613\\naliases: [\"0613\"]\\nThemodel_idistheidentifierthatwillberecordedintheLLMlogs. Youcanusethistospecifythemodel,oryoucan\\noptionallyincludealistofaliasesforthatmodel. The model_nameistheactualmodelidentifierthatwillbepassedto\\nthe API, which must match exactly what the API expects.\\nIf the model is a completion model (such asgpt-3.5-turbo-instruct) addcompletion: true to the configu-\\nration.\\nIf the model supports structured extraction using json_schema, addsupports_schema: true to the configuration.\\nFor reasoning models likeo1or o3-miniadd reasoning: true .\\nWith this configuration in place, the following command should run a prompt against the new model:\\nllm -m 0613 \\'What is the capital of France?\\'\\nRunllm modelsto confirm that the new model is now available:\\nllm models\\nExample output:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-3.5-turbo-0613 (aliases: 0613)\\nRunningllm logs -n 1should confirm that the prompt and response has been correctly logged to the database.\\n2.4 Other models\\nLLM supports OpenAI models by default. You can installplugins to add support for other models. You can also add\\nadditional OpenAI-API-compatible modelsusing a configuration file.\\n2.4.1 Installing and using a local model\\nLLM pluginscan provide local models that run on your machine.\\nTo installllm-gpt4all, providing 17 models from the GPT4All project, run this:\\nllm install llm-gpt4all\\nRunllm modelsto see the expanded list of available models.\\nTo run a prompt through one of the models from GPT4All specify it using-m/--model:\\n2.4. Other models 33\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France?\\'\\nThe model will be downloaded and cached the first time you use it.\\nCheck theplugin directoryfor the latest list of available plugins for other models.\\n2.4.2 OpenAI-compatible models\\nProjectssuchasLocalAIofferaRESTAPIthatimitatestheOpenAIAPIbutcanbeusedtorunothermodels,including\\nmodels that can be installed on your own machine. These can be added using the same configuration mechanism.\\nThe model_id is the name LLM will use for the model. Themodel_name is the name which needs to be passed to\\nthe API - this might differ from themodel_id, especially if themodel_idcould potentially clash with other installed\\nmodels.\\nThe api_basekey can be used to point the OpenAI client library at a different API endpoint.\\nToaddthe orca-mini-3bmodelhostedbyalocalinstallationofLocalAI,addthistoyour extra-openai-models.\\nyamlfile:\\n- model_id: orca-openai-compat\\nmodel_name: orca-mini-3b.ggmlv3\\napi_base: \"http://localhost:8080\"\\nIf theapi_baseis set, the existing configuredopenaiAPI key will not be sent by default.\\nYou can setapi_key_nameto the name of a key stored using theAPI key managementfeature.\\nAdd completion: true if the model is a completion model that uses a /completion as opposed to a /\\ncompletion/chatendpoint.\\nIf a model does not support streaming, addcan_stream: false to disable the streaming option.\\nIf a model supports structured output via JSON schemas, you can addsupports_schema: true to support this\\nfeature.\\nIf a model is a vision model, you can addvision: true to support this feature and use image attachments.\\nIf a model is an audio model, you can addaudio: true to support this feature and use audio attachments.\\nHaving configured the model like this, runllm modelsto check that it installed correctly. You can then run prompts\\nagainst it like so:\\nllm -m orca-openai-compat \\'What is the capital of France?\\'\\nAnd confirm they were logged correctly with:\\nllm logs -n 1\\n34 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nExtra HTTP headers\\nSome providers such as openrouter.ai may require the setting of additional HTTP headers. You can set those using the\\nheaders: key like this:\\n- model_id: claude\\nmodel_name: anthropic/claude-2\\napi_base: \"https://openrouter.ai/api/v1\"\\napi_key_name: openrouter\\nheaders:\\nHTTP-Referer: \"https://llm.datasette.io/\"\\nX-Title: LLM\\n2.5 Tools\\nMany Large Language Models have been trained to execute tools as part of responding to a prompt. LLM supports\\ntool usage with both the command-line interface and the Python API.\\nExposing tools to LLMscarries risks! Be sure to read thewarning below.\\n2.5.1 How tools work\\nA tool is effectively a function that the model can request to be executed. Here’s how that works:\\n1. The initial prompt to the model includes a list of available tools, containing their names, descriptions and pa-\\nrameters.\\n2. The model can choose to call one (or sometimes more than one) of those tools, returning a request for the tool\\nto execute.\\n3. The code that calls the model - in this case LLM itself - then executes the specified tool with the provided\\narguments.\\n4. LLM prompts the model a second time, this time including the output of the tool execution.\\n5. The model can then use that output to generate its next response.\\nThissequencecanrunseveraltimesinaloop,allowingtheLLMtoaccessdata,actonthatdataandthenpassthatdata\\noff to other tools for further processing.\\nTools can be dangerous\\nWarning: Tools can be dangerous\\nApplications built on top of LLMs suffer from a class of attacks called prompt injection attacks. These occur when a\\nmaliciousthirdpartyinjectscontentintotheLLMwhichcausesittotaketool-basedactionsthatactagainsttheinterests\\nof the user of that application.\\nBe very careful about which tools you enable when you potentially might be exposed to untrusted sources of content -\\nwebpages,GitHubissuespostedbyotherpeople,emailandmessagesthathavebeensenttoyouthatcouldcomefrom\\nan attacker.\\nWatch out for thelethal trifectaof prompt injection exfiltration attacks. If your tool-enabled LLM has the following:\\n• access to private data\\n2.5. Tools 35\\nLLM documentation, Release 0.26-31-g0bf655a\\n• exposure to malicious instructions\\n• the ability to exfiltrate information\\nAnyone who can feed malicious instructions into your LLM - by leaving them on a web page it visits, or sending\\nan email to an inbox that it monitors - could be able to trick your LLM into using other tools to access your private\\ninformation and then exfiltrate (pass out) that data to somewhere the attacker can see it.\\n2.5.2 Trying out tools\\nLLM comes with a default tool installed, calledllm_version. You can try that out like this:\\nllm --tool llm_version \"What version of LLM is this?\" --td\\nYou can also use-T llm_versionas a shortcut for--tool llm_version.\\nThe output should look like this:\\nTool call: llm_version({})\\n0.26a0\\nThe installed version of the LLM is 0.26a0.\\nFurther tools can be installed using plugins, or you can use thellm --functions option to pass tools implemented\\nas PYthon functions directly, asdescribed here.\\n2.5.3 LLM’s implementation of tools\\nIn LLM every tool is a defined as a Python function. The function can take any number of arguments and can return a\\nstring or an object that can be converted to a string.\\nTool functions should include a docstring that describes what the function does. This docstring will become the de-\\nscription that is passed to the model.\\nToolscanalsobedefinedas toolboxclasses,asubclassof llm.Toolboxthatallowsmultiplerelatedtoolstobebundled\\ntogether. Toolbox classes can be be configured when they are instantiated, and can also maintain state in between\\nmultiple tool calls.\\nThe Python API can accept functions directly. The command-line interface has two ways for tools to be defined: via\\nplugins that implement theregister_tools() plugin hook, or directly on the command-line using the--functions\\nargument to specify a block of Python code defining one or more functions - or a path to a Python file containing the\\nsame.\\nYou can use toolswith the LLM command-line toolorwith the Python API.\\n36 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.5.4 Default tools\\nLLM includes some default tools for you to try out:\\n• llm_version()returns the current version of LLM\\n• llm_time()returns the current local and UTC time\\nTry them like this:\\nllm -T llm_version -T llm_time \\'Give me the current time and LLM version\\' --td\\n2.5.5 Tips for implementing tools\\nConsult theregister_tools() plugin hookdocumentation for examples of how to implement tools in plugins.\\nIfyourpluginneedsaccesstoAPIsecretsIrecommendstoringthoseusing llm keys set api-nameandthenreading\\nthem using thellm.get_key()utility function. This avoids secrets being logged to the database as part of tool calls.\\n2.6 Schemas\\nLarge Language Models are very good at producing structured output as JSON or other formats. LLM’sschemas\\nfeature allows you to define the exact structure of JSON data you want to receive from a model.\\nThis feature is supported by models from OpenAI, Anthropic, Google Gemini and can be implemented for othersvia\\nplugins.\\nThis page describes schemas used via thellmcommand-line tool. Schemas can also be used from thePython API.\\n2.6.1 Schemas tutorial\\nIn this tutorial we’re going to use schemas to analyze some news stories.\\nBut first, let’s invent some dogs!\\nGetting started with dogs\\nLLMs are great at creating test data. Let’s define a simple schema for a dog, using LLM’sconcise schema syntax.\\nWe’ll pass that to LLm withllm --schemaand prompt it to “invent a cool dog”:\\nllm --schema \\'name, age int, one_sentence_bio\\' \\'invent a cool dog\\'\\nI got back Ziggy:\\n{\\n\"name\": \"Ziggy\",\\n\"age\": 4,\\n\"one_sentence_bio\": \"Ziggy is a hyper-intelligent, bioluminescent dog who loves to␣\\n˓→perform tricks in the dark and guides his owner home using his glowing fur.\"\\n}\\n2.6. Schemas 37\\nLLM documentation, Release 0.26-31-g0bf655a\\nThe response matched my schema, withnameand one_sentence_biostring columns and an integer forage.\\nWe’re using the default LLM model here -gpt-4o-mini. Add-m modelto use another model - for example use-m\\no3-minito have O3 mini invent some dogs.\\nFor a list of available models that support schemas, run this command:\\nllm models --schemas\\nWant several more dogs? You can pass in that same schema using--schema-multiand ask for several at once:\\nllm --schema-multi \\'name, age int, one_sentence_bio\\' \\'invent 3 really cool dogs\\'\\nHere’s what I got:\\n{\\n\"items\": [\\n{\\n\"name\": \"Echo\",\\n\"age\": 3,\\n\"one_sentence_bio\": \"Echo is a sleek, silvery-blue Siberian Husky with mesmerizing␣\\n˓→blue eyes and a talent for mimicking sounds, making him a natural entertainer.\"\\n},\\n{\\n\"name\": \"Nova\",\\n\"age\": 2,\\n\"one_sentence_bio\": \"Nova is a vibrant, spotted Dalmatian with an adventurous␣\\n˓→spirit and a knack for agility courses, always ready to leap into action.\"\\n},\\n{\\n\"name\": \"Pixel\",\\n\"age\": 4,\\n\"one_sentence_bio\": \"Pixel is a playful, tech-savvy Poodle with a rainbow-colored␣\\n˓→coat, known for her ability to interact with smart devices and her love for puzzle␣\\n˓→toys.\"\\n}\\n]\\n}\\nSo that’s the basic idea: we can feed in a schema and LLM will pass it to the underlying model and (usually) get back\\nJSON that conforms to that schema.\\nThis stuff gets alot more useful when you start applying it to larger amounts of text, extracting structured details from\\nunstructured content.\\nExtracting people from a news articles\\nWearegoingtoextractdetailsofthepeoplewhoarementionedindifferentnewsstories,andthenusethosetocompile\\na database.\\nLet’s start by compiling a schema. For each person mentioned we want to extract the following details:\\n• Their name\\n• The organization they work for\\n• Their role\\n38 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n• What we learned about them from the story\\nWe will also record the article headline and the publication date, to make things easier for us later on.\\nUsing LLM’s custom, concise schema language, this time with newlines separating the individual fields (for the dogs\\nexample we used commas):\\nname: the person\\'s name\\norganization: who they represent\\nrole: their job title or role\\nlearned: what we learned about them from this story\\narticle_headline: the headline of the story\\narticle_date: the publication date in YYYY-MM-DD\\nAs you can see, this schema definition is pretty simple - each line has the name of a property we want to capture, then\\nan optional: followed by a description, which doubles as instructions for the model.\\nThe full syntax isdescribed below- you can also include type information for things like numbers.\\nLet’s run this against a news article.\\nVisit AP News and grab the URL to an article. I’m using this one:\\nhttps://apnews.com/article/trump-federal-employees-firings-\\n˓→a85d1aaf1088e050d39dcf7e3664bb9f\\nThere’s quite a lot of HTML on that page, possibly even enough to exceed GPT-4o mini’s 128,000 token input limit.\\nWe’ll use another tool called strip-tags to reduce that. If you have uv installed you can call it usinguvx strip-tags,\\notherwise you’ll need to install it first:\\nuv tool install strip-tags\\n# Or \"pip install\" or \"pipx install\"\\nNow we can run this command to extract the people from that article:\\ncurl \\'https://apnews.com/article/trump-federal-employees-firings-\\n˓→a85d1aaf1088e050d39dcf7e3664bb9f\\' | \\\\\\nuvx strip-tags | \\\\\\nllm --schema-multi \"\\nname: the person\\'s name\\norganization: who they represent\\nrole: their job title or role\\nlearned: what we learned about them from this story\\narticle_headline: the headline of the story\\narticle_date: the publication date in YYYY-MM-DD\\n\" --system \\'extract people mentioned in this article\\'\\nThe output I got started like this:\\n{\\n\"items\": [\\n{\\n\"name\": \"William Alsup\",\\n\"organization\": \"U.S. District Court\",\\n\"role\": \"Judge\",\\n\"learned\": \"He ruled that the mass firings of probationary employees were likely␣\\n˓→unlawful and criticized the authority exercised by the Office of Personnel Management.\\n(continues on next page)\\n2.6. Schemas 39\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n˓→\",\\n\"article_headline\": \"Judge finds mass firings of federal probationary workers were␣\\n˓→likely unlawful\",\\n\"article_date\": \"2025-02-26\"\\n},\\n{\\n\"name\": \"Everett Kelley\",\\n\"organization\": \"American Federation of Government Employees\",\\n\"role\": \"National President\",\\n\"learned\": \"He hailed the court\\'s decision as a victory for employees who were␣\\n˓→illegally fired.\",\\n\"article_headline\": \"Judge finds mass firings of federal probationary workers were␣\\n˓→likely unlawful\",\\n\"article_date\": \"2025-02-26\"\\n}\\nThis data has been logged to LLM’sSQLite database. We can retrieve the data back out again using thellm logs\\ncommand like this:\\nllm logs -c --data\\nThe -cflag means “use most recent conversation”, and the--dataflag outputs just the JSON data that was captured\\nin the response.\\nWe’re going to want to use the same schema for other things. Schemas that we use are automatically logged to the\\ndatabase - we can view them usingllm schemas:\\nllm schemas\\nHere’s the output:\\n- id: 3b7702e71da3dd791d9e17b76c88730e\\nsummary: |\\n{items: [{name, organization, role, learned, article_headline, article_date}]}\\nusage: |\\n1 time, most recently 2025-02-28T04:50:02.032081+00:00\\nTo view the full schema, run that command with--full:\\nllm schemas --full\\nWhich outputs:\\n- id: 3b7702e71da3dd791d9e17b76c88730e\\nschema: |\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"items\": {\\n\"type\": \"array\",\\n\"items\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n(continues on next page)\\n40 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"type\": \"string\",\\n\"description\": \"the person\\'s name\"\\n},\\n...\\nThat 3b7702e71da3dd791d9e17b76c88730eID can be used to run the same schema again. Let’s try that now on a\\ndifferent URL:\\ncurl \\'https://apnews.com/article/bezos-katy-perry-blue-origin-launch-\\n˓→4a074e534baa664abfa6538159c12987\\' | \\\\\\nuvx strip-tags | \\\\\\nllm --schema 3b7702e71da3dd791d9e17b76c88730e \\\\\\n--system \\'extract people mentioned in this article\\'\\nHere we are using--schemabecause our schema ID already corresponds to an array of items.\\nThe result starts like this:\\n{\\n\"items\": [\\n{\\n\"name\": \"Katy Perry\",\\n\"organization\": \"Blue Origin\",\\n\"role\": \"Singer\",\\n\"learned\": \"Katy Perry will join the all-female celebrity crew for a spaceflight␣\\n˓→organized by Blue Origin.\",\\n\"article_headline\": \"Katy Perry and Gayle King will join Jeff Bezos’ fiancee␣\\n˓→Lauren Sanchez on Blue Origin spaceflight\",\\n\"article_date\": \"2023-10-15\"\\n},\\nOne more trick: let’s turn our schema and system prompt combination into atemplate.\\nllm --schema 3b7702e71da3dd791d9e17b76c88730e \\\\\\n--system \\'extract people mentioned in this article\\' \\\\\\n--save people\\nThis creates a new template called “people”. We can confirm the template was created correctly using:\\nllm templates show people\\nWhich will output the YAML version of the template looking like this:\\nname: people\\nschema_object:\\nproperties:\\nitems:\\nitems:\\nproperties:\\narticle_date:\\ndescription: the publication date in YYYY-MM-DD\\ntype: string\\narticle_headline:\\ndescription: the headline of the story\\n(continues on next page)\\n2.6. Schemas 41\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntype: string\\nlearned:\\ndescription: what we learned about them from this story\\ntype: string\\nname:\\ndescription: the person\\'s name\\ntype: string\\norganization:\\ndescription: who they represent\\ntype: string\\nrole:\\ndescription: their job title or role\\ntype: string\\nrequired:\\n- name\\n- organization\\n- role\\n- learned\\n- article_headline\\n- article_date\\ntype: object\\ntype: array\\nrequired:\\n- items\\ntype: object\\nsystem: extract people mentioned in this article\\nWe can now run our people extractor against another fresh URL. Let’s use one from The Guardian:\\ncurl https://www.theguardian.com/commentisfree/2025/feb/27/billy-mcfarland-new-fyre-\\n˓→festival-fantasist | \\\\\\nstrip-tags | llm -t people\\nStoring the schema in a template means we can just usellm -t peopleto run the prompt. Here’s what I got back:\\n{\\n\"items\": [\\n{\\n\"name\": \"Billy McFarland\",\\n\"organization\": \"Fyre Festival\",\\n\"role\": \"Organiser\",\\n\"learned\": \"Billy McFarland is known for organizing the infamous Fyre Festival and␣\\n˓→was sentenced to six years in prison for wire fraud related to it. He is attempting to␣\\n˓→revive the festival with Fyre 2.\",\\n\"article_headline\": \"Welcome back Billy McFarland and a new Fyre festival. Shows␣\\n˓→you can’t keep a good fantasist down\",\\n\"article_date\": \"2025-02-27\"\\n}\\n]\\n}\\nDepending on the model, schema extraction may work against images and PDF files as well.\\nI took a screenshot of part of this story in the Onion and saved it to the following URL:\\n42 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nhttps://static.simonwillison.net/static/2025/onion-zuck.jpg\\nWe can pass that as anattachment using the-aoption. This time let’s use GPT-4o:\\nllm -t people -a https://static.simonwillison.net/static/2025/onion-zuck.jpg -m gpt-4o\\nWhich gave me back this:\\n{\\n\"items\": [\\n{\\n\"name\": \"Mark Zuckerberg\",\\n\"organization\": \"Facebook\",\\n\"role\": \"CEO\",\\n\"learned\": \"He addressed criticism by suggesting anyone with similar values and␣\\n˓→thirst for power could make the same mistakes.\",\\n\"article_headline\": \"Mark Zuckerberg Insists Anyone With Same Skewed Values And␣\\n˓→Unrelenting Thirst For Power Could Have Made Same Mistakes\",\\n\"article_date\": \"2018-06-14\"\\n}\\n]\\n}\\nNow that we’ve extracted people from a number of different sources, let’s load them into a database.\\nThellmlogs commandhasseveralfeaturesforworkingwithloggedJSONobjects. Sincewe’vebeenrecordingmultiple\\nobjects from each page in an\"items\" array using ourpeople template we can access those using the following\\ncommand:\\nllm logs --schema t:people --data-key items\\nIn place oft:people we could use the3b7702e71da3dd791d9e17b76c88730e schema ID or even the original\\nschema string instead, seespecifying a schema.\\nThis command outputs newline-delimited JSON for every item that has been captured using the specified schema:\\n{\"name\": \"Katy Perry\", \"organization\": \"Blue Origin\", \"role\": \"Singer\", \"learned\": \"She␣\\n˓→is one of the passengers on the upcoming spaceflight with Blue Origin.\"}\\n{\"name\": \"Gayle King\", \"organization\": \"Blue Origin\", \"role\": \"TV Journalist\", \"learned\\n˓→\": \"She is participating in the upcoming Blue Origin spaceflight.\"}\\n{\"name\": \"Lauren Sanchez\", \"organization\": \"Blue Origin\", \"role\": \"Helicopter Pilot and␣\\n˓→former TV Journalist\", \"learned\": \"She selected the crew for the Blue Origin␣\\n˓→spaceflight.\"}\\n{\"name\": \"Aisha Bowe\", \"organization\": \"Engineering firm\", \"role\": \"Former NASA Rocket␣\\n˓→Scientist\", \"learned\": \"She is part of the crew for the spaceflight.\"}\\n{\"name\": \"Amanda Nguyen\", \"organization\": \"Research Scientist\", \"role\": \"Activist and␣\\n˓→Scientist\", \"learned\": \"She is included in the crew for the upcoming Blue Origin␣\\n˓→flight.\"}\\n{\"name\": \"Kerianne Flynn\", \"organization\": \"Movie Producer\", \"role\": \"Producer\", \"learned\\n˓→\": \"She will also be a passenger on the upcoming spaceflight.\"}\\n{\"name\": \"Billy McFarland\", \"organization\": \"Fyre Festival\", \"role\": \"Organiser\",\\n˓→\"learned\": \"He was sentenced to six years in prison for wire fraud in 2018 and has␣\\n˓→launched a new festival called Fyre 2.\", \"article_headline\": \"Welcome back Billy␣\\n˓→McFarland and a new Fyre festival. Shows you can\\\\u2019t keep a good fantasist down\",\\n(continues on next page)\\n2.6. Schemas 43\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n˓→\"article_date\": \"2025-02-27\"}\\n{\"name\": \"Mark Zuckerberg\", \"organization\": \"Facebook\", \"role\": \"CEO\", \"learned\": \"He␣\\n˓→attempted to dismiss criticism by suggesting that anyone with similar values and␣\\n˓→thirst for power could have made the same mistakes.\", \"article_headline\": \"Mark␣\\n˓→Zuckerberg Insists Anyone With Same Skewed Values And Unrelenting Thirst For Power␣\\n˓→Could Have Made Same Mistakes\", \"article_date\": \"2018-06-14\"}\\nIf we add--data-arraywe’ll get back a valid JSON array of objects instead:\\nllm logs --schema t:people --data-key items --data-array\\nOutput starts:\\n[{\"name\": \"Katy Perry\", \"organization\": \"Blue Origin\", \"role\": \"Singer\", \"learned\": \"She␣\\n˓→is one of the passengers on the upcoming spaceflight with Blue Origin.\"},\\n{\"name\": \"Gayle King\", \"organization\": \"Blue Origin\", \"role\": \"TV Journalist\", \"learned\\n˓→\": \"She is participating in the upcoming Blue Origin spaceflight.\"},\\nWe can load this into a SQLite database using sqlite-utils, in particular the sqlite-utils insert command.\\nuv tool install sqlite-utils\\n# or pip install or pipx install\\nNow we can pipe the JSON into that tool to create a database with apeopletable:\\nllm logs --schema t:people --data-key items --data-array | \\\\\\nsqlite-utils insert data.db people -\\nTo see a table of the name, organization and role columns use sqlite-utils rows:\\nsqlite-utils rows data.db people -t -c name -c organization -c role\\nWhich produces:\\nname organization role\\n--------------- ------------------ -----------------------------------------\\nKaty Perry Blue Origin Singer\\nGayle King Blue Origin TV Journalist\\nLauren Sanchez Blue Origin Helicopter Pilot and former TV Journalist\\nAisha Bowe Engineering firm Former NASA Rocket Scientist\\nAmanda Nguyen Research Scientist Activist and Scientist\\nKerianne Flynn Movie Producer Producer\\nBilly McFarland Fyre Festival Organiser\\nMark Zuckerberg Facebook CEO\\nWe can also explore the database in a web interface using Datasette:\\nuvx datasette data.db\\n# Or install datasette first:\\nuv tool install datasette # or pip install or pipx install\\ndatasette data.db\\nVisit http://127.0.0.1:8001/data/peopleto start navigating the data.\\n44 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.6.2 Using JSON schemas\\nThe above examples have both usedconcise schema syntax. LLM converts this format to JSON schema, and you can\\nuse JSON schema directly yourself if you wish.\\nJSON schema covers the following:\\n• The data types of fields (string, number, array, object, etc.)\\n• Required vs. optional fields\\n• Nested data structures\\n• Constraints on values (minimum/maximum, patterns, etc.)\\n• Descriptions of those fields - these can be used to guide the language model\\nDifferentmodelsmaysupportdifferentsubsetsoftheoverallJSONschemalanguage. Youshouldexperimenttofigure\\nout what works for the model you are using.\\nLLMrecommendsthatthetopleveloftheschemaisanobject,notanarray,forincreasedcompatibilityacrossmultiple\\nmodels. I suggest using{\"items\": [array of objects]} if you want to return an array.\\nThe dogs schema above,name, age int, one_sentence_bio, would look like this as a full JSON schema:\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n},\\n\"one_sentence_bio\": {\\n\"type\": \"string\"\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\",\\n\"one_sentence_bio\"\\n]\\n}\\nThis JSON can be passed directly to the--schemaoption, or saved in a file and passed as the filename.\\nllm --schema \\'{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n},\\n\"one_sentence_bio\": {\\n\"type\": \"string\"\\n(continues on next page)\\n2.6. Schemas 45\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\",\\n\"one_sentence_bio\"\\n]\\n}\\' \\'a surprising dog\\'\\nExample output:\\n{\\n\"name\": \"Baxter\",\\n\"age\": 3,\\n\"one_sentence_bio\": \"Baxter is a rescue dog who learned to skateboard and now performs␣\\n˓→tricks at local parks, astonishing everyone with his skill!\"\\n}\\n2.6.3 Ways to specify a schema\\nLLMacceptsschemadefinitionsforbothrunningpromptsandexploringloggedresponses,usingthe --schemaoption.\\nThis option can take multiple forms:\\n• A string providing a JSON schema:--schema \\'{\"type\": \"object\", ...} \\'\\n• Acondensed schema definition: --schema \\'name,age int\\'\\n• The name or path of a file on disk containing a JSON schema:--schema dogs.schema.json\\n• The hexadecimal ID of a previously logged schema:--schema 520f7aabb121afd14d0c6c237b39ba2d -\\nthese IDs can be found using thellm schemascommand.\\n• A schema that has beensaved in a template: --schema t:name-of-template, seeSaving reusable schemas\\nin templates.\\n2.6.4 Concise LLM schema syntax\\nJSON schema’s can be time-consuming to construct by hand. LLM also supports a concise alternative syntax for\\nspecifying a schema.\\nA simple schema for an object with two string properties callednameand biolooks like this:\\nname, bio\\nYou can include type information by adding a type indicator after the property name, separated by a space.\\nname, bio, age int\\nSupported types areint for integers,float for floating point numbers,str for strings (the default) andbool for\\ntrue/false booleans.\\nTo include a description of the field to act as a hint to the model, add one after a colon:\\n46 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nname: the person\\'s name, age int: their age, bio: a short bio\\nIf your schema is getting long you can switch from comma-separated to newline-separated, which also allows you to\\nuse commas in those descriptions:\\nname: the person\\'s name\\nage int: their age\\nbio: a short bio, no more than three sentences\\nYou can experiment with the syntax using thellm schemas dsl command, which converts the input into a JSON\\nschema:\\nllm schemas dsl \\'name, age int\\'\\nOutput:\\n{\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"\\n},\\n\"age\": {\\n\"type\": \"integer\"\\n}\\n},\\n\"required\": [\\n\"name\",\\n\"age\"\\n]\\n}\\nThe Python utility functionllm.schema_dsl(schema) can be used to convert this syntax into the equivalent JSON\\nschema dictionary when working with schemasin the Python API.\\n2.6.5 Saving reusable schemas in templates\\nIf you want to store a schema with a name so you can reuse it easily in the future, the easiest way to do so is to save it\\nin a template.\\nThe quickest way to do that is with thellm --saveoption:\\nllm --schema \\'name, age int, one_sentence_bio\\' --save dog\\nNow you can use it like this:\\nllm --schema t:dog \\'invent a dog\\'\\nOr:\\nllm --schema-multi t:dog \\'invent three dogs\\'\\n2.6. Schemas 47\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.6.6 Browsing logged JSON objects created using schemas\\nBy default, all JSON produced using schemas is logged toa SQLite database. You can use special options to thellm\\nlogscommand to extract just those JSON objects in a useful format.\\nThe llm logs --schema X filter option can be used to filter just for responses that were created using the specified\\nschema. You can pass the full schema JSON, a path to the schema on disk or the schema ID.\\nThe --dataoption causes just the JSON data collected by that schema to be outputted, as newline-delimited JSON.\\nIf you instead want a JSON array of objects (with starting and ending square braces) you can use--data-array\\ninstead.\\nLet’s invent some dogs:\\nllm --schema-multi \\'name, ten_word_bio\\' \\'invent 3 cool dogs\\'\\nllm --schema-multi \\'name, ten_word_bio\\' \\'invent 2 cool dogs\\'\\nHaving logged these cool dogs, you can see just the data that was returned by those prompts like this:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data\\nWeneedtouse --schema-multiherebecauseweusedthatwhenwefirstcreatedtheserecords. The --schemaoption\\nis also supported, and can be passed a filename or JSON schema or schema ID as well.\\nOutput:\\n{\"items\": [{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"}, {\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a␣\\n˓→talent for agility and tricks.\"}]}\\n{\"items\": [{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee␣\\n˓→and outdoor adventures.\"}, {\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with␣\\n˓→mesmerizing blue eyes, enjoys snow and play.\"}, {\"name\": \"Ziggy\", \"ten_word_bio\":\\n˓→\"Quirky pug who loves belly rubs and quirky outfits.\"}]}\\nNote that the dogs are nested in that\"items\"key. To access the list of items from that key use--data-key items:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items\\nOutput:\\n{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee and␣\\n˓→outdoor adventures.\"}\\n{\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with mesmerizing blue eyes, enjoys snow␣\\n˓→and play.\"}\\n{\"name\": \"Ziggy\", \"ten_word_bio\": \"Quirky pug who loves belly rubs and quirky outfits.\"}\\n{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"}\\n{\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a talent for agility and␣\\n˓→tricks.\"}\\nFinally, to output a JSON array instead of newline-delimited JSON use--data-array:\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items --data-array\\nOutput:\\n48 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n[{\"name\": \"Bolt\", \"ten_word_bio\": \"Lightning-fast border collie, loves frisbee and␣\\n˓→outdoor adventures.\"},\\n{\"name\": \"Luna\", \"ten_word_bio\": \"Mystical husky with mesmerizing blue eyes, enjoys␣\\n˓→snow and play.\"},\\n{\"name\": \"Ziggy\", \"ten_word_bio\": \"Quirky pug who loves belly rubs and quirky outfits.\"}\\n˓→,\\n{\"name\": \"Robo\", \"ten_word_bio\": \"A cybernetic dog with laser eyes and super␣\\n˓→intelligence.\"},\\n{\"name\": \"Flamepaw\", \"ten_word_bio\": \"Fire-resistant dog with a talent for agility and␣\\n˓→tricks.\"}]\\nAdd--data-idstoinclude \"response_id\"and \"conversation_id\"fieldsineachofthereturnedobjectsreflect-\\ningthedatabaseIDsoftheresponseandconversationtheywereapartof. Thiscanbeusefulfortrackingthesourceof\\neach individual row.\\nllm logs --schema-multi \\'name, ten_word_bio\\' --data-key items --data-ids\\nOutput:\\n{\"name\": \"Nebula\", \"ten_word_bio\": \"A cosmic puppy with starry fur, loves adventures in␣\\n˓→space.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Echo\", \"ten_word_bio\": \"A clever hound with extraordinary hearing, master of␣\\n˓→hide-and-seek.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Biscuit\", \"ten_word_bio\": \"An adorable chef dog, bakes treats that everyone␣\\n˓→loves.\", \"response_id\": \"01jn4dawj8sq0c6t3emf4k5ryx\", \"conversation_id\":\\n˓→\"01jn4dawj8sq0c6t3emf4k5ryx\"}\\n{\"name\": \"Cosmo\", \"ten_word_bio\": \"Galactic explorer, loves adventures and chasing␣\\n˓→shooting stars.\", \"response_id\": \"01jn4daycb3svj0x7kvp7zrp4q\", \"conversation_id\":\\n˓→\"01jn4daycb3svj0x7kvp7zrp4q\"}\\n{\"name\": \"Pixel\", \"ten_word_bio\": \"Tech-savvy pup, builds gadgets and loves virtual␣\\n˓→playtime.\", \"response_id\": \"01jn4daycb3svj0x7kvp7zrp4q\", \"conversation_id\":\\n˓→\"01jn4daycb3svj0x7kvp7zrp4q\"}\\nIf a row already has a property called\"conversation_id\" or \"response_id\" additional underscores will be ap-\\npended to the ID key until it no longer overlaps with the existing keys.\\nThe--id-gt $IDand --id-gte $IDoptionscan beusefulforignoring loggedschemadataprior toacertain point,\\nsee Filtering past a specific IDfor details.\\n2.7 Templates\\nAtemplatecancombineaprompt,systemprompt,model,defaultmodeloptions,schema,andfragmentsintoasingle\\nreusable unit.\\nOnly one template can be used at a time. To compose multiple shorter pieces of prompts together consider using\\nfragmentsinstead.\\n2.7. Templates 49\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.7.1 Getting started with –save\\nThe easiest way to create a template is using the--save template_nameoption.\\nHere’s how to create a template for summarizing text:\\nllm \\'$input - summarize this\\' --save summarize\\nPut $input where you would like the user’s input to be inserted. If you omit this their input will be added to the end\\nof your regular prompt:\\nllm \\'Summarize the following: \\' --save summarize\\nYou can also create templates using system prompts:\\nllm --system \\'Summarize this\\' --save summarize\\nYou can set the default model for a template using--model:\\nllm --system \\'Summarize this\\' --model gpt-4o --save summarize\\nYou can also save default options:\\nllm --system \\'Speak in French\\' -o temperature 1.8 --save wild-french\\nIf you want to include a literal$sign in your prompt, use$$instead:\\nllm --system \\'Estimate the cost in $$ of this: $input\\' --save estimate\\nUse --tool/-Tone or more times to add tools to the template:\\nllm -T llm_time --system \\'Always include the current time in the answer\\' --save time\\nYou can also use--functionsto add Python function code directly to the template:\\nllm --functions \\'def reverse_string(s): return s[::-1]\\' --system \\'reverse any input\\' --\\n˓→save reverse\\nllm -t reverse \\'Hello, world!\\'\\nAdd--schemato bake aschemainto your template:\\nllm --schema dog.schema.json \\'invent a dog\\' --save dog\\nIf you add--extractthe setting toextract the first fenced code blockwill be persisted in the template.\\nllm --system \\'write a Python function\\' --extract --save python-function\\nllm -t python-function \\'calculate haversine distance between two points\\'\\nIn each of these cases the template will be saved in YAML format in a dedicated directory on disk.\\n50 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.7.2 Using a template\\nYou can execute a named template using the-t/--templateoption:\\ncurl -s https://example.com/ | llm -t summarize\\nThis can be combined with the-moption to specify a different model:\\ncurl -s https://llm.datasette.io/en/latest/ | \\\\\\nllm -t summarize -m gpt-3.5-turbo-16k\\nTemplates can also be specified as a direct path to a YAML file on disk:\\nllm -t path/to/template.yaml \\'extra prompt here\\'\\nOr as a URL to a YAML file hosted online:\\nllm -t https://raw.githubusercontent.com/simonw/llm-templates/refs/heads/main/python-app.\\n˓→yaml \\\\\\n\\'Python app to pick a random line from a file\\'\\nNotethattemplatesloadedviaURLswillhaveany functions: keysignored,toavoidaccidentallyexecutingarbitrary\\ncode. This restriction also applies to templates loaded via thetemplate loaders plugin mechanism.\\n2.7.3 Listing available templates\\nThis command lists all available templates:\\nllm templates\\nThe output looks something like this:\\ncmd : system: reply with macos terminal commands only, no extra information\\nglados : system: You are GlaDOS prompt: Summarize this:\\n2.7.4 Templates as YAML files\\nTemplates are stored as YAML files on disk.\\nYou can edit (or create) a YAML file for a template using thellm templates editcommand:\\nllm templates edit summarize\\nThis will open the system default editor.\\nTip: You can control which editor will be used here using theEDITORenvironment variable - for example, to use VS\\nCode:\\nexport EDITOR=\"code -w\"\\nAdd that to your~/.zshrc or ~/.bashrc file depending on which shell you use (zsh is the default on macOS since\\nmacOS Catalina in 2019).\\n2.7. Templates 51\\nLLM documentation, Release 0.26-31-g0bf655a\\nYou can create or edit template files directly in the templates directory. The location of this directory is shown by the\\nllm templates pathcommand:\\nllm templates path\\nExample output:\\n/Users/simon/Library/Application Support/io.datasette.llm/templates\\nA basic YAML template looks like this:\\nprompt: \\'Summarize this: $input\\'\\nOr use YAML multi-line strings for longer inputs. I created this usingllm templates edit steampunk:\\nprompt: >\\nSummarize the following text.\\nInsert frequent satirical steampunk-themed illustrative anecdotes.\\nReally go wild with that.\\nText to summarize: $input\\nTheprompt: > causes the followingindented text to be treatedas a single string, with newlinescollapsed to spaces.\\nUse prompt: | to preserve newlines.\\nRunningthatwith llm -t steampunkagainstGPT-4o(viastrip-tagstoremoveHTMLtagsfromtheinputandminify\\nwhitespace):\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t steampunk -m gpt-4o\\nOutput:\\nIn a fantastical steampunk world, Simon Willison decided to merge an old MP3 recording with slides\\nfrom the talk using iMovie. After exporting the slides as images and importing them into iMovie, he had\\nto disable the default Ken Burns effect using the “Crop” tool. Then, Simon manually synchronized the\\naudiobyadjustingthedurationofeachimage. Finally,hepublishedthemasterpiecetoYouTube,withthe\\nwhimsical magic of steampunk-infused illustrations leaving his viewers in awe.\\nSystem prompts\\nWhen working with models that support system prompts you can set a system prompt using asystem: key like so:\\nsystem: Summarize this\\nIf you specify only a system prompt you don’t need to use the$input variable -llm will use the user’s input as the\\nwhole of the regular prompt, which will then be processed using the instructions set in that system prompt.\\nYou can combine system and regular prompts like so:\\nsystem: You speak like an excitable Victorian adventurer\\nprompt: \\'Summarize this: $input\\'\\n52 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nFragments\\nTemplates can referenceFragmentsusing thefragments: and system_fragments: keys. These should be a list of\\nfragment URLs, filepaths or hashes:\\nfragments:\\n- https://example.com/robots.txt\\n- /path/to/file.txt\\n- 993fd38d898d2b59fd2d16c811da5bdac658faa34f0f4d411edde7c17ebb0680\\nsystem_fragments:\\n- https://example.com/systm-prompt.txt\\nOptions\\nDefault options can be set using theoptions: key:\\nname: wild-french\\nsystem: Speak in French\\noptions:\\ntemperature: 1.8\\nTools\\nThe tools: key can provide a list of tool names from other plugins - either function names or toolbox specifiers:\\nname: time-plus\\ntools:\\n- llm_time\\n- Datasette(\"https://example.com/timezone-lookup\")\\nThe functions: key can provide a multi-line string of Python code defining additional functions:\\nname: my-functions\\nfunctions: |\\ndef reverse_string(s: str):\\nreturn s[::-1]\\ndef greet(name: str):\\nreturn f\"Hello, {name}!\"\\nSchemas\\nUsethe schema_object: keytoembedaJSONschema(asYAML)inyourtemplate. Theeasiestwaytocreatethese\\niswiththe llm --schema ... --save name-of-templatecommand-theresultshouldlooksomethinglikethis:\\nname: dogs\\nschema_object:\\nproperties:\\ndogs:\\nitems:\\nproperties:\\n(continues on next page)\\n2.7. Templates 53\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbio:\\ntype: string\\nname:\\ntype: string\\ntype: object\\ntype: array\\ntype: object\\nAdditional template variables\\nTemplates that work against the user’s normal prompt input (content that is either piped to the tool via standard input\\nor passed as a command-line argument) can use the$inputvariable.\\nYou can use additional named variables. These will then need to be provided using the-p/--param option when\\nexecuting the template.\\nHere’s an example YAML template calledrecipe, which you can create usingllm templates edit recipe:\\nprompt: |\\nSuggest a recipe using ingredients: $ingredients\\nIt should be based on cuisine from this country: $country\\nThis can be executed like so:\\nllm -t recipe -p ingredients \\'sausages, milk\\' -p country Germany\\nMy output started like this:\\nRecipe: German Sausage and Potato Soup\\nIngredients:\\n• 4 German sausages\\n• 2 cups whole milk\\nThis example combines input piped to the tool with additional parameters. Call thissummarize:\\nsystem: Summarize this text in the voice of $voice\\nThen to run it:\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nI got this:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\n54 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nSpecifying default parameters\\nWhen creating a template using the--save option you can pass-p name value to store the default values for pa-\\nrameters:\\nllm --system \\'Summarize this text in the voice of $voice\\' \\\\\\n--model gpt-4o -p voice GlaDOS --save summarize\\nYou can specify default values for parameters in the YAML using thedefaults: key.\\nsystem: Summarize this text in the voice of $voice\\ndefaults:\\nvoice: GlaDOS\\nWhen running without-pit will choose the default:\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t summarize\\nBut you can override the defaults with-p:\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t summarize -p voice Yoda\\nI got this:\\nText,summarizeinYoda’svoice,Iwill: “Hmm,youngpadawan. Summaryofthistext,youseek. Hmmm.\\n...\\nConfiguring code extraction\\nTo configure theextract first fenced code blocksetting for the template, add this:\\nextract: true\\nSetting a default model for a template\\nTemplates executed usingllm -t template-namewill execute using the default model that the user has configured\\nfor the tool - orgpt-3.5-turboif they have not configured their own default.\\nYou can specify a new default model for a template using themodel: key in the associated YAML. Here’s a template\\ncalled roast:\\nmodel: gpt-4o\\nsystem: roast the user at every possible opportunity, be succinct\\nExample:\\nllm -t roast \\'How are you today?\\'\\nI’m doing great but with your boring questions, I must admit, I’ve seen more life in a cemetery.\\n2.7. Templates 55\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.7.5 Template loaders from plugins\\nLLM plugins canregister prefixesthat can be used to load templates from external sources.\\nllm-templates-github is an example which adds agh: prefix which can be used to load templates from GitHub.\\nYou can install that plugin like this:\\nllm install llm-templates-github\\nUse thellm templates loaderscommand to see details of the registered loaders.\\nllm templates loaders\\nOutput:\\ngh:\\nLoad a template from GitHub or local cache if available\\nFormat: username/repo/template_name (without the .yaml extension)\\nor username/template_name which means username/llm-templates/template_name\\nThen you can then use it like this:\\ncurl -sL \\'https://llm.datasette.io/\\' | llm -t gh:simonw/summarize\\nThe -sLflags tocurlare used to follow redirects and suppress progress meters.\\nThis command will fetch the content of the LLM index page and feed it to the template defined by summarize.yaml in\\nthe simonw/llm-templates GitHub repository.\\nIf two template loader plugins attempt to register the same prefix one of them will have_1 added to the end of their\\nprefix. Usellm templates loadersto check if this has occurred.\\n2.8 Fragments\\nLLM prompts can optionally be composed out offragments - reusable pieces of text that are logged just once to the\\ndatabase and can then be attached to multiple prompts.\\nTheseareparticularlyusefulwhenyouareworkingwithlongcontextmodels, whichsupportfeedinglargeamountsof\\ntext in as part of your prompt.\\nFragmentsprimarilyexisttosavespaceinthedatabase,butmaybeusedtosupportotherfeaturessuchasvendorprompt\\ncaching as well.\\nFragments can be specified using several different mechanisms:\\n• URLs to text files online\\n• Paths to text files on disk\\n• Aliases that have been attached to a specific fragment\\n• Hash IDs of stored fragments, where the ID is the SHA256 hash of the fragment content\\n• Fragments that are provided by custom plugins - these look likeplugin-name:argument\\n56 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.8.1 Using fragments in a prompt\\nUse the-f/--fragmentoption to specify one or more fragments to be used as part of your prompt:\\nllm -f https://llm.datasette.io/robots.txt \"Explain this robots.txt file in detail\"\\nHere we are specifying a fragment using a URL. The contents of that URL will be included in the prompt that is sent\\nto the model, prepended prior to the prompt text.\\nThe -f option can be used multiple times to combine together multiple fragments.\\nFragments can also be files on disk, for example:\\nllm -f setup.py \\'extract the metadata\\'\\nUse -to specify a fragment that is read from standard input:\\nllm -f - \\'extract the metadata\\' < setup.py\\nThis will read the contents ofsetup.pyfrom standard input and use it as a fragment.\\nFragmentscanalsobeusedaspartofyoursystemprompt. Use --sf valueor --system-fragment valueinstead\\nof -f.\\n2.8.2 Using fragments in chat\\nThe chatcommand also supports the-f and --sf arguments to start a chat with fragments.\\nllm chat -f my_doc.txt\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> Explain this document to me\\nFragments can also be addedduring a chat conversation using the!fragment <my_fragment>command.\\nChatting with gpt-4\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> !fragment https://llm.datasette.io/en/stable/fragments.html\\nThis can be combined with!multi:\\n> !multi\\nExplain the difference between fragments and templates to me\\n!fragment https://llm.datasette.io/en/stable/fragments.html https://llm.datasette.io/en/\\n˓→stable/templates.html\\n!end\\nAny!fragmentlines found in a prompt created with!editwill not be parsed.\\n2.8. Fragments 57\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.8.3 Browsing fragments\\nYoucanviewatruncatedversionofthefragmentsyouhavepreviouslystoredinyourdatabasewiththe llm fragments\\ncommand:\\nllm fragments\\nThe output from that command looks like this:\\n- hash: 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\naliases: []\\ndatetime_utc: \\'2025-04-06 07:36:53\\'\\nsource: https://raw.githubusercontent.com/simonw/llm-docs/refs/heads/main/llm/0.22.txt\\ncontent: |-\\n<documents>\\n<document index=\"1\">\\n<source>docs/aliases.md</source>\\n<document_content>\\n(aliases)=\\n#...\\n- hash: 16b686067375182573e2aa16b5bfc1e64d48350232535d06444537e51f1fd60c\\naliases: []\\ndatetime_utc: \\'2025-04-06 23:03:47\\'\\nsource: simonw/files-to-prompt/pyproject.toml\\ncontent: |-\\n[project]\\nname = \"files-to-prompt\"\\nversion = \"0.6\"\\ndescription = \"Concatenate a directory full of...\\nThose longhashvalues are IDs that can be used to reference a fragment in the future:\\nllm -f 16b686067375182573e2aa16b5bfc1e64d48350232535d06444537e51f1fd60c \\'Extract metadata\\n˓→\\'\\nUse -q searchtermone or more times to search for fragments that match a specific set of search terms.\\nTo view the full content of a fragment usellm fragments show:\\nllm fragments show 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\n2.8.4 Setting aliases for fragments\\nYou can assign aliases to fragments that you use often using thellm fragments setcommand:\\nllm fragments set mydocs ./docs.md\\nTo remove an alias, usellm fragments remove:\\nllm fragments remove mydocs\\nYou can then use that alias in place of the fragment hash ID:\\n58 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm -f mydocs \\'How do I access metadata?\\'\\nUse llm fragments --aliasesto see a full list of fragments that have been assigned aliases:\\nllm fragments --aliases\\n2.8.5 Viewing fragments in your logs\\nThe llm logscommand lists the fragments that were used for a prompt. By default these are listed as fragment hash\\nIDs, but you can use the--expandoption to show the full content of each fragment.\\nThis command will show the expanded fragments for your most recent conversation:\\nllm logs -c --expand\\nYou can filter for logs that used a specific fragment using the-f/--fragmentoption:\\nllm logs -c -f 0d6e368f9bc21f8db78c01e192ecf925841a957d8b991f5bf9f6239aa4d81815\\nThis accepts URLs, file paths, aliases, and hash IDs.\\nMultiple -f options will return responses that usedall of the specified fragments.\\nFragmentsarereturnedby llm logs --jsonaswell. Bydefaultthesearetruncatedbutyoucanaddthe -e/--expand\\noption to show the full content of each fragment.\\nllm logs -c --json --expand\\n2.8.6 Using fragments from plugins\\nLLM plugins can provide custom fragment loaders which do useful things.\\nOne example is the llm-fragments-github plugin. This can convert the files from a public GitHub repository into a list\\nof fragments, allowing you to ask questions about the full repository.\\nHere’s how to try that out:\\nllm install llm-fragments-github\\nllm -f github:simonw/s3-credentials \\'Suggest new features for this tool\\'\\nThis plugin turns a single call to-f github:simonw/s3-credentials into multiple fragments, one for every text\\nfile in the simonw/s3-credentials GitHub repository.\\nRunningllm logs -cwill show that this prompt incorporated 26 fragments, one for each file.\\nRunningllm logs -c --usage --expand(shortcut: llm logs -cue)includestokenusageinformationandturns\\neach fragment ID into a full copy of that file. Here’s the output of that command.\\nFragment plugins can returnattachments(such as images) as well.\\nSeethe register_fragment_loaders()pluginhook documentationfordetailsonwritingyourowncustomfragmentplu-\\ngin.\\n2.8. Fragments 59\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.8.7 Listing available fragment prefixes\\nThe llm fragments loaders command shows all prefixes that have been installed by plugins, along with their\\ndocumentation:\\nllm install llm-fragments-github\\nllm fragments loaders\\nExample output:\\ngithub:\\nLoad files from a GitHub repository as fragments\\nArgument is a GitHub repository URL or username/repository\\nissue:\\nFetch GitHub issue and comments as Markdown\\nArgument is either \"owner/repo/NUMBER\"\\nor \"https://github.com/owner/repo/issues/NUMBER\"\\n2.9 Model aliases\\nLLM supports model aliases, which allow you to refer to a model by a short name instead of its full ID.\\n2.9.1 Listing aliases\\nTo list current aliases, run this:\\nllm aliases\\nExample output:\\n4o : gpt-4o\\nchatgpt-4o : chatgpt-4o-latest\\n4o-mini : gpt-4o-mini\\n4.1 : gpt-4.1\\n4.1-mini : gpt-4.1-mini\\n4.1-nano : gpt-4.1-nano\\n3.5 : gpt-3.5-turbo\\nchatgpt : gpt-3.5-turbo\\nchatgpt-16k : gpt-3.5-turbo-16k\\n3.5-16k : gpt-3.5-turbo-16k\\n4 : gpt-4\\ngpt4 : gpt-4\\n4-32k : gpt-4-32k\\ngpt-4-turbo-preview : gpt-4-turbo\\n4-turbo : gpt-4-turbo\\n4t : gpt-4-turbo\\ngpt-4.5 : gpt-4.5-preview\\n3.5-instruct : gpt-3.5-turbo-instruct\\n(continues on next page)\\n60 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nchatgpt-instruct : gpt-3.5-turbo-instruct\\nada : text-embedding-ada-002 (embedding)\\nada-002 : text-embedding-ada-002 (embedding)\\n3-small : text-embedding-3-small (embedding)\\n3-large : text-embedding-3-large (embedding)\\n3-small-512 : text-embedding-3-small-512 (embedding)\\n3-large-256 : text-embedding-3-large-256 (embedding)\\n3-large-1024 : text-embedding-3-large-1024 (embedding)\\nAdd--jsonto get that list back as JSON:\\nllm aliases list --json\\nExample output:\\n{\\n\"3.5\": \"gpt-3.5-turbo\",\\n\"chatgpt\": \"gpt-3.5-turbo\",\\n\"4\": \"gpt-4\",\\n\"gpt4\": \"gpt-4\",\\n\"ada\": \"ada-002\"\\n}\\n2.9.2 Adding a new alias\\nThe llm aliases set <alias> <model-id>command can be used to add a new alias:\\nllm aliases set mini gpt-4o-mini\\nYou can also pass one or more-q searchoptions to set an alias on the first model matching those search terms:\\nllm aliases set mini -q 4o -q mini\\nNow you can run thegpt-4o-minimodel using theminialias like this:\\nllm -m mini \\'An epic Greek-style saga about a cheesecake that builds a SQL database from␣\\n˓→scratch\\'\\nAliases can be set for both regular models andembedding modelsusing the same command. To set an alias ofoaifor\\nthe OpenAIada-002embedding model use this:\\nllm aliases set oai ada-002\\nNow you can embed a string using that model like so:\\nllm embed -c \\'hello world\\' -m oai\\nOutput:\\n[-0.014945968054234982, 0.0014304015785455704, ...]\\n2.9. Model aliases 61\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.9.3 Removing an alias\\nThe llm aliases remove <alias>command will remove the specified alias:\\nllm aliases remove mini\\n2.9.4 Viewing the aliases file\\nAliases are stored in analiases.jsonfile in the LLM configuration directory.\\nTo see the path to that file, run this:\\nllm aliases path\\nTo view the content of that file, run this:\\ncat \"$(llm aliases path)\"\\n2.10 Embeddings\\nEmbedding models allow you to take a piece of text - a word, sentence, paragraph or even a whole article, and convert\\nthat into an array of floating point numbers.\\nThis floating point array is called an “embedding vector”, and works as a numerical representation of the semantic\\nmeaning of the content in a many-multi-dimensional space.\\nBy calculating the distance between embedding vectors, we can identify which content is semantically “nearest” to\\nother content.\\nThis can be used to build features like related article lookups. It can also be used to build semantic search, where a\\nuser can search for a phrase and get back results that are semantically similar to that phrase even if they do not share\\nany exact keywords.\\nSomeembeddingmodelslikeCLIPcanevenworkagainstbinaryfilessuchasimages. Thesecanbeusedtosearchfor\\nimages that are similar to other images, or to search for images that are semantically similar to a piece of text.\\nLLM supports multiple embedding models throughplugins. Once installed, an embedding model can be used on the\\ncommand-line or via the Python API to calculate and store embeddings for content, and then to perform similarity\\nsearches against those embeddings.\\nSee LLM now provides tools for working with embeddings for an extended explanation of embeddings, why they are\\nuseful and what you can do with them.\\n2.10.1 Embedding with the CLI\\nLLM provides command-line utilities for calculating and storing embeddings for pieces of content.\\n62 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm embed\\nThe llm embed command can be used to calculate embedding vectors for a string of content. These can be returned\\ndirectly to the terminal, stored in a SQLite database, or both.\\nReturning embeddings to the terminal\\nThe simplest way to use this command is to pass content to it using the-c/--contentoption, like this:\\nllm embed -c \\'This is some content\\' -m 3-small\\n-m 3-smallspecifiestheOpenAI text-embedding-3-smallmodel. YouwillneedtohavesetanOpenAIAPIkey\\nusing llm keys set openaifor this to work.\\nYou can install plugins to access other models. The llm-sentence-transformers plugin can be used to run models on\\nyour own laptop, such as the MiniLM-L6 model:\\nllm install llm-sentence-transformers\\nllm embed -c \\'This is some content\\' -m sentence-transformers/all-MiniLM-L6-v2\\nThe llm embedcommand returns a JSON array of floating point numbers directly to the terminal:\\n[0.123, 0.456, 0.789...]\\nYou can omit the-m/--modeloption if you set adefault embedding model.\\nYoucanalsosetthe LLM_EMBEDDING_MODELenvironmentvariabletosetadefaultmodelforall llm embedcommands\\nin the current shell session:\\nexport LLM_EMBEDDING_MODEL=3-small\\nllm embed -c \\'This is some content\\'\\nLLM also offers a binary storage format for embeddings, described inembeddings storage format.\\nYoucanoutputembeddingsusingthatformatasrawbytesusing --format blob,orinhexadecimalusing --format\\nhex, or in Base64 using--format base64:\\nllm embed -c \\'This is some content\\' -m 3-small --format base64\\nThis outputs:\\n8NGzPFtdgTqHcZw7aUT6u+++WrwwpZo8XbSxv...\\nSome models such as llm-clip can run against binary data. You can pass in binary data using the-i and --binary\\noptions:\\nllm embed --binary -m clip -i image.jpg\\nOr from standard input like this:\\ncat image.jpg | llm embed --binary -m clip -i -\\n2.10. Embeddings 63\\nLLM documentation, Release 0.26-31-g0bf655a\\nStoring embeddings in SQLite\\nEmbeddingsaremuchmoreusefulifyoustorethemsomewhere,soyoucancalculatesimilarityscoresbetweendifferent\\nembeddings later on.\\nLLM includes the concept of acollection of embeddings. A collection groups together a set of stored embeddings\\ncreated using the same model, each with a unique ID within that collection.\\nEmbeddings also store a hash of the content that was embedded. This hash is later used to avoid calculating duplicate\\nembeddings for the same content.\\nFirst, we’ll set a default model so we don’t have to keep repeating it:\\nllm embed-models default 3-small\\nThe llm embedcommand can store results directly in a named collection like this:\\nllm embed quotations philkarlton-1 -c \\\\\\n\\'There are only two hard things in Computer Science: cache invalidation and naming␣\\n˓→things\\'\\nThis stores the given text in thequotationscollection under the keyphilkarlton-1.\\nYou can also pipe content to standard input, like this:\\ncat one.txt | llm embed files one\\nThis will store the embedding for the contents ofone.txtin thefilescollection under the keyone.\\nA collection will be created the first time you mention it.\\nCollections have a fixed embedding model, which is the model that was used for the first embedding stored in that\\ncollection.\\nIn the above example this would have been the default embedding model at the time that the command was run.\\nThe following example stores the embedding for the string “my happy hound” in a collection calledphrases under\\nthe keyhoundand using the model3-small:\\nllm embed phrases hound -m 3-small -c \\'my happy hound\\'\\nBydefault,theSQLitedatabaseusedtostoreembeddingsisthe embeddings.dbintheusercontentdirectorymanaged\\nby LLM.\\nYou can see the path to this directory by runningllm collections path.\\nYou can store embeddings in a different SQLite database by passing a path to it using the-d/--database option to\\nllm embed. If this file does not exist yet the command will create it:\\nllm embed phrases hound -d my-embeddings.db -c \\'my happy hound\\'\\nThis creates a database file calledmy-embeddings.dbin the current directory.\\n64 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nStoring content and metadata\\nBy default, only the entry ID and the embedding vector are stored in the database table.\\nYou can store a copy of the original text in thecontentcolumn by passing the--storeoption:\\nllm embed phrases hound -c \\'my happy hound\\' --store\\nYou can also store a JSON object containing arbitrary metadata in themetadatacolumn by passing the--metadata\\noption. This example uses both--storeand --metadataoptions:\\nllm embed phrases hound \\\\\\n-m 3-small \\\\\\n-c \\'my happy hound\\' \\\\\\n--metadata \\'{\"name\": \"Hound\"}\\' \\\\\\n--store\\nData stored in this way will be returned by calls tollm similar, for example:\\nllm similar phrases -c \\'hound\\'\\n{\"id\": \"hound\", \"score\": 0.8484683588631485, \"content\": \"my happy hound\", \"metadata\": {\\n˓→\"name\": \"Hound\"}}\\nllm embed-multi\\nThe llm embedcommand embeds a single string at a time.\\nllm embed-multi can be used to embed multiple strings at once, taking advantage of any efficiencies that the em-\\nbedding model may provide when processing multiple strings.\\nThis command can be called in one of three ways:\\n1. With a CSV, TSV, JSON or newline-delimited JSON file\\n2. With a SQLite database and a SQL query\\n3. With one or more paths to directories, each accompanied by a glob pattern\\nAll three mechanisms support these options:\\n• -m model_idto specify the embedding model to use\\n• -d database.dbto specify a different database file to store the embeddings in\\n• --storeto store the original content in the embeddings table in addition to the embedding vector\\n• --prefixto prepend a prefix to the stored ID of each item\\n• --prependto prepend a string to the content before embedding\\n• --batch-size SIZEto process embeddings in batches of the specified size\\nThe--prependoptionisusefulforembeddingmodelsthatrequireyoutoprependaspecialtokentothecontentbefore\\nembedding it. nomic-embed-text-v2-moe for example requires documents to be prepended\\'search_document: \\'\\nand search queries to be prepended\\'search_query: \\'.\\n2.10. Embeddings 65\\nLLM documentation, Release 0.26-31-g0bf655a\\nEmbedding data from a CSV, TSV or JSON file\\nYoucanembeddatafromaCSV,TSVorJSONfilebypassingthatfiletothecommandasthesecondoption, afterthe\\ncollection name.\\nYourfilemustcontainatleasttwocolumns. ThefirstoneisexpectedtocontaintheIDoftheitem,andanysubsequent\\ncolumns will be treated as containing content to be embedded.\\nAn example CSV file might look like this:\\nid,content\\none,This is the first item\\ntwo,This is the second item\\nTSV would use tabs instead of commas.\\nJSON files can be structured like this:\\n[\\n{\"id\": \"one\", \"content\": \"This is the first item\"},\\n{\"id\": \"two\", \"content\": \"This is the second item\"}\\n]\\nOr as newline-delimited JSON like this:\\n{\"id\": \"one\", \"content\": \"This is the first item\"}\\n{\"id\": \"two\", \"content\": \"This is the second item\"}\\nIn each of these cases the file can be passed tollm embed-multilike this:\\nllm embed-multi items mydata.csv\\nThe first argument is the name of the collection, the second is the filename.\\nYou can also pipe content to standard input of the tool using-:\\ncat mydata.json | llm embed-multi items -\\nLLMwillattempttodetecttheformatofyourdataautomatically. Ifthisdoesn’tworkyoucanspecifytheformatusing\\nthe --formatoption. This is required if you are piping newline-delimited JSON to standard input.\\ncat mydata.json | llm embed-multi items - --format nl\\nOther supported--formatoptions arecsv, tsvand json.\\nThis example embeds the data from a JSON file in a collection calleditems in database calleddocs.db using the\\n3-smallmodelandstorestheoriginalcontentinthe embeddingstableaswell,addingaprefixof my-items/toeach\\nID:\\nllm embed-multi items mydata.json \\\\\\n-d docs.db \\\\\\n-m 3-small \\\\\\n--prefix my-items/ \\\\\\n--store\\n66 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nEmbedding data from a SQLite database\\nYoucanembeddatafromaSQLitedatabaseusing --sql,optionallycombinedwith --attachtoattachanadditional\\ndatabase.\\nIf you are storing embeddings in the same database as the source data, you can do this:\\nllm embed-multi docs \\\\\\n-d docs.db \\\\\\n--sql \\'select id, title, content from documents\\' \\\\\\n-m 3-small\\nThe docs.db database here contains adocuments table, and we want to embed thetitle and content columns\\nfrom that table and store the results back in the same database.\\nTo load content from a database other than the one you are using to store embeddings, attach it with the--attach\\noption and usealias.tablein your SQLite query:\\nllm embed-multi docs \\\\\\n-d embeddings.db \\\\\\n--attach other other.db \\\\\\n--sql \\'select id, title, content from other.documents\\' \\\\\\n-m 3-small\\nEmbedding data from files in directories\\nLLM can embed the content of every text file in a specified directory, using the file’s path and name as the ID.\\nConsider a directory structure like this:\\ndocs/aliases.md\\ndocs/contributing.md\\ndocs/embeddings/binary.md\\ndocs/embeddings/cli.md\\ndocs/embeddings/index.md\\ndocs/index.md\\ndocs/logging.md\\ndocs/plugins/directory.md\\ndocs/plugins/index.md\\nTo embed all of those documents, you can run the following:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs \\'**/*.md\\' \\\\\\n-d documentation.db \\\\\\n--store\\nHere--files docs \\'**/*.md\\' specifies that thedocsdirectory should be scanned for files matching the**/*.md\\nglob pattern - which will match Markdown files in any nested directory.\\nThe result of the above command is aembeddingstable with the following IDs:\\n2.10. Embeddings 67\\nLLM documentation, Release 0.26-31-g0bf655a\\naliases.md\\ncontributing.md\\nembeddings/binary.md\\nembeddings/cli.md\\nembeddings/index.md\\nindex.md\\nlogging.md\\nplugins/directory.md\\nplugins/index.md\\nEach corresponding to embedded content for the file in question.\\nThe --prefixoption can be used to add a prefix to each ID:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs \\'**/*.md\\' \\\\\\n-d documentation.db \\\\\\n--store \\\\\\n--prefix llm-docs/\\nThis will result in the following IDs instead:\\nllm-docs/aliases.md\\nllm-docs/contributing.md\\nllm-docs/embeddings/binary.md\\nllm-docs/embeddings/cli.md\\nllm-docs/embeddings/index.md\\nllm-docs/index.md\\nllm-docs/logging.md\\nllm-docs/plugins/directory.md\\nllm-docs/plugins/index.md\\nFilesareassumedtobe utf-8, butLLMwillfallbackto latin-1ifitencountersanencodingerror. Youcanspecify\\na different set of encodings using the--encodingoption.\\nThis example will tryutf-16first and thenmac_romanbefore falling back tolatin-1:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs \\'**/*.md\\' \\\\\\n-d documentation.db \\\\\\n--encoding utf-16 \\\\\\n--encoding mac_roman \\\\\\n--encoding latin-1\\nIf a file cannot be read it will be logged to standard error but the script will keep on running.\\nIf you are embedding binary content such as images for use with CLIP, add the--binaryoption:\\nllm embed-multi photos \\\\\\n-m clip \\\\\\n--files photos/ \\'*.jpeg\\' --binary\\n68 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm similar\\nThellm similarcommandsearchesacollectionofembeddingsfortheitemsthataremostsimilartoagivenoritem\\nID, based on cosine similarity.\\nThis currently uses a slow brute-force approach which does not scale well to large collections. See issue 216 for plans\\nto add a more scalable approach via vector indexes provided by plugins.\\nTo search thequotationscollection for items that are semantically similar to\\'computer science\\':\\nllm similar quotations -c \\'computer science\\'\\nThis embeds the provided string and returns a newline-delimited list of JSON objects like this:\\n{\"id\": \"philkarlton-1\", \"score\": 0.8323904531677017, \"content\": null, \"metadata\": null}\\nUse -p/--plainto get back results in plain text instead of JSON:\\nllm similar quotations -c \\'computer science\\' -p\\nExample output:\\nphilkarlton-1 (0.8323904531677017)\\nYou can compare against text stored in a file using-i filename:\\nllm similar quotations -i one.txt\\nOr feed text to standard input using-i -:\\necho \\'computer science\\' | llm similar quotations -i -\\nWhen using a model like CLIP, you can find images similar to an input image using-i filenamewith --binary:\\nllm similar photos -i image.jpg --binary\\nYou can filter results to only show IDs that begin with a specific prefix using –prefix:\\nllm similar quotations --prefix \\'movies/\\' -c \\'star wars\\'\\nllm embed-models\\nTo list all available embedding models, including those provided by plugins, run this command:\\nllm embed-models\\nThe output should look something like this:\\nOpenAIEmbeddingModel: text-embedding-ada-002 (aliases: ada, ada-002)\\nOpenAIEmbeddingModel: text-embedding-3-small (aliases: 3-small)\\nOpenAIEmbeddingModel: text-embedding-3-large (aliases: 3-large)\\n...\\nAdd-qone or more times to search for models matching those terms:\\n2.10. Embeddings 69\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm embed-models -q 3-small\\nllm embed-models default\\nThis command can be used to get and set the default embedding model.\\nThis will return the name of the current default model:\\nllm embed-models default\\nYou can set a different default like this:\\nllm embed-models default 3-small\\nThis will set the default model to OpenAI’s3-smallmodel.\\nAny of the supported aliases for a model can be passed to this command.\\nYou can unset the default model using--remove-default:\\nllm embed-models default --remove-default\\nWhennodefaultmodelisset,the llm embedandllm embed-multicommandswillrequirethatamodelisspecified\\nusing -m/--model.\\nllm collections list\\nTo list all of the collections in the embeddings database, run this command:\\nllm collections list\\nAdd--jsonfor JSON output:\\nllm collections list --json\\nAdd-d/--databaseto specify a different database file:\\nllm collections list -d my-embeddings.db\\nllm collections delete\\nTo delete a collection from the database, run this:\\nllm collections delete collection-name\\nPass-dto specify a different database file:\\nllm collections delete collection-name -d my-embeddings.db\\n70 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.10.2 Using embeddings from Python\\nYou can load an embedding model using its model ID or alias like this:\\nimport llm\\nembedding_model = llm.get_embedding_model(\"3-small\")\\nTo embed a string, returning a Python list of floating point numbers, use the.embed()method:\\nvector = embedding_model.embed(\"my happy hound\")\\nIf the embedding model can handle binary input, you can call.embed()with a byte string instead. You can check the\\nsupports_binaryproperty to see if this is supported:\\nif embedding_model.supports_binary:\\nvector = embedding_model.embed(open(\"my-image.jpg\", \"rb\").read())\\nThe embedding_model.supports_textproperty indicates if the model supports text input.\\nMany embeddings models are more efficient when you embed multiple strings or binary strings at once. To embed\\nmultiple strings at once, use the.embed_multi()method:\\nvectors = list(embedding_model.embed_multi([\"my happy hound\", \"my dissatisfied cat\"]))\\nThis returns a generator that yields one embedding vector per string.\\nEmbeddings are calculated in batches. By default all items will be processed in a single batch, unless the underlying\\nembedding model has defined its own preferred batch size. You can pass a custom batch size usingbatch_size=N,\\nfor example:\\nvectors = list(embedding_model.embed_multi(lines_from_file, batch_size=20))\\nWorking with collections\\nThe llm.Collectionclass can be used to work withcollections of embeddings from Python code.\\nA collection is a named group of embedding vectors, each stored along with their IDs in a SQLite database table.\\nToworkwithembeddingsinthiswayyouwillneedaninstanceofasqlite-utilsDatabaseobject. Youcanthenpassthat\\nto thellm.Collectionconstructor along with the unique string name of the collection and the ID of the embedding\\nmodel you will be using with that collection:\\nimport sqlite_utils\\nimport llm\\n# This collection will use an in-memory database that will be\\n# discarded when the Python process exits\\ncollection = llm.Collection(\"entries\", model_id=\"3-small\")\\n# Or you can persist the database to disk like this:\\ndb = sqlite_utils.Database(\"my-embeddings.db\")\\ncollection = llm.Collection(\"entries\", db, model_id=\"3-small\")\\n# You can pass a model directly using model= instead of model_id=\\n(continues on next page)\\n2.10. Embeddings 71\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nembedding_model = llm.get_embedding_model(\"3-small\")\\ncollection = llm.Collection(\"entries\", db, model=embedding_model)\\nIf the collection already exists in the database you can omit themodel or model_id argument - the model ID will be\\nread from thecollectionstable.\\nTo embed a single string and store it in the collection, use theembed()method:\\ncollection.embed(\"hound\", \"my happy hound\")\\nThis stores the embedding for the string “my happy hound” in theentriescollection under the keyhound.\\nAddstore=Trueto store the text content itself in the database table along with the embedding vector.\\nTo attach additional metadata to an item, pass a JSON-compatible dictionary as themetadata=argument:\\ncollection.embed(\"hound\", \"my happy hound\", metadata={\"name\": \"Hound\"}, store=True)\\nThis additional metadata will be stored as JSON in themetadatacolumn of the embeddings database table.\\nStoring embeddings in bulk\\nThe collection.embed_multi() method can be used to store embeddings for multiple items at once. This can be\\nmore efficient for some embedding models.\\ncollection.embed_multi(\\n[\\n(\"hound\", \"my happy hound\"),\\n(\"cat\", \"my dissatisfied cat\"),\\n],\\n# Add this to store the strings in the content column:\\nstore=True,\\n)\\nTo include metadata to be stored with each item, callembed_multi_with_metadata():\\ncollection.embed_multi_with_metadata(\\n[\\n(\"hound\", \"my happy hound\", {\"name\": \"Hound\"}),\\n(\"cat\", \"my dissatisfied cat\", {\"name\": \"Cat\"}),\\n],\\n# This can also take the store=True argument:\\nstore=True,\\n)\\nThebatch_size=argumentdefaultsto100,andwillbeusedunlesstheembeddingmodelitselfdefinesalowerbatch\\nsize. You can adjust this if you are having trouble with memory while embedding large collections:\\ncollection.embed_multi(\\n(\\n(i, line)\\nfor i, line in enumerate(lines_in_file)\\n),\\n(continues on next page)\\n72 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbatch_size=10\\n)\\nCollection class reference\\nA collection instance has the following properties and methods:\\n• id- the integer ID of the collection in the database\\n• name- the string name of the collection (unique in the database)\\n• model_id- the string ID of the embedding model used for this collection\\n• model()- returns theEmbeddingModelinstance, based on thatmodel_id\\n• count()- returns the integer number of items in the collection\\n• embed(id: str, text: str, metadata: dict=None, store: bool=False) - embeds the given\\nstring and stores it in the collection under the given ID. Can optionally include metadata (stored as JSON) and\\nstore the text content itself in the database table.\\n• embed_multi(entries: Iterable, store: bool=False, batch_size: int=100) - see above\\n• embed_multi_with_metadata(entries: Iterable, store: bool=False, batch_size:\\nint=100)- see above\\n• similar(query: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembedding\\nof the given query string\\n• similar_by_id(id: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembed-\\nding of the item with the given ID\\n• similar_by_vector(vector: List[float], number: int=10, skip_id: str=None) - returns a\\nlist of entries that are most similar to the given embedding vector, optionally skipping the entry with the given\\nID\\n• delete()- deletes the collection and its embeddings from the database\\nThere is also aCollection.exists(db, name) class method which returns a boolean value and can be used to\\ndetermine if a collection exists or not in a database:\\nif Collection.exists(db, \"entries\"):\\nprint(\"The entries collection exists\")\\nRetrieving similar items\\nOnceyouhavepopulatedacollectionofembeddingsyoucanretrievetheentriesthataremostsimilartoagivenstring\\nusing thesimilar()method.\\nThis method uses a brute force approach, calculating distance scores against every document. This is fine for small\\ncollections, but will not scale to large collections. See issue 216 for plans to add a more scalable approach via vector\\nindexes provided by plugins.\\nfor entry in collection.similar(\"hound\"):\\nprint(entry.id, entry.score)\\n2.10. Embeddings 73\\nLLM documentation, Release 0.26-31-g0bf655a\\nThe string will first by embedded using the model for the collection.\\nThe entryobject returned is an object with the following properties:\\n• id- the string ID of the item\\n• score- the floating point similarity score between the item and the query string\\n• content- the string text content of the item, if it was stored - orNone\\n• metadata- the dictionary (from JSON) metadata for the item, if it was stored - orNone\\nThis defaults to returning the 10 most similar items. You can change this by passing a differentnumber=argument:\\nfor entry in collection.similar(\"hound\", number=5):\\nprint(entry.id, entry.score)\\nThesimilar_by_id()methodtakestheIDofanotheriteminthecollectionandreturnsthemostsimilaritemstothat\\none, based on the embedding that has already been stored for it:\\nfor entry in collection.similar_by_id(\"cat\"):\\nprint(entry.id, entry.score)\\nThe item itself is excluded from the results.\\nSQL schema\\nHere’s the SQL schema used by the embeddings database:\\nCREATE TABLE [collections] (\\n[id] INTEGER PRIMARY KEY,\\n[name] TEXT,\\n[model] TEXT\\n)\\nCREATE TABLE \"embeddings\" (\\n[collection_id] INTEGER REFERENCES [collections]([id]),\\n[id] TEXT,\\n[embedding] BLOB,\\n[content] TEXT,\\n[content_blob] BLOB,\\n[content_hash] BLOB,\\n[metadata] TEXT,\\n[updated] INTEGER,\\nPRIMARY KEY ([collection_id], [id])\\n)\\n2.10.3 Writing plugins to add new embedding models\\nRead theplugin tutorialfor details on how to develop and package a plugin.\\nThis page shows an example plugin that implements and registers a new embedding model.\\nThere are two components to an embedding model plugin:\\n1. An implementation of theregister_embedding_models()hook, which takes aregistercallback function\\nand calls it to register the new model with the LLM plugin system.\\n74 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2. A class that extends thellm.EmbeddingModelabstract base class.\\nThe only required method on this class isembed_batch(texts), which takes an iterable of strings and returns\\nan iterator over lists of floating point numbers.\\nThefollowingexampleusesthesentence-transformerspackagetoprovideaccesstotheMiniLM-L6embeddingmodel.\\nimport llm\\nfrom sentence_transformers import SentenceTransformer\\n@llm.hookimpl\\ndef register_embedding_models(register):\\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\"\\nregister(SentenceTransformerModel(model_id, model_id), aliases=(\"all-MiniLM-L6-v2\",))\\nclass SentenceTransformerModel(llm.EmbeddingModel):\\ndef __init__(self, model_id, model_name):\\nself.model_id = model_id\\nself.model_name = model_name\\nself._model = None\\ndef embed_batch(self, texts):\\nif self._model is None:\\nself._model = SentenceTransformer(self.model_name)\\nresults = self._model.encode(texts)\\nreturn (list(map(float, result)) for result in results)\\nOnce installed, the model provided by this plugin can be used with thellm embedcommand like this:\\ncat file.txt | llm embed -m sentence-transformers/all-MiniLM-L6-v2\\nOr via its registered alias like this:\\ncat file.txt | llm embed -m all-MiniLM-L6-v2\\nllm-sentence-transformers is a complete example of a plugin that provides an embedding model.\\nExecute Jina embeddings with a CLI using llm-embed-jina talks through a similar process to add support for the Jina\\nembeddings models.\\nEmbedding binary content\\nIf your model can embed binary content, use thesupports_binaryproperty to indicate that:\\nclass ClipEmbeddingModel(llm.EmbeddingModel):\\nmodel_id = \"clip\"\\nsupports_binary = True\\nsupports_text= True\\nsupports_text defaults toTrue and so is not necessary here. You can set it toFalse if your model only supports\\nbinary data.\\nIf your model accepts binary, your.embed_batch() model may be called with a list of Python bytestrings. These\\nmay be mixed with regular strings if the model accepts both types of input.\\n2.10. Embeddings 75\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm-clip is an example of a model that can embed both binary and text content.\\n2.10.4 Embedding storage format\\nThe default output format of thellm embedcommand is a JSON array of floating point numbers.\\nLLM stores embeddings in space-efficient format: a little-endian binary sequences of 32-bit floating point numbers,\\neach represented using 4 bytes.\\nThese are stored in aBLOBcolumn in a SQLite database.\\nThe following Python functions can be used to convert between this format and an array of floating point numbers:\\nimport struct\\ndef encode(values):\\nreturn struct.pack(\"<\" + \"f\" * len(values), *values)\\ndef decode(binary):\\nreturn struct.unpack(\"<\" + \"f\" * (len(binary) // 4), binary)\\nThese functions are available asllm.encode()and llm.decode().\\nIf you are using NumPy you can decode one of these binary values like this:\\nimport numpy as np\\nnumpy_array = np.frombuffer(value, \"<f4\")\\nThe <f4format string here ensures NumPy will treat the data as a little-endian sequence of 32-bit floats.\\n2.11 Plugins\\nLLMpluginscanenhanceLLMbymakingalternativeLargeLanguageModelsavailable,eitherviaAPIorbyrunning\\nthe models locally on your machine.\\nPlugins can also add new commands to thellmCLI tool.\\nThe plugin directorylists available plugins that you can install and use.\\nDeveloping a model plugindescribes how to build a new plugin in detail.\\n2.11.1 Installing plugins\\nPlugins must be installed in the same virtual environment as LLM itself.\\nYou can find names of plugins to install in theplugin directory\\nUse thellm installcommand (a thin wrapper aroundpip install) to install plugins in the correct environment:\\nllm install llm-gpt4all\\nPlugins can be uninstalled withllm uninstall:\\nllm uninstall llm-gpt4all -y\\n76 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nThe -yflag skips asking for confirmation.\\nYou can see additional models that have been added by plugins by running:\\nllm models\\nOr add--optionsto include details of the options available for each model:\\nllm models --options\\nTo run a prompt against a newly installed model, pass its name as the-m/--modeloption:\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France?\\'\\nListing installed plugins\\nRunllm pluginsto list installed plugins:\\nllm plugins\\n[\\n{\\n\"name\": \"llm-anthropic\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\": \"0.11\"\\n},\\n{\\n\"name\": \"llm-gguf\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n],\\n\"version\": \"0.1a0\"\\n},\\n{\\n\"name\": \"llm-clip\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_embedding_models\"\\n],\\n\"version\": \"0.1\"\\n},\\n{\\n\"name\": \"llm-cmd\",\\n\"hooks\": [\\n\"register_commands\"\\n],\\n\"version\": \"0.2a0\"\\n},\\n{\\n\"name\": \"llm-gemini\",\\n(continues on next page)\\n2.11. Plugins 77\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"hooks\": [\\n\"register_embedding_models\",\\n\"register_models\"\\n],\\n\"version\": \"0.3\"\\n}\\n]\\nRunning with a subset of plugins\\nBy default, LLM will load all plugins that are installed in the same virtual environment as LLM itself.\\nYou can control the set of plugins that is loaded using theLLM_LOAD_PLUGINSenvironment variable.\\nSet that to the empty string to disable all plugins:\\nLLM_LOAD_PLUGINS=\\'\\' llm ...\\nOr to a comma-separated list of plugin names to load only those plugins:\\nLLM_LOAD_PLUGINS=\\'llm-gpt4all,llm-cluster\\' llm ...\\nYou can use thellm pluginscommand to check that it is working correctly:\\nLLM_LOAD_PLUGINS=\\'\\' llm plugins\\n2.11.2 Plugin directory\\nThe following plugins are available for LLM. Here’show to install them.\\nLocal models\\nThese plugins all help you run LLMs directly on your own computer:\\n• llm-ggufuses llama.cpp to run models published in the GGUF format.\\n• llm-mlx (Mac only) uses Apple’s MLX framework to provide extremely high performance access to a large\\nnumber of local models.\\n• llm-ollamaadds support for local models run using Ollama.\\n• llm-llamafileadds support for local models that are running locally using llamafile.\\n• llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n• llm-gpt4all adds support for various models released by the GPT4All project that are optimized to run locally\\non your own machine. These models include versions of Vicuna, Orca, Falcon and MPT - here’s a full list of\\nmodels.\\n• llm-mpt30badds support for the MPT-30B local model.\\n78 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nRemote APIs\\nThese plugins can be used to interact with remotely hosted models via their API:\\n• llm-mistral adds support for Mistral AI’s language and embedding models.\\n• llm-geminiadds support for Google’s Gemini models.\\n• llm-anthropicsupports Anthropic’s Claude 3 family, 3.5 Sonnet and beyond.\\n• llm-command-rsupports Cohere’s Command R and Command R Plus API models.\\n• llm-rekasupports the Reka family of models via their API.\\n• llm-perplexity by Alexandru Geana supports the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-onlinewhich can search for things online andllama-3-70b-instruct.\\n• llm-groqby Moritz Angermann provides access to fast models hosted by Groq.\\n• llm-grokby Benedikt Hiepler providing access to Grok model using the xAI API Grok.\\n• llm-anyscale-endpointssupports models hosted on the Anyscale Endpoints platform, including Llama 2 70B.\\n• llm-replicateadds support for remote models hosted on Replicate, including Llama 2 from Meta AI.\\n• llm-fireworkssupports models hosted by Fireworks AI.\\n• llm-openrouterprovides access to models hosted on OpenRouter.\\n• llm-cohere by Alistair Shepherd providescohere-generate and cohere-summarize API models, powered\\nby Cohere.\\n• llm-bedrockadds support for Nova by Amazon via Amazon Bedrock.\\n• llm-bedrock-anthropicby Sean Blakey adds support for Claude and Claude Instant by Anthropic via Amazon\\nBedrock.\\n• llm-bedrock-metaby Fabian Labat adds support for Llama 2 and Llama 3 by Meta via Amazon Bedrock.\\n• llm-togetheradds support for the Together AI extensive family of hosted openly licensed models.\\n• llm-deepseek adds support for the DeepSeek’s DeepSeek-Chat and DeepSeek-Coder models.\\n• llm-lambda-labs provides access to models hosted by Lambda Labs, including the Nous Hermes 3 series.\\n• llm-venice provides access to uncensored models hosted by privacy-focused Venice AI, including Llama 3.1\\n405B.\\nIfanAPImodelhostprovidesanOpenAI-compatibleAPIyoucanalsoconfigureLLMtotalktoitwithoutneedingan\\nextra plugin.\\nTools\\nThe following plugins add newtoolsthat can be used by models:\\n• llm-tools-simpleevalimplements simple expression support for things like mathematics.\\n• llm-tools-quickjs provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run\\nJavaScript code. The environment persists between calls so the model can set variables and build functions\\nand reuse them later on.\\n• llm-tools-sqlite can run read-only SQL queries against local SQLite databases.\\n• llm-tools-datasette can run SQL queries against a remote Datasette instance.\\n• llm-tools-exaby Dan Turkel can perform web searches and question-answering using exa.ai.\\n2.11. Plugins 79\\nLLM documentation, Release 0.26-31-g0bf655a\\n• llm-tools-rag by Dan Turkel can perform searches over your LLM embedding collections for simple RAG.\\nFragments and template loaders\\nLLM 0.24introduced support for plugins that define-f prefix:value or -t prefix:value custom loaders for\\nfragments and templates.\\n• llm-video-framesusesffmpegtoturnavideointoasequenceofJPEGframessuitableforfeedingintoavision\\nmodelthatdoesn’tsupportvideoinputs: llm -f video-frames:video.mp4 \\'describe the key scenes\\nin this video\\'.\\n• llm-templates-githubsupports loading templates shared on GitHub, e.g.llm -t gh:simonw/pelican-svg.\\n• llm-templates-fabric provides access to the Fabric collection of prompts: cat setup.py | llm -t\\nfabric:explain_code.\\n• llm-fragments-github can load entire GitHub repositories in a single operation:llm -f github:simonw/\\nfiles-to-prompt \\'explain this code\\'. It can also fetch issue threads as Markdown using llm -f\\nissue:https://github.com/simonw/llm-fragments-github/issues/3.\\n• llm-hacker-newsimports conversations from Hacker News as fragments:llm -f hn:43615912 \\'summary\\nwith illustrative direct quotes\\'.\\n• llm-fragments-pypiloadsPyPIpackages’descriptionandmetadataasfragments: llm -f pypi:ruff \"What\\nflake8 plugins does ruff re-implement?\".\\n• llm-fragments-pdfbyDanTurkelconvertsPDFstomarkdownwithPyMuPDF4LLMtouseasfragments: llm\\n-f pdf:something.pdf \"what\\'s this about?\".\\n• llm-fragments-site-textbyDanTurkelconvertswebsitestomarkdownwithTrafilaturatouseasfragments: llm\\n-f site:https://example.com \"summarize this\".\\n• llm-fragments-reader runs a URL theough the Jina Reader API: llm -f \\'reader:https://\\nsimonwillison.net/tags/jina/\\' summary.\\nEmbedding models\\nEmbedding modelsare models that can be used to generate and store embedding vectors for text.\\n• llm-sentence-transformers adds support for embeddings using the sentence-transformers library, which pro-\\nvides access to a wide range of embedding models.\\n• llm-clipprovidestheCLIPmodel,whichcanbeusedtoembedimagesandtextinthesamevectorspace,enabling\\ntext search against images. See Build an image search engine with llm-clip for more on this plugin.\\n• llm-embed-jina provides Jina AI’s 8K text embedding models.\\n• llm-embed-onnx provides seven embedding models that can be executed using the ONNX model framework.\\n80 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nExtra commands\\n• llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so you\\ncan review it, edit it and then hit<enter>to execute orctrl+cto cancel.\\n• llm-cmd-compprovidesakeybindingforyourshellthatwilllaunchachattobuildthecommand. Whenready,\\nhit <enter>and it will go right back into your shell command line, so you can run it.\\n• llm-pythonadds allm python command for running a Python interpreter in the same virtual environment as\\nLLM. This is useful for debugging, and also provides a convenient way to interact with the LLMPython APIif\\nyou installed LLM using Homebrew orpipx.\\n• llm-cluster adds allm cluster command for calculating clusters for a collection of embeddings. Calculated\\nclusters can then be passed to a Large Language Model to generate a summary description.\\n• llm-jq lets you pipe in JSON data and a prompt describing ajq program, then executes the generated program\\nagainst the JSON.\\nJust for fun\\n• llm-markovaddsasimplemodelthatgeneratesoutputusingaMarkovchain. Thisexampleisusedinthetutorial\\nWriting a plugin to support a new model.\\n2.11.3 Plugin hooks\\nPlugins useplugin hooksto customize LLM’s behavior. These hooks are powered by the Pluggy plugin system.\\nEachplugincanimplementoneormorehooksusingthe@hookimpldecoratoragainstoneofthehookfunctionnames\\ndescribed on this page.\\nLLM imitates the Datasette plugin system. The Datasette plugin documentation describes how plugins work.\\nregister_commands(cli)\\nThis hook adds new commands to thellmCLI tool - for examplellm extra-command.\\nThis example plugin adds a newhello-worldcommand that prints “Hello world!”:\\nfrom llm import hookimpl\\nimport click\\n@hookimpl\\ndef register_commands(cli):\\n@cli.command(name=\"hello-world\")\\ndef hello_world():\\n\"Print hello world\"\\nclick.echo(\"Hello world!\")\\nThis new command will be added tollm --helpand can be run usingllm hello-world.\\n2.11. Plugins 81\\nLLM documentation, Release 0.26-31-g0bf655a\\nregister_models(register)\\nThis hook can be used to register one or more additional models.\\nimport llm\\n@llm.hookimpl\\ndef register_models(register):\\nregister(HelloWorld())\\nclass HelloWorld(llm.Model):\\nmodel_id = \"helloworld\"\\ndef execute(self, prompt, stream, response):\\nreturn [\"hello world\"]\\nIf your model includes an async version, you can register that too:\\nclass AsyncHelloWorld(llm.AsyncModel):\\nmodel_id = \"helloworld\"\\nasync def execute(self, prompt, stream, response):\\nreturn [\"hello world\"]\\n@llm.hookimpl\\ndef register_models(register):\\nregister(HelloWorld(), AsyncHelloWorld(), aliases=(\"hw\",))\\nThis demonstrates how to register a model with both sync and async versions, and how to specify an alias for that\\nmodel.\\nThe model plugin tutorialdescribes how to use this hook in detail. Asynchronous modelsare described here.\\nregister_embedding_models(register)\\nThis hook can be used to register one or more additional embedding models, as described inWriting plugins to add\\nnew embedding models.\\nimport llm\\n@llm.hookimpl\\ndef register_embedding_models(register):\\nregister(HelloWorld())\\nclass HelloWorld(llm.EmbeddingModel):\\nmodel_id = \"helloworld\"\\ndef embed_batch(self, items):\\nreturn [[1, 2, 3], [4, 5, 6]]\\n82 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nregister_tools(register)\\nThis hook can register one or more tool functions for use with LLM. Seethe tools documentationfor more details.\\nThis example registers two tools:upperand count_character_in_word.\\nimport llm\\ndef upper(text: str) -> str:\\n\"\"\"Convert text to uppercase.\"\"\"\\nreturn text.upper()\\ndef count_char(text: str, character: str) -> int:\\n\"\"\"Count the number of occurrences of a character in a word.\"\"\"\\nreturn text.count(character)\\n@llm.hookimpl\\ndef register_tools(register):\\nregister(upper)\\n# Here the name= argument is used to specify a different name for the tool:\\nregister(count_char, name=\"count_character_in_word\")\\nTools can also be implemented as classes, as described inToolbox classesin the Python API documentation.\\nYou can register classes like theMemory example from there by passing the class (not an instance of the class) to\\nregister():\\nimport llm\\nclass Memory(llm.Toolbox):\\n...\\n@llm.hookimpl\\ndef register_tools(register):\\nregister(Memory)\\nOnce installed, this tool can be used like so:\\nllm chat -T Memory\\nIf a tool name starts with a capital letter it is assumed to be a toolbox class, not a regular tool function.\\nHere’s an example session with the Memory tool:\\nChatting with gpt-4.1-mini\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt\\nType \\'!fragment <my_fragment> [<another_fragment> ...]\\' to insert one or more fragments\\n> Remember my name is Henry\\nTool call: Memory_set({\\'key\\': \\'user_name\\', \\'value\\': \\'Henry\\'})\\nnull\\nGot it, Henry! I\\'ll remember your name. How can I assist you today?\\n(continues on next page)\\n2.11. Plugins 83\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n> what keys are there?\\nTool call: Memory_keys({})\\n[\\n\"user_name\"\\n]\\nCurrently, there is one key stored: \"user_name\". Would you like to add or retrieve any␣\\n˓→information?\\n> read it\\nTool call: Memory_get({\\'key\\': \\'user_name\\'})\\nHenry\\nThe value stored under the key \"user_name\" is Henry. Is there anything else you\\'d like␣\\n˓→to do?\\n> add Barrett to it\\nTool call: Memory_append({\\'key\\': \\'user_name\\', \\'value\\': \\'Barrett\\'})\\nnull\\nI have added \"Barrett\" to the key \"user_name\". If you want, I can now show you the␣\\n˓→updated value.\\n> show value\\nTool call: Memory_get({\\'key\\': \\'user_name\\'})\\nHenry\\nBarrett\\nThe value stored under the key \"user_name\" is now:\\nHenry\\nBarrett\\nIs there anything else you would like to do?\\nregister_template_loaders(register)\\nPlugins can register newtemplate loadersusing theregister_template_loadershook.\\nTemplate loaders work with thellm -t prefix:name syntax. The prefix specifies the loader, then the registered\\nloaderfunctioniscalledwiththenameasanargument. Theloaderfunctionshouldreturnan llm.Template()object.\\nThis example plugin registersmy-prefixas a new template loader. Once installed it can be used like this:\\nllm -t my-prefix:my-template\\nHere’s the Python code:\\nimport llm\\n@llm.hookimpl\\ndef register_template_loaders(register):\\n(continues on next page)\\n84 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nregister(\"my-prefix\", my_template_loader)\\ndef my_template_loader(template_path: str) -> llm.Template:\\n\"\"\"\\nDocumentation for the template loader goes here. It will be displayed\\nwhen users run the \\'llm templates loaders\\' command.\\n\"\"\"\\ntry:\\n# Your logic to fetch the template content\\n# This is just an example:\\nprompt = \"This is a sample prompt for {}\".format(template_path)\\nsystem = \"You are an assistant specialized in {}\".format(template_path)\\n# Return a Template object with the required fields\\nreturn llm.Template(\\nname=template_path,\\nprompt=prompt,\\nsystem=system,\\n)\\nexcept Exception as e:\\n# Raise a ValueError with a clear message if the template cannot be found\\nraise ValueError(f\"Template \\'{template_path}\\' could not be loaded: {str(e)}\")\\nThe llm.Templateclass has the following constructor:\\nclass llm.Template(*,name: str,prompt: str | None = None, system: str | None = None,attachments: List[str] |\\nNone = None, attachment_types: List[AttachmentType] | None = None,model: str | None =\\nNone,defaults: Dict[str, Any] | None = None,options: Dict[str, Any] | None = None,\\nextract: bool | None = None, extract_last: bool | None = None,schema_object: dict | None\\n= None,fragments: List[str] | None = None,system_fragments: List[str] | None = None,\\ntools: List[str] | None = None, functions: str | None = None)\\nThe loader function should raise aValueErrorif the template cannot be found or loaded correctly, providing a clear\\nerror message.\\nNote thatfunctions: provided by templates using this plugin hook will not be made available, to avoid the risk of\\nplugin hooks that load templates from remote sources introducing arbitrary code execution vulnerabilities.\\nregister_fragment_loaders(register)\\nPlugins can register new fragment loaders using theregister_template_loaders hook. These can then be used\\nwith thellm -f prefix:argumentsyntax.\\nFragment loader plugins differ from template loader plugins in that you can stack more than one fragment loader call\\ntogether in the same prompt.\\nAfragmentloadercanreturnoneormorestringfragmentsorattachments,oramixtureofthetwo. Thefragmentswill\\nbe concatenated together into the prompt string, while any attachments will be added to the list of attachments to be\\nsent to the model.\\nThe prefixspecifies the loader. Theargumentwill be passed to that registered callback..\\nThecallbackworksinaverysimilarwaytotemplateloaders,butreturnseitherasingle llm.Fragment,alistof llm.\\nFragmentobjects, a singlellm.Attachment, or a list that can mixllm.Attachmentand llm.Fragmentobjects.\\n2.11. Plugins 85\\nLLM documentation, Release 0.26-31-g0bf655a\\nThellm.Fragmentconstructor takes a required string argument (the content of the fragment) and an optional second\\nsourceargument, which is a string that may be displayed as debug information. For files this is a path and for URLs\\nit is a URL. Your plugin can use anything you like for thesourcevalue.\\nSee the Python API documentation for attachmentsfor details of thellm.Attachmentclass.\\nHere is some example code:\\nimport llm\\n@llm.hookimpl\\ndef register_fragment_loaders(register):\\nregister(\"my-fragments\", my_fragment_loader)\\ndef my_fragment_loader(argument: str) -> llm.Fragment:\\n\"\"\"\\nDocumentation for the fragment loader goes here. It will be displayed\\nwhen users run the \\'llm fragments loaders\\' command.\\n\"\"\"\\ntry:\\nfragment = \"Fragment content for {}\".format(argument)\\nsource = \"my-fragments:{}\".format(argument)\\nreturn llm.Fragment(fragment, source)\\nexcept Exception as ex:\\n# Raise a ValueError with a clear message if the fragment cannot be loaded\\nraise ValueError(\\nf\"Fragment \\'my-fragments:{argument}\\' could not be loaded: {str(ex)}\"\\n)\\n# Or for the case where you want to return multiple fragments and attachments:\\ndef my_fragment_loader(argument: str) -> list[llm.Fragment]:\\n\"Docs go here.\"\\nreturn [\\nllm.Fragment(\"Fragment 1 content\", \"my-fragments:{argument}\"),\\nllm.Fragment(\"Fragment 2 content\", \"my-fragments:{argument}\"),\\nllm.Attachment(path=\"/path/to/image.png\"),\\n]\\nA plugin like this one can be called like so:\\nllm -f my-fragments:argument\\nIf multiple fragments are returned they will be used as if the user passed multiple-f Xarguments to the command.\\nMultiple fragments are particularly useful for things like plugins that return every file in a directory. If these were\\nconcatenated together by the plugin, a change to a single file would invalidate the de-duplicatino cache for that whole\\nfragment. Giving each file its own fragment means we can avoid storing multiple copies of that full collection if only\\na single file has changed.\\n86 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.11.4 Developing a model plugin\\nThis tutorial will walk you through developing a new plugin for LLM that adds support for a new Large Language\\nModel.\\nWe will be developing a plugin that implements a simple Markov chain to generate words based on an input string.\\nMarkovchainsarenottechnicallylargelanguagemodels,buttheyprovideausefulexercisefordemonstratinghowthe\\nLLM tool can be extended through plugins.\\nThe initial structure of the plugin\\nFirst create a new directory with the name of your plugin - it should be called something likellm-markov.\\nmkdir llm-markov\\ncd llm-markov\\nIn that directory create a file calledllm_markov.pycontaining this:\\nimport llm\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\ndef execute(self, prompt, stream, response, conversation):\\nreturn [\"hello world\"]\\nThe def register_models() function here is called by the plugin system (thanks to the@hookimpl decorator). It\\nuses theregister()function passed to it to register an instance of the new model.\\nThe Markov class implements the model. It sets amodel_id - an identifier that can be passed tollm -m in order to\\nidentify the model to be executed.\\nThe logic for executing the model goes in theexecute()method. We’ll extend this to do something more useful in a\\nlater step.\\nNext, create apyproject.tomlfile. This is necessary to tell LLM how to load your plugin:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThis is the simplest possible configuration. It defines a plugin name and provides an entry point forllmtelling it how\\nto load the plugin.\\nIf you are comfortable with Python virtual environments you can create one now for your project, activate it and run\\npip install llmbefore the next step.\\nIf you aren’t familiar with virtual environments, don’t worry: you can develop plugins without them. You’ll need to\\nhave LLM installed using Homebrew orpipxor one of the other installation options.\\n2.11. Plugins 87\\nLLM documentation, Release 0.26-31-g0bf655a\\nInstalling your plugin to try it out\\nHaving created a directory with apyproject.tomlfile and anllm_markov.pyfile, you can install your plugin into\\nLLM by running this from inside yourllm-markovdirectory:\\nllm install -e .\\nThe -estands for “editable” - it means you’ll be able to make further changes to thellm_markov.pyfile that will be\\nreflected without you having to reinstall the plugin.\\nThe .means the current directory. You can also install editable plugins by passing a path to their directory this:\\nllm install -e path/to/llm-markov\\nTo confirm that your plugin has installed correctly, run this command:\\nllm plugins\\nThe output should look like this:\\n[\\n{\\n\"name\": \"llm-markov\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\": \"0.1\"\\n},\\n{\\n\"name\": \"llm.default_plugins.openai_models\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n]\\n}\\n]\\nThis command lists default plugins that are included with LLM as well as new plugins that have been installed.\\nNow let’s try the plugin by running a prompt through it:\\nllm -m markov \"the cat sat on the mat\"\\nIt outputs:\\nhello world\\nNext, we’ll make it execute and return the results of a Markov chain.\\n88 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nBuilding the Markov chain\\nMarkov chains can be thought of as the simplest possible example of a generative language model. They work by\\nbuilding an index of words that have been seen following other words.\\nHere’s what that index looks like for the phrase “the cat sat on the mat”\\n{\\n\"the\": [\"cat\", \"mat\"],\\n\"cat\": [\"sat\"],\\n\"sat\": [\"on\"],\\n\"on\": [\"the\"]\\n}\\nHere’s a Python function that builds that data structure from a text input:\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\nWe can try that out by pasting it into the interactive Python interpreter and running this:\\n>>> transitions = build_markov_table(\"the cat sat on the mat\")\\n>>> transitions\\n{\\'the\\': [\\'cat\\', \\'mat\\'], \\'cat\\': [\\'sat\\'], \\'sat\\': [\\'on\\'], \\'on\\': [\\'the\\']}\\nExecuting the Markov chain\\nTo execute the model, we start with a word. We look at the options for words that might come next and pick one of\\nthose at random. Then we repeat that process until we have produced the desired number of output words.\\nSomewordsmightnothaveanyfollowingwordsfromourtrainingsentence. Forourimplementationwewillfallback\\non picking a random word from our collection.\\nWe will implement this as a Python generator, using the yield keyword to produce each token:\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nIf you aren’t familiar with generators, the above code could also be implemented like this - creating a Python list and\\nreturning it at the end of the function:\\n2.11. Plugins 89\\nLLM documentation, Release 0.26-31-g0bf655a\\ndef generate_list(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\noutput = []\\nfor i in range(length):\\noutput.append(next_word)\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nreturn output\\nYou can try out thegenerate()function like this:\\nlookup = build_markov_table(\"the cat sat on the mat\")\\nfor word in generate(transitions, 20):\\nprint(word)\\nOr you can generate a full string sentence with it like this:\\nsentence = \" \".join(generate(transitions, 20))\\nAdding that to the plugin\\nOur execute()method from earlier currently returns the list[\"hello world\"].\\nUpdate that to use our new Markov chain generator instead. Here’s the full text of the newllm_markov.pyfile:\\nimport llm\\nimport random\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\n(continues on next page)\\n90 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nfor word in generate(transitions, 20):\\nyield word + \\' \\'\\nTheexecute()methodcanaccessthetextpromptthattheuserprovidedusing prompt.prompt-promptisa Prompt\\nobject that might include other more advanced input details as well.\\nNow when you run this you should see the output of the Markov chain!\\nllm -m markov \"the cat sat on the mat\"\\nthe mat the cat sat on the cat sat on the mat cat sat on the mat cat sat on\\nUnderstanding execute()\\nThe full signature of theexecute()method is:\\ndef execute(self, prompt, stream, response, conversation):\\nThe prompt argument is aPrompt object that contains the text that the user provided, the system prompt and the\\nprovided options.\\nstreamis a boolean that says if the model is being run in streaming mode.\\nresponse is theResponse object that is being created by the model. This is provided so you can write additional\\ninformation toresponse.response_json, which may be logged to the database.\\nconversation is theConversation that the prompt is a part of - orNone if no conversation was provided. Some\\nmodels may useconversation.responses to access previous prompts and responses in the conversation and use\\nthem to construct a call to the LLM that includes previous context.\\nPrompts and responses are logged to the database\\nThe prompt and the response will be logged to a SQLite database automatically by LLM. You can see the single most\\nrecent addition to the logs using:\\nllm logs -n 1\\nThe output should look something like this:\\n[\\n{\\n\"id\": \"01h52s4yez2bd1qk2deq49wk8h\",\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {},\\n\"response\": \"on the cat sat on the cat sat on the mat cat sat on the cat sat on the␣\\n˓→cat \",\\n(continues on next page)\\n2.11. Plugins 91\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"response_json\": null,\\n\"conversation_id\": \"01h52s4yey7zc5rjmczy3ft75g\",\\n\"duration_ms\": 0,\\n\"datetime_utc\": \"2023-07-11T15:29:34.685868\",\\n\"conversation_name\": \"the cat sat on the mat\",\\n\"conversation_model\": \"markov\"\\n}\\n]\\nPlugins can log additional information to the database by assigning a dictionary to theresponse.response_json\\nproperty during theexecute()method.\\nHere’s how to include that fulltransitionstable in theresponse_jsonin the log:\\ndef execute(self, prompt, stream, response, conversation):\\ntext = self.prompt.prompt\\ntransitions = build_markov_table(text)\\nfor word in generate(transitions, 20):\\nyield word + \\' \\'\\nresponse.response_json = {\"transitions\": transitions}\\nNow when you run the logs command you’ll see that too:\\nllm logs -n 1\\n[\\n{\\n\"id\": 623,\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {},\\n\"response\": \"on the mat the cat sat on the cat sat on the mat sat on the cat sat on␣\\n˓→the \",\\n\"response_json\": {\\n\"transitions\": {\\n\"the\": [\\n\"cat\",\\n\"mat\"\\n],\\n\"cat\": [\\n\"sat\"\\n],\\n\"sat\": [\\n\"on\"\\n],\\n\"on\": [\\n\"the\"\\n]\\n}\\n},\\n\"reply_to_id\": null,\\n(continues on next page)\\n92 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"chat_id\": null,\\n\"duration_ms\": 0,\\n\"datetime_utc\": \"2023-07-06T01:34:45.376637\"\\n}\\n]\\nIn this particular case this isn’t a great idea here though: thetransitionstable is duplicate information, since it can\\nbe reproduced from the input data - and it can get really large for longer prompts.\\nAdding options\\nLLM models can take options. For large language models these can be things liketemperatureor top_k.\\nOptions are passed using the-o/--optioncommand line parameters, for example:\\nllm -m gpt4 \"ten pet pelican names\" -o temperature 1.5\\nWe’re going to add two options to our Markov chain model:\\n• length: Number of words to generate\\n• delay: a floating point number of Delay in between output token\\nThedelaytokenwillletussimulateastreaminglanguagemodel,wheretokenstaketimetogenerateandarereturned\\nby theexecute()function as they become ready.\\nOptions are defined using an inner class on the model, calledOptions. It should extend thellm.Optionsclass.\\nFirst, add this import to the top of yourllm_markov.pyfile:\\nfrom typing import Optional\\nThen add thisOptionsclass to your model:\\nclass Markov(Model):\\nmodel_id = \"markov\"\\nclass Options(llm.Options):\\nlength: Optional[int] = None\\ndelay: Optional[float] = None\\nLet’s add extra validation rules to our options. Length must be at least 2. Duration must be between 0 and 10.\\nThe Optionsclass uses Pydantic 2, which can support all sorts of advanced validation rules.\\nWe can also add inline documentation, which can then be displayed by thellm models --optionscommand.\\nAdd these imports to the top ofllm_markov.py:\\nfrom pydantic import field_validator, Field\\nWe can now add Pydantic field validators for our two new rules, plus inline documentation:\\nclass Options(llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\",\\ndefault=None\\n(continues on next page)\\n2.11. Plugins 93\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\",\\ndefault=None\\n)\\n@field_validator(\"length\")\\ndef validate_length(cls, length):\\nif length is None:\\nreturn None\\nif length < 2:\\nraise ValueError(\"length must be >= 2\")\\nreturn length\\n@field_validator(\"delay\")\\ndef validate_delay(cls, delay):\\nif delay is None:\\nreturn None\\nif not 0 <= delay <= 10:\\nraise ValueError(\"delay must be between 0 and 10\")\\nreturn delay\\nLets test our options validation:\\nllm -m markov \"the cat sat on the mat\" -o length -1\\nError: length\\nValue error, length must be >= 2\\nNext, we will modify ourexecute()method to handle those options. Add this to the beginning ofllm_markov.py:\\nimport time\\nThen replace theexecute()method with this one:\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or 20\\nfor word in generate(transitions, length):\\nyield word + \\' \\'\\nif prompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nAddcan_stream = Trueto the top of theMarkovmodel class, on the line below`model_id = “markov”. This tells\\nLLM that the model is able to stream content to the console.\\nThe fullllm_markov.pyfile should now look like this:\\nimport llm\\nimport random\\nimport time\\nfrom typing import Optional\\n(continues on next page)\\n94 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nfrom pydantic import field_validator, Field\\n@llm.hookimpl\\ndef register_models(register):\\nregister(Markov())\\ndef build_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nfor i in range(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturn transitions\\ndef generate(transitions, length, start_word=None):\\nall_words = list(transitions.keys())\\nnext_word = start_word or random.choice(all_words)\\nfor i in range(length):\\nyield next_word\\noptions = transitions.get(next_word) or all_words\\nnext_word = random.choice(options)\\nclass Markov(llm.Model):\\nmodel_id = \"markov\"\\ncan_stream = True\\nclass Options(llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\", default=None\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\", default=None\\n)\\n@field_validator(\"length\")\\ndef validate_length(cls, length):\\nif length is None:\\nreturn None\\nif length < 2:\\nraise ValueError(\"length must be >= 2\")\\nreturn length\\n@field_validator(\"delay\")\\ndef validate_delay(cls, delay):\\nif delay is None:\\nreturn None\\n(continues on next page)\\n2.11. Plugins 95\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif not 0 <= delay <= 10:\\nraise ValueError(\"delay must be between 0 and 10\")\\nreturn delay\\ndef execute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or 20\\nfor word in generate(transitions, length):\\nyield word + \" \"\\nif prompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nNow we can request a 20 word completion with a 0.1s delay between tokens like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1\\nLLMprovidesa --no-streamoptionuserscanusetoturnoffstreaming. UsingthatoptioncausesLLMtogatherthe\\nresponse from the stream and then return it to the console in one block. You can try that like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1 --no-stream\\nIn this case it will still delay for 2s total while it gathers the tokens, then output them all at once.\\nThat --no-stream option causes thestream argument passed toexecute() to be false. Yourexecute() method\\ncan then behave differently depending on whether it is streaming or not.\\nOptions are also logged to the database. You can see those here:\\nllm logs -n 1\\n[\\n{\\n\"id\": 636,\\n\"model\": \"markov\",\\n\"prompt\": \"the cat sat on the mat\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"options_json\": {\\n\"length\": 20,\\n\"delay\": 0.1\\n},\\n\"response\": \"the mat on the mat on the cat sat on the mat sat on the mat cat sat on␣\\n˓→the \",\\n\"response_json\": null,\\n\"reply_to_id\": null,\\n\"chat_id\": null,\\n\"duration_ms\": 2063,\\n\"datetime_utc\": \"2023-07-07T03:02:28.232970\"\\n}\\n]\\n96 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nDistributing your plugin\\nThere are many different options for distributing your new plugin so other people can try it out.\\nYoucancreateadownloadablewheelor .zipor .tar.gzfiles, orsharethepluginthroughGitHubGistsorreposito-\\nries.\\nYou can also publish your plugin to PyPI, the Python Package Index.\\nWheels and sdist packages\\nTheeasiestoptionistoproduceadistributablepackageistousethe buildcommand. First,installthe buildpackage\\nby running this:\\npython -m pip install build\\nThen runbuildin your plugin directory to create the packages:\\npython -m build\\nThis will create two files:dist/llm-markov-0.1.tar.gzand dist/llm-markov-0.1-py3-none-any.whl.\\nEither of these files can be used to install the plugin:\\nllm install dist/llm_markov-0.1-py3-none-any.whl\\nIf you host this file somewhere online other people will be able to install it usingpip install against the URL to\\nyour package:\\nllm install \\'https://.../llm_markov-0.1-py3-none-any.whl\\'\\nYou can run the following command at any time to uninstall your plugin, which is useful for testing out different\\ninstallation methods:\\nllm uninstall llm-markov -y\\nGitHub Gists\\nA neat quick option for distributing a simple plugin is to host it in a GitHub Gist. These are available for free with a\\nGitHub account, and can be public or private. Gists can contain multiple files but don’t support directory structures -\\nwhich is OK, because our plugin is just two files,pyproject.tomland llm_markov.py.\\nHere’s an example Gist I created for this tutorial:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d\\nYoucanturnaGistintoaninstallable .zipURLbyright-clickingonthe“DownloadZIP”buttonandselecting“Copy\\nLink”. Here’s that link for my example Gist:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\ncc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip\\nThe plugin can be installed using thellm installcommand like this:\\nllm install \\'https://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\n˓→cc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip\\'\\n2.11. Plugins 97\\nLLM documentation, Release 0.26-31-g0bf655a\\nGitHub repositories\\nThesametrickworksforregularGitHubrepositoriesaswell: the“DownloadZIP”buttoncanbefoundbyclickingthe\\ngreen “Code” button at the top of the repository. The URL which that provides can then be used to install the plugin\\nthat lives in that repository.\\nPublishing plugins to PyPI\\nThe Python Package Index (PyPI) is the official repository for Python packages. You can upload your plugin to PyPI\\nand reserve a name for it - once you have done that, anyone will be able to install your plugin usingllm install\\n<name>.\\nFollow these instructions to publish a package to PyPI. The short version:\\npython -m pip install twine\\npython -m twine upload dist/*\\nYou will need an account on PyPI, then you can enter your username and password - or create a token in the PyPI\\nsettings and use__token__as the username and the token as the password.\\nAdding metadata\\nBeforeuploadingapackagetoPyPIit’sagoodideatoadddocumentationandexpand pyproject.tomlwithadditional\\nmetadata.\\nCreate aREADME.md file in the root of your plugin directory with instructions about how to install, configure and use\\nyour plugin.\\nYou can then replacepyproject.tomlwith something like this:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\ndescription = \"Plugin for LLM adding a Markov chain generating model\"\\nreadme = \"README.md\"\\nauthors = [{name = \"Simon Willison\"}]\\nlicense = {text = \"Apache-2.0\"}\\nclassifiers = [\\n\"License :: OSI Approved :: Apache Software License\"\\n]\\ndependencies = [\\n\"llm\"\\n]\\nrequires-python = \">3.7\"\\n[project.urls]\\nHomepage = \"https://github.com/simonw/llm-markov\"\\nChangelog = \"https://github.com/simonw/llm-markov/releases\"\\nIssues = \"https://github.com/simonw/llm-markov/issues\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThis will pull in your README to be displayed as part of your project’s listing page on PyPI.\\n98 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nIt addsllmas a dependency, ensuring it will be installed if someone tries to install your plugin package without it.\\nIt adds some links to useful pages (you can drop theproject.urls section if those links are not useful for your\\nproject).\\nYoushoulddropa LICENSEfileintotheGitHubrepositoryforyourpackageaswell. IliketousetheApache2license\\nlike this.\\nWhat to do if it breaks\\nSometimes you may make a change to your plugin that causes it to break, preventingllm from starting. For example\\nyou may see an error like this one:\\n$ llm \\'hi\\'\\nTraceback (most recent call last):\\n...\\nFile llm-markov/llm_markov.py\", line 10\\nregister(Markov()):\\n^\\nSyntaxError: invalid syntax\\nYou may find that you are unable to uninstall the plugin usingllm uninstall llm-markov because the command\\nitself fails with the same error.\\nShould this happen, you can uninstall the plugin after first disabling it using theLLM_LOAD_PLUGINS environment\\nvariable like this:\\nLLM_LOAD_PLUGINS=\\'\\' llm uninstall llm-markov\\n2.11.5 Advanced model plugins\\nThe model plugin tutorialcovers the basics of developing a plugin that adds support for a new model. This document\\ncovers more advanced topics.\\nFeatures to consider for your model plugin include:\\n• Accepting API keysusing the standardmechanism that incorporatesllm keys set, environment variables and\\nsupport for passing an explicit key to the model.\\n• Including support forAsync modelsthat can be used with Python’sasynciolibrary.\\n• Support forstructured outputusing JSON schemas.\\n• Support fortools.\\n• Handlingattachments(images, audio and more) for multi-modal models.\\n• Trackingtoken usagefor models that charge by the token.\\n2.11. Plugins 99\\nLLM documentation, Release 0.26-31-g0bf655a\\nTip: lazily load expensive dependencies\\nIf your plugin depends on an expensive library such as PyTorch you should avoid importing that dependency (or a\\ndependency that uses that dependency) at the top level of your module. Expensive imports in plugins mean that even\\nsimple commands likellm --helpcan take a long time to run.\\nInstead, move those imports to inside the methods that need them. Here’s an example change to llm-sentence-\\ntransformers that shaved 1.8 seconds off the time it took to runllm --help!\\nModels that accept API keys\\nModels that call out to API providers such as OpenAI, Anthropic or Google Gemini usually require an API key.\\nLLM’s API key management mechanismis described here.\\nIfyourpluginrequiresanAPIkeyyoushouldsubclassthe llm.KeyModelclassinsteadofthe llm.Modelclass. Start\\nyour model definition like this:\\nimport llm\\nclass HostedModel(llm.KeyModel):\\nneeds_key = \"hosted\" # Required\\nkey_env_var = \"HOSTED_API_KEY\" # Optional\\nThistellsLLMthatyourmodelrequiresanAPIkey,whichmaybesavedinthekeyregistryunderthekeyname hosted\\nor might also be provided as theHOSTED_API_KEYenvironment variable.\\nThen when you define yourexecute()method it should take an extrakey=parameter like this:\\ndef execute(self, prompt, stream, response, conversation, key=None):\\n# key= here will be the API key to use\\nLLM will pass in the key from the environment variable, key registry or that has been passed to LLM as the--key\\ncommand-line option or themodel.prompt(..., key=)parameter.\\nAsync models\\nPlugins can optionally provide an asynchronous version of their model, suitable for use with Python asyncio. This is\\nparticularly useful for remote models accessible by an HTTP API.\\nThe async version of a model subclassesllm.AsyncModelinstead ofllm.Model. It must implement anasync def\\nexecute()async generator method instead ofdef execute().\\nThis example shows a subset of the OpenAI default plugin illustrating how this method might work:\\nfrom typing import AsyncGenerator\\nimport llm\\nclass MyAsyncModel(llm.AsyncModel):\\n# This can duplicate the model_id of the sync model:\\nmodel_id = \"my-model-id\"\\nasync def execute(\\nself, prompt, stream, response, conversation=None\\n) -> AsyncGenerator[str, None]:\\n(continues on next page)\\n100 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif stream:\\ncompletion = await client.chat.completions.create(\\nmodel=self.model_id,\\nmessages=messages,\\nstream=True,\\n)\\nasync for chunk in completion:\\nyield chunk.choices[0].delta.content\\nelse:\\ncompletion = await client.chat.completions.create(\\nmodel=self.model_name or self.model_id,\\nmessages=messages,\\nstream=False,\\n)\\nif completion.choices[0].message.content is not None:\\nyield completion.choices[0].message.content\\nIfyourmodeltakesanAPIkeyyoushouldinsteadsubclass llm.AsyncKeyModelandhavea key=parameteronyour\\n.execute()method:\\nclass MyAsyncModel(llm.AsyncKeyModel):\\n...\\nasync def execute(\\nself, prompt, stream, response, conversation=None, key=None\\n) -> AsyncGenerator[str, None]:\\nThisasyncmodelinstanceshouldthenbepassedtothe register()methodinthe register_models()pluginhook:\\n@hookimpl\\ndef register_models(register):\\nregister(\\nMyModel(), MyAsyncModel(), aliases=(\"my-model-aliases\",)\\n)\\nSupporting schemas\\nIf your model supportsstructured outputagainst a defined JSON schema you can implement support by first adding\\nsupports_schema = Trueto the class:\\nclass MyModel(llm.KeyModel):\\n...\\nsupport_schema = True\\nAnd then adding code to your.execute() method that checks forprompt.schema and, if it is present, uses that to\\nprompt the model.\\nprompt.schemawillalwaysbeaPythondictionaryrepresentingaJSONschema,eveniftheuserpassedinaPydantic\\nmodel class.\\nCheck the llm-gemini and llm-anthropic plugins for example of this pattern in action.\\n2.11. Plugins 101\\nLLM documentation, Release 0.26-31-g0bf655a\\nSupporting tools\\nAddingtools supportinvolves several steps:\\n1. Addsupports_tools = Trueto your model class.\\n2. If prompt.toolsis populated, turn that list ofllm.Toolobjects into the correct format for your model.\\n3. Look out for requests to call tools in the responses from your model. Callresponse.add_tool_call(llm.\\nToolCall(...))foreachofthose. Thisshouldworkforstreamingandnon-streamingandasyncandnon-async\\ncases.\\n4. Ifyourprompthasa prompt.tool_resultslist,passtheinformationfromthose llm.ToolResultobjectsto\\nyour model.\\n5. Include prompt.tools and prompt.tool_results and tool calls from response.\\ntool_calls_or_raise()in the conversation history constructed by your plugin.\\n6. Make sure your code is OK with prompts that do not haveprompt.prompt set to a value, since they may be\\ncarrying exclusively the results of a tool call.\\nThis commit to llm-gemini implementing tools helps demonstrate what this looks like for a real plugin.\\nHere are the relevant dataclasses:\\nclass llm.Tool(name: str,description: Optional[str] = None, input_schema: Dict = <factory>,implementation:\\nOptional[Callable] = None,plugin: Optional[str] = None)\\nclass llm.ToolCall(name: str, arguments: dict,tool_call_id: str | None = None)\\nclass llm.ToolResult(name: str,output: str,attachments: List[llm.models.Attachment] = <factory>,\\ntool_call_id: Optional[str] = None, instance: Optional[llm.models.Toolbox] = None,\\nexception: Optional[Exception] = None)\\nAttachments for multi-modal models\\nModels such as GPT-4o, Claude 3.5 Sonnet and Google’s Gemini 1.5 are multi-modal: they accept input in the form\\nof images and maybe even audio, video and other formats.\\nLLM calls theseattachments. Models can specify the types of attachments they accept and then implement special\\ncode in the.execute()method to handle them.\\nSee the Python attachments documentationfor details on using attachments in the Python API.\\nSpecifying attachment types\\nA Modelsubclass can list the types of attachments it accepts by defining aattachment_typesclass attribute:\\nclass NewModel(llm.Model):\\nmodel_id = \"new-model\"\\nattachment_types = {\\n\"image/png\",\\n\"image/jpeg\",\\n\"image/webp\",\\n\"image/gif\",\\n}\\n102 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nThese content types are detected when an attachment is passed to LLM usingllm -a filename, or can be specified\\nby the user using the--attachment-type filename image/pngoption.\\nNote: MP3 files will have their attachment type detected asaudio/mpeg, notaudio/mp3.\\nLLMwillusethe attachment_typesattributetovalidatethatprovidedattachmentsshouldbeacceptedbeforepassing\\nthem to the model.\\nHandling attachments\\nThe prompt object passed to the execute() method will have an attachments attribute containing a list of\\nAttachmentobjects provided by the user.\\nAn Attachmentinstance has the following properties:\\n• url (str): The URL of the attachment, if it was provided as a URL\\n• path (str): The resolved file path of the attachment, if it was provided as a file\\n• type (str): The content type of the attachment, if it was provided\\n• content (bytes): The binary content of the attachment, if it was provided\\nGenerally only one ofurl,pathor contentwill be set.\\nYou should usually access the type and the content through one of these methods:\\n• attachment.resolve_type() -> str: Returns thetype if it is available, otherwise attempts to guess the\\ntype by looking at the first few bytes of content\\n• attachment.content_bytes() -> bytes: Returnsthebinarycontent,whichitmayneedtoreadfromafile\\nor fetch from a URL\\n• attachment.base64_content() -> str: Returns that content as a base64-encoded string\\nA id() method returns a database ID for this content, which is either a SHA256 hash of the binary content or, in the\\ncase of attachments hosted at an external URL, a hash of{\"url\": url} instead. This is an implementation detail\\nwhich you should not need to access directly.\\nNote that it’s possible for a prompt with an attachments to not include a text prompt at all, in which caseprompt.\\npromptwill beNone.\\nHere’s how the OpenAI plugin handles attachments, including the case where noprompt.promptwas provided:\\nif not prompt.attachments:\\nmessages.append({\"role\": \"user\", \"content\": prompt.prompt})\\nelse:\\nattachment_message = []\\nif prompt.prompt:\\nattachment_message.append({\"type\": \"text\", \"text\": prompt.prompt})\\nfor attachment in prompt.attachments:\\nattachment_message.append(_attachment(attachment))\\nmessages.append({\"role\": \"user\", \"content\": attachment_message})\\n# And the code for creating the attachment message\\ndef _attachment(attachment):\\nurl = attachment.url\\nbase64_content = \"\"\\nif not url or attachment.resolve_type().startswith(\"audio/\"):\\n(continues on next page)\\n2.11. Plugins 103\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nbase64_content = attachment.base64_content()\\nurl = f\"data:{attachment.resolve_type()};base64,{base64_content}\"\\nif attachment.resolve_type().startswith(\"image/\"):\\nreturn {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\\nelse:\\nformat_ = \"wav\" if attachment.resolve_type() == \"audio/wav\" else \"mp3\"\\nreturn {\\n\"type\": \"input_audio\",\\n\"input_audio\": {\\n\"data\": base64_content,\\n\"format\": format_,\\n},\\n}\\nAsyoucansee,ituses attachment.urlifthatisavailableandotherwisefallsbacktousingthe base64_content()\\nmethod to embed the image directly in the JSON sent to the API. For the OpenAI API audio attachments are always\\nincluded as base64-encoded strings.\\nAttachments from previous conversations\\nModels that implement the ability to continue a conversation can reconstruct the previous message JSON using the\\nresponse.attachmentsattribute.\\nHere’s how the OpenAI plugin does that:\\nfor prev_response in conversation.responses:\\nif prev_response.attachments:\\nattachment_message = []\\nif prev_response.prompt.prompt:\\nattachment_message.append(\\n{\"type\": \"text\", \"text\": prev_response.prompt.prompt}\\n)\\nfor attachment in prev_response.attachments:\\nattachment_message.append(_attachment(attachment))\\nmessages.append({\"role\": \"user\", \"content\": attachment_message})\\nelse:\\nmessages.append(\\n{\"role\": \"user\", \"content\": prev_response.prompt.prompt}\\n)\\nmessages.append({\"role\": \"assistant\", \"content\": prev_response.text_or_raise()})\\nThe response.text_or_raise() method used there will return the text from the response or raise aValueError\\nexception if the response is anAsyncResponseinstance that has not yet been fully resolved.\\nThis is a slightly weird hack to work around the common need to share logic for building up themessageslist across\\nboth sync and async models.\\n104 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nTracking token usage\\nModelsthatchargebythetokenshouldtrackthenumberoftokensusedbyeachprompt. The response.set_usage()\\nmethod can be used to record the number of tokens used by a response - these will then be made available through the\\nPython API and logged to the SQLite database for command-line users.\\nresponsehere is the response object that is passed to.execute()as an argument.\\nCall response.set_usage() at the end of your.execute() method. It accepts keyword arguments input=,\\noutput= and details= - all three are optional.input and output should be integers, anddetails should be a\\ndictionary that provides additional information beyond the input and output token counts.\\nThis example logs 15 input tokens, 340 output tokens and notes that 37 tokens were cached:\\nresponse.set_usage(input=15, output=340, details={\"cached\": 37})\\nTracking resolved model names\\nIn some cases the model ID that the user requested may not be the exact model that is executed. Many providers have\\na model-latestalias which may execute different models over time.\\nIfthoseAPIsreturnthe realmodelIDthatwasused,yourplugincanrecordthatinthe resources.resolved_model\\ncolumn in the logs by calling this method and passing the string representing the resolved, final model ID:\\nresponse.set_resolved_model(resolved_model_id)\\nThis string will be recorded in the database and shown in the output ofllm logsand llm logs --json.\\nLLM_RAISE_ERRORS\\nWhile working on a plugin it can be useful to request that errors are raised instead of being caught and logged, so you\\ncan access them from the Python debugger.\\nSet theLLM_RAISE_ERRORSenvironment variable to enable this behavior, then runllmlike this:\\nLLM_RAISE_ERRORS=1 python -i -m llm ...\\nThe-ioptionmeans Pythonwill dropinto aninteractive shellif anerror occurs. You canthen opena debuggerat the\\nmost recent error using:\\nimport pdb; pdb.pm()\\n2.11.6 Utility functions for plugins\\nLLM provides some utility functions that may be useful to plugins.\\n2.11. Plugins 105\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm.get_key()\\nThismethodcanbeusedtolookupsecretsthatusershavestoredusingthe llmkeysset command. Ifyourpluginneeds\\nto access an API key or other secret this can be a convenient way to provide that.\\nThis returns either a string containing the key orNoneif the key could not be resolved.\\nUse thealias=\"name\"option to retrieve the key set with that alias:\\ngithub_key = llm.get_key(alias=\"github\")\\nYoucanalsoadd env=\"ENV_VAR\"tofallbacktolookinginthatenvironmentvariableifthekeyhasnotbeenconfigured:\\ngithub_key = llm.get_key(alias=\"github\", env=\"GITHUB_TOKEN\")\\nIn some cases you may allow users to provide a key as input, where they could input either the key itself or specify an\\nalias to lookup inkeys.json. Use theinput=parameter for that:\\ngithub_key = llm.get_key(input=input_from_user, alias=\"github\", env=\"GITHUB_TOKEN\")\\nAn previous version of function used positional arguments in a confusing order. These are still supported but the new\\nkeyword arguments are recommended as a better way to usellm.get_key()going forward.\\nllm.user_dir()\\nLLM stores various pieces of logging and configuration data in a directory on the user’s machine.\\nOn macOS this directory is~/Library/Application Support/io.datasette.llm, but this will differ on other\\noperating systems.\\nThe llm.user_dir()function returns the path to this directory as apathlib.Pathobject, after creating that direc-\\ntory if it does not yet exist.\\nPlugins can use this to store their own data in a subdirectory of this directory.\\nimport llm\\nuser_dir = llm.user_dir()\\nplugin_dir = data_path = user_dir / \"my-plugin\"\\nplugin_dir.mkdir(exist_ok=True)\\ndata_path = plugin_dir / \"plugin-data.db\"\\nllm.ModelError\\nIf your model encounters an error that should be reported to the user you can raise this exception. For example:\\nimport llm\\nraise ModelError(\"MPT model not installed - try running \\'llm mpt30b download\\'\")\\nThis will be caught by the CLI layer and displayed to the user as an error message.\\n106 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nResponse.fake()\\nWhen writing tests for a model it can be useful to generate fake response objects, for example in this test from llm-\\nmpt30b:\\ndef test_build_prompt_conversation():\\nmodel = llm.get_model(\"mpt\")\\nconversation = model.conversation()\\nconversation.responses = [\\nllm.Response.fake(model, \"prompt 1\", \"system 1\", \"response 1\"),\\nllm.Response.fake(model, \"prompt 2\", None, \"response 2\"),\\nllm.Response.fake(model, \"prompt 3\", None, \"response 3\"),\\n]\\nlines = model.build_prompt(llm.Prompt(\"prompt 4\", model), conversation)\\nassert lines == [\\n\"<|im_start|>system\\\\system 1<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 1<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 1<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 2<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 2<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 3<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\nresponse 3<|im_end|>\\\\n\",\\n\"<|im_start|>user\\\\nprompt 4<|im_end|>\\\\n\",\\n\"<|im_start|>assistant\\\\n\",\\n]\\nThe signature ofllm.Response.fake()is:\\ndef fake(cls, model: Model, prompt: str, system: str, response: str):\\n2.12 Python API\\nLLM provides a Python API for executing prompts, in addition to the command-line interface.\\nUnderstanding this API is also important for writingPlugins.\\n2.12.1 Basic prompt execution\\nTo run a prompt against thegpt-4o-minimodel, run this:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\n# key= is optional, you can configure the key in other ways\\nresponse = model.prompt(\\n\"Five surprising names for a pet pelican\",\\nkey=\"sk-...\"\\n)\\nprint(response.text())\\nNote that the prompt will not be evaluated until you call thatresponse.text()method - a form of lazy loading.\\nIf you inspect the response before it has been evaluated it will look like this:\\n2.12. Python API 107\\nLLM documentation, Release 0.26-31-g0bf655a\\n<Response prompt=\\'Your prompt\\' text=\\'... not yet done ...\\'>\\nThe llm.get_model() function accepts model IDs or aliases. You can also omit it to use the currently configured\\ndefault model, which isgpt-4o-miniif you have not changed the default.\\nIn this example the key is set by Python code. You can also provide the key using theOPENAI_API_KEYenvironment\\nvariable, or use thellm keys set openaicommand to store it in akeys.jsonfile, seeAPI key management.\\nThe __str__()method ofresponsealso returns the text of the response, so you can do this instead:\\nprint(llm.get_model().prompt(\"Five surprising names for a pet pelican\"))\\nYou can run this command to see a list of available models and their aliases:\\nllm models\\nIf you have set aOPENAI_API_KEYenvironment variable you can omit themodel.key = line.\\nCalling llm.get_model()with an invalid model ID will raise allm.UnknownModelErrorexception.\\nSystem prompts\\nFor models that accept a system prompt, pass it assystem=\"...\":\\nresponse = model.prompt(\\n\"Five surprising names for a pet pelican\",\\nsystem=\"Answer like GlaDOS\"\\n)\\nAttachments\\nModels that accept multi-modal input (images, audio, video etc) can be passed attachments using theattachments=\\nkeyword argument. This accepts a list ofllm.Attachment()instances.\\nThis example shows two attachments - one from a file path and one from a URL:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\\n\"Describe these images\",\\nattachments=[\\nllm.Attachment(path=\"pelican.jpg\"),\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\"),\\n]\\n)\\nUse llm.Attachment(content=b\"binary image content here\")to pass binary content directly.\\nYou can check which attachment types (if any) a model supports using themodel.attachment_typesset:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nprint(model.attachment_types)\\n# {\\'image/gif\\', \\'image/png\\', \\'image/jpeg\\', \\'image/webp\\'}\\n(continues on next page)\\n108 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nif \"image/jpeg\" in model.attachment_types:\\n# Use a JPEG attachment here\\n...\\nTools\\nToolsare functions that can be executed by the model as part of a chain of responses.\\nYou can define tools in Python code - with a docstring to describe what they do - and then pass them to themodel.\\nprompt() method using thetools= keyword argument. If the model decides to request a tool call theresponse.\\ntool_calls()method show what the model wants to execute:\\nimport llm\\ndef upper(text: str) -> str:\\n\"\"\"Convert text to uppercase.\"\"\"\\nreturn text.upper()\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.prompt(\"Convert panda to upper\", tools=[upper])\\ntool_calls = response.tool_calls()\\n# [ToolCall(name=\\'upper\\', arguments={\\'text\\': \\'panda\\'}, tool_call_id=\\'...\\')]\\nYou can callresponse.execute_tool_calls()to execute those calls and get back the results:\\ntool_results = response.execute_tool_calls()\\n# [ToolResult(name=\\'upper\\', output=\\'PANDA\\', tool_call_id=\\'...\\')]\\nYoucanusethe model.chain()topasstheresultsoftoolcallsbacktothemodelautomaticallyassubsequentprompts:\\nchain_response = model.chain(\\n\"Convert panda to upper\",\\ntools=[upper],\\n)\\nprint(chain_response.text())\\n# The word \"panda\" converted to uppercase is \"PANDA\".\\nYou can also loop through themodel.chain()response to get a stream of tokens, like this:\\nfor chunk in model.chain(\\n\"Convert panda to upper\",\\ntools=[upper],\\n):\\nprint(chunk, end=\"\", flush=True)\\nThis will stream each of the chain of responses in turn as they are generated.\\nYoucanaccesstheindividualresponsesthatmakeupthechainusing chain.responses(). Thiscanbeiteratedover\\nas the chain executes like this:\\nchain = model.chain(\\n\"Convert panda to upper\",\\n(continues on next page)\\n2.12. Python API 109\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntools=[upper],\\n)\\nfor response in chain.responses():\\nprint(response.prompt)\\nfor chunk in response:\\nprint(chunk, end=\"\", flush=True)\\nTool debugging hooks\\nPass a function to thebefore_call= parameter ofmodel.chain() to have that called before every tool call is exe-\\ncuted. You can raisellm.CancelToolCall()to cancel that tool call.\\nThe method signature isdef before_call(tool: Optional[llm.Tool], tool_call: llm.ToolCall) -\\nthatfirst toolargumentcanbe Noneifthemodelrequestsatoolbeexecutedthathasnotbeenprovidedinthe tools=\\nlist.\\nHere’s an example:\\nimport llm\\nfrom typing import Optional\\ndef upper(text: str) -> str:\\n\"Convert text to uppercase.\"\\nreturn text.upper()\\ndef before_call(tool: Optional[llm.Tool], tool_call: llm.ToolCall):\\nprint(f\"About to call tool {tool.name} with arguments {tool_call.arguments}\")\\nif tool.name == \"upper\" and \"bad\" in repr(tool_call.arguments):\\nraise llm.CancelToolCall(\"Not allowed to call upper on text containing \\'bad\\'\")\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.chain(\\n\"Convert panda to upper and badger to upper\",\\ntools=[upper],\\nbefore_call=before_call,\\n)\\nprint(response.text())\\nIf you raisellm.CancelToolCall in thebefore_call function the model will be informed that the tool call was\\ncancelled.\\nThe after_call= parameter can be used to run a logging function after each tool call has been executed.\\nThe method signature isdef after_call(tool: llm.Tool, tool_call: llm.ToolCall, tool_result:\\nllm.ToolResult). This continues the previous example:\\ndef after_call(tool: llm.Tool, tool_call: llm.ToolCall, tool_result: llm.ToolResult):\\nprint(f\"Tool {tool.name} called with arguments {tool_call.arguments} returned {tool_\\n˓→result.output}\")\\nresponse = model.chain(\\n\"Convert panda to upper and badger to upper\",\\ntools=[upper],\\n(continues on next page)\\n110 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nafter_call=after_call,\\n)\\nprint(response.text())\\nTools can return attachments\\nToolscanreturn attachmentsinadditiontoreturningtext. Attachmentsthatarereturnedfromatoolcallwillbepassed\\nto the model as attachments for the next prompt in the chain.\\nTo return one or more attachments, return allm.ToolOutput instance from your tool function. This can have an\\noutput=string and anattachments=list ofllm.Attachmentinstances.\\nHere’s an example:\\nimport llm\\ndef generate_image(prompt: str) -> llm.ToolOutput:\\n\"\"\"Generate an image based on the prompt.\"\"\"\\nimage_content = generate_image_from_prompt(prompt)\\nreturn llm.ToolOutput(\\noutput=\"Image generated successfully\",\\nattachments=[llm.Attachment(\\ncontent=image_content,\\nmimetype=\"image/png\"\\n)],\\n)\\nToolbox classes\\nFunctions are useful for simple tools, but some tools may have more advanced needs. You can also define tools as a\\nclass (known as a “toolbox”), which provides the following advantages:\\n• Toolbox tools can bundle multiple tools together\\n• Toolbox tools can be configured, e.g. to give filesystem tools access to a specific directory\\n• Toolbox instances can persist shared state in between tool invocations\\nToolboxes are classes that extendllm.Toolbox. Any methods that do not begin with an underscore will be exposed\\nas tool functions.\\nThis example sets up key/value memory storage that can be used by the model:\\nimport llm\\nclass Memory(llm.Toolbox):\\n_memory = None\\ndef _get_memory(self):\\nif self._memory is None:\\nself._memory = {}\\nreturn self._memory\\n(continues on next page)\\n2.12. Python API 111\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ndef set(self, key: str, value: str):\\n\"Set something as a key\"\\nself._get_memory()[key] = value\\ndef get(self, key: str):\\n\"Get something from a key\"\\nreturn self._get_memory().get(key) or \"\"\\ndef append(self, key: str, value: str):\\n\"Append something as a key\"\\nmemory = self._get_memory()\\nmemory[key] = (memory.get(key) or \"\") + \"\\\\n\" + value\\ndef keys(self):\\n\"Return a list of keys\"\\nreturn list(self._get_memory().keys())\\nYou can then use that from Python like this:\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nmemory = Memory()\\nconversation = model.conversation(tools=[memory])\\nprint(conversation.chain(\"Set name to Simon\", after_call=print).text())\\nprint(memory._memory)\\n# Should show {\\'name\\': \\'Simon\\'}\\nprint(conversation.chain(\"Set name to Penguin\", after_call=print).text())\\n# Now it should be {\\'name\\': \\'Penguin\\'}\\nprint(conversation.chain(\"Print current name\", after_call=print).text())\\nSee theregister_tools() plugin hook documentationfor an example of this tool in action as a CLI plugin.\\nSchemas\\nAs withthe CLI toolsome models support passing a JSON schema should be used for the resulting response.\\nYoucanpassthistothe prompt(schema=)parameteraseitheraPythondictionaryoraPydantic BaseModelsubclass:\\nimport llm, json\\nfrom pydantic import BaseModel\\nclass Dog(BaseModel):\\nname: str\\nage: int\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"Describe a nice dog\", schema=Dog)\\ndog = json.loads(response.text())\\nprint(dog)\\n# {\"name\":\"Buddy\",\"age\":3}\\n112 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nYou can also pass a schema directly, like this:\\nresponse = model.prompt(\"Describe a nice dog\", schema={\\n\"properties\": {\\n\"name\": {\"title\": \"Name\", \"type\": \"string\"},\\n\"age\": {\"title\": \"Age\", \"type\": \"integer\"},\\n},\\n\"required\": [\"name\", \"age\"],\\n\"title\": \"Dog\",\\n\"type\": \"object\",\\n})\\nYou can also use LLM’salternative schema syntaxvia thellm.schema_dsl(schema_dsl)function. This provides\\na quick way to construct a JSON schema for simple cases:\\nprint(model.prompt(\\n\"Describe a nice dog with a surprising name\",\\nschema=llm.schema_dsl(\"name, age int, bio\")\\n))\\nPassmulti=Trueto generate a schema that returns multiple items matching that specification:\\nprint(model.prompt(\\n\"Describe 3 nice dogs with surprising names\",\\nschema=llm.schema_dsl(\"name, age int, bio\", multi=True)\\n))\\nFragments\\nThe fragment systemfrom the CLI tool can also be accessed from the Python API, by passingfragments= and/or\\nsystem_fragments=lists of strings to theprompt()method:\\nresponse = model.prompt(\\n\"What do these documents say about dogs?\",\\nfragments=[\\nopen(\"dogs1.txt\").read(),\\nopen(\"dogs2.txt\").read(),\\n],\\nsystem_fragments=[\\n\"You answer questions like Snoopy\",\\n]\\n)\\nThis mechanism has limited utility in Python, as you can also assemble the contents of these strings together into the\\nprompt=and system=strings directly.\\nFragments become more interesting if you are working with LLM’s mechanisms for storing prompts to a SQLite\\ndatabase, which are not yet part of the stable, documented Python API.\\nSomemodelpluginsmayincludefeaturesthattakeadvantageoffragments,forexamplellm-anthropicaimstousethem\\nas part of a mechanism that taps into Claude’s prompt caching system.\\n2.12. Python API 113\\nLLM documentation, Release 0.26-31-g0bf655a\\nModel options\\nFor models that support options (view those withllm models --options) you can pass options as keyword argu-\\nments to the.prompt()method:\\nmodel = llm.get_model()\\nprint(model.prompt(\"Names for otters\", temperature=0.2))\\nPassing an API key\\nModels that accept API keys should take an additionalkey=parameter to theirmodel.prompt()method:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nprint(model.prompt(\"Names for beavers\", key=\"sk-...\"))\\nIf you don’t provide this argument LLM will attempt to find it from an environment variable (OPENAI_API_KEY for\\nOpenAI, others for different plugins) or from keys that have been saved using thellm keys setcommand.\\nSomemodelpluginsmaynotyethavebeenupgradedtohandlethe key=parameter,inwhichcaseyouwillneedtouse\\none of the other mechanisms.\\nModels from plugins\\nAnymodelsyouhaveinstalledaspluginswillalsobeavailablethroughthismechanism,forexampletouseAnthropic’s\\nClaude 3.5 Sonnet model with llm-anthropic:\\npip install llm-anthropic\\nThen in your Python code:\\nimport llm\\nmodel = llm.get_model(\"claude-3.5-sonnet\")\\n# Use this if you have not set the key using \\'llm keys set claude\\':\\nmodel.key = \\'YOUR_API_KEY_HERE\\'\\nresponse = model.prompt(\"Five surprising names for a pet pelican\")\\nprint(response.text())\\nSome models do not use API keys at all.\\nAccessing the underlying JSON\\nMost model plugins also make a JSON version of the prompt response available. The structure of this will differ\\nbetween model plugins, so building against this is likely to result in code that only works with that specific model\\nprovider.\\nYou can access this JSON data as a Python dictionary using theresponse.json()method:\\nimport llm\\nfrom pprint import pprint\\nmodel = llm.get_model(\"gpt-4o-mini\")\\n(continues on next page)\\n114 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nresponse = model.prompt(\"3 names for an otter\")\\njson_data = response.json()\\npprint(json_data)\\nHere’s that example output from GPT-4o mini:\\n{\\'content\\': \\'Sure! Here are three fun names for an otter:\\\\n\\'\\n\\'\\\\n\\'\\n\\'1. **Splash**\\\\n\\'\\n\\'2. **Bubbles**\\\\n\\'\\n\\'3. **Otto** \\\\n\\'\\n\\'\\\\n\\'\\n\\'Feel free to mix and match or use these as inspiration!\\',\\n\\'created\\': 1739291215,\\n\\'finish_reason\\': \\'stop\\',\\n\\'id\\': \\'chatcmpl-AznO31yxgBjZ4zrzBOwJvHEWgdTaf\\',\\n\\'model\\': \\'gpt-4o-mini-2024-07-18\\',\\n\\'object\\': \\'chat.completion.chunk\\',\\n\\'usage\\': {\\'completion_tokens\\': 43,\\n\\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0,\\n\\'audio_tokens\\': 0,\\n\\'reasoning_tokens\\': 0,\\n\\'rejected_prediction_tokens\\': 0},\\n\\'prompt_tokens\\': 13,\\n\\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0},\\n\\'total_tokens\\': 56}}\\nToken usage\\nMany models can return a count of the number of tokens used while executing the prompt.\\nThe response.usage()method provides an abstraction over this:\\npprint(response.usage())\\nExample output:\\nUsage(input=5,\\noutput=2,\\ndetails={\\'candidatesTokensDetails\\': [{\\'modality\\': \\'TEXT\\',\\n\\'tokenCount\\': 2}],\\n\\'promptTokensDetails\\': [{\\'modality\\': \\'TEXT\\', \\'tokenCount\\': 5}]})\\nThe .inputand .outputproperties are integers representing the number of input and output tokens. The.details\\nproperty may be a dictionary with additional custom values that vary by model.\\n2.12. Python API 115\\nLLM documentation, Release 0.26-31-g0bf655a\\nStreaming responses\\nFor models that support it you can stream responses as they are generated, like this:\\nresponse = model.prompt(\"Five diabolical names for a pet goat\")\\nfor chunk in response:\\nprint(chunk, end=\"\")\\nTheresponse.text()method described earlier does this for you - it runs through the iterator and gathers the results\\ninto a string.\\nIf a response has been evaluated,response.text()will continue to return the same string.\\n2.12.2 Async models\\nSome plugins provide async versions of their supported models, suitable for use with Python asyncio.\\nTo use an async model, use thellm.get_async_model()function instead ofllm.get_model():\\nimport llm\\nmodel = llm.get_async_model(\"gpt-4o\")\\nYou can then run a prompt usingawait model.prompt(...):\\nprint(await model.prompt(\\n\"Five surprising names for a pet pelican\"\\n).text())\\nOr useasync for chunk in ...to stream the response as it is generated:\\nasync for chunk in model.prompt(\\n\"Five surprising names for a pet pelican\"\\n):\\nprint(chunk, end=\"\", flush=True)\\nThis await model.prompt() method takes the same arguments as the synchronousmodel.prompt() method, for\\noptions and attachments andkey=and suchlike.\\nTool functions can be sync or async\\nToolfunctions canbebothsynchronousorasynchronous. Thelatteraredefinedusing async def tool_name(...).\\nEither kind of function can be passed to thetools=[...]parameter.\\nIf anasync def function is used in a synchronous context LLM will automatically execute it in a thread pool using\\nasyncio.run(). This means the following will work even in non-asynchronous Python scripts:\\nasync def hello(name: str) -> str:\\n\"Say hello to name\"\\nreturn \"Hello there \" + name\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nchain_response = model.chain(\\n\"Say hello to Percival\", tools=[hello]\\n)\\nprint(chain_response.text())\\n116 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nThis also works forasync def methods ofllm.Toolboxsubclasses.\\nTool use for async models\\nTool use is also supported for async models, using either synchronous or asynchronous tool functions. Synchronous\\nfunctionswillblocktheeventloopsoonlyusethoseinasynchronouscontextifyouarecertaintheyareextremelyfast.\\nThe response.execute_tool_calls() and chain_response.text() and chain_response.responses()\\nmethods must all be awaited when run against asynchronous models:\\nimport llm\\nmodel = llm.get_async_model(\"gpt-4.1\")\\ndef upper(string):\\n\"Converts string to uppercase\"\\nreturn string.upper()\\nchain = model.chain(\\n\"Convert panda to uppercase then pelican to uppercase\",\\ntools=[upper],\\nafter_call=print\\n)\\nprint(await chain.text())\\nTo iterate over the chained response output as it arrives useasync for:\\nasync for chunk in model.chain(\\n\"Convert panda to uppercase then pelican to uppercase\",\\ntools=[upper]\\n):\\nprint(chunk, end=\"\", flush=True)\\nThe before_calland after_callhooks can be async functions when used with async models.\\n2.12.3 Conversations\\nLLM supportsconversations, where you ask follow-up questions of a model as part of an ongoing conversation.\\nTo start a new conversation, use themodel.conversation()method:\\nmodel = llm.get_model()\\nconversation = model.conversation()\\nYou can then use theconversation.prompt()method to execute prompts against this conversation:\\nresponse = conversation.prompt(\"Five fun facts about pelicans\")\\nprint(response.text())\\nThis works exactly the same as themodel.prompt() method, except that the conversation will be maintained across\\nmultiple prompts. So if you run this next:\\nresponse2 = conversation.prompt(\"Now do skunks\")\\nprint(response2.text())\\n2.12. Python API 117\\nLLM documentation, Release 0.26-31-g0bf655a\\nYou will get back five fun facts about skunks.\\nThe conversation.prompt()method supports attachments as well:\\nresponse = conversation.prompt(\\n\"Describe these birds\",\\nattachments=[\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\")\\n]\\n)\\nAccessconversation.responses for a list of all of the responses that have so far been returned during the conver-\\nsation.\\nConversations using tools\\nYou can pass a list of tool functions to thetools=[]argument when you start a new conversation:\\nimport llm\\ndef upper(text: str) -> str:\\n\"convert text to upper case\"\\nreturn text.upper()\\ndef reverse(text: str) -> str:\\n\"reverse text\"\\nreturn text[::-1]\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nconversation = model.conversation(tools=[upper, reverse])\\nYou can then call theconversation.chain()method multiple times to have a conversation that uses those tools:\\nprint(conversation.chain(\\n\"Convert panda to uppercase and reverse it\"\\n).text())\\nprint(conversation.chain(\\n\"Same with pangolin\"\\n).text())\\nThe before_call= and after_call= parameters described above can be passed directly to the model.\\nconversation()method to set those options for all chained prompts in that conversation.\\n2.12.4 Listing models\\nThe llm.get_models()list returns a list of all available models, including those from plugins.\\nimport llm\\nfor model in llm.get_models():\\nprint(model.model_id)\\nUse llm.get_async_models()to list async models:\\n118 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nfor model in llm.get_async_models():\\nprint(model.model_id)\\n2.12.5 Running code when a response has completed\\nFor some applications, such as tracking the tokens used by an application, it may be useful to execute code as soon as\\na response has finished being executed\\nYou can do this using theresponse.on_done(callback)method, which causes your callback function to be called\\nas soon as the response has finished (all tokens have been returned).\\nThe signature of the method you provide isdef callback(response)- it can be optionally anasync def method\\nwhen working with asynchronous models.\\nExample usage:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"a poem about a hippo\")\\nresponse.on_done(lambda response: print(response.usage()))\\nprint(response.text())\\nWhich outputs:\\nUsage(input=20, output=494, details={})\\nIn a sunlit glade by a bubbling brook,\\nLived a hefty hippo, with a curious look.\\n...\\nOr using anasynciomodel, where you need toawait response.on_done(done)to queue up the callback:\\nimport asyncio, llm\\nasync def run():\\nmodel = llm.get_async_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\"a short poem about a brick\")\\nasync def done(response):\\nprint(await response.usage())\\nprint(await response.text())\\nawait response.on_done(done)\\nprint(await response.text())\\nasyncio.run(run())\\n2.12. Python API 119\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.12.6 Other functions\\nThe llmtop level package includes some useful utility functions.\\nset_alias(alias, model_id)\\nThe llm.set_alias()function can be used to define a new alias:\\nimport llm\\nllm.set_alias(\"mini\", \"gpt-4o-mini\")\\nThe second argument can be a model identifier or another alias, in which case that alias will be resolved.\\nIf thealiases.jsonfile does not exist or contains invalid JSON it will be created or overwritten.\\nremove_alias(alias)\\nRemoves the alias with the given name from thealiases.jsonfile.\\nRaises KeyErrorif the alias does not exist.\\nimport llm\\nllm.remove_alias(\"turbo\")\\nset_default_model(alias)\\nThis sets the default model to the given model ID or alias. Any changes to defaults will be persisted in the LLM\\nconfiguration folder, and will affect all programs using LLM on the system, including thellmCLI tool.\\nimport llm\\nllm.set_default_model(\"claude-3.5-sonnet\")\\nget_default_model()\\nThis returns the currently configured default model, orgpt-4o-miniif no default has been set.\\nimport llm\\nmodel_id = llm.get_default_model()\\nTo detect if no default has been set you can use this pattern:\\nif llm.get_default_model(default=None) is None:\\nprint(\"No default has been set\")\\nHere thedefault=parameter specifies the value that should be returned if there is no configured default.\\n120 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nset_default_embedding_model(alias) and get_default_embedding_model()\\nThese two methods work the same asset_default_model() and get_default_model() but for the defaultem-\\nbedding modelinstead.\\n2.13 Logging to SQLite\\nllmdefaults to logging all prompts and responses to a SQLite database.\\nYou can find the location of that database using thellm logs pathcommand:\\nllm logs path\\nOn my Mac that outputs:\\n/Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nThis will differ for other operating systems.\\nTo avoid logging an individual prompt, pass--no-logor -nto the command:\\nllm \\'Ten names for cheesecakes\\' -n\\nTo turn logging by default off:\\nllm logs off\\nIf you’ve turned off logging you can still log an individual prompt and response by adding--log:\\nllm \\'Five ambitious names for a pet pterodactyl\\' --log\\nTo turn logging by default back on again:\\nllm logs on\\nTo see the status of the logs database, run this:\\nllm logs status\\nExample output:\\nLogging is ON for all prompts\\nFound log database at /Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nNumber of conversations logged: 33\\nNumber of responses logged: 48\\nDatabase file size: 19.96MB\\n2.13. Logging to SQLite 121\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.13.1 Viewing the logs\\nYou can view the logs using thellm logscommand:\\nllm logs\\nThis will output the three most recent logged items in Markdown format, showing both the prompt and the response\\nformatted using Markdown.\\nTo get back just the most recent prompt response as plain text, add-r/--response:\\nllm logs -r\\nUse -x/--extractto extract and return the first fenced code block from the selected log entries:\\nllm logs --extract\\nOr --xl/--extract-lastfor the last fenced code block:\\nllm logs --extract-last\\nAdd--jsonto get the log messages in JSON instead:\\nllm logs --json\\nAdd-n 10to see the ten most recent items:\\nllm logs -n 10\\nOr -n 0to see everything that has ever been logged:\\nllm logs -n 0\\nYou can truncate the display of the prompts and responses using the-t/--truncateoption. This can help make the\\nJSON output more readable - though the--shortoption is usually better.\\nllm logs -n 1 -t --json\\nExample output:\\n[\\n{\\n\"id\": \"01jm8ec74wxsdatyn5pq1fp0s5\",\\n\"model\": \"anthropic/claude-3-haiku-20240307\",\\n\"prompt\": \"hi\",\\n\"system\": null,\\n\"prompt_json\": null,\\n\"response\": \"Hello! How can I assist you today?\",\\n\"conversation_id\": \"01jm8ec74taftdgj2t4zra9z0j\",\\n\"duration_ms\": 560,\\n\"datetime_utc\": \"2025-02-16T22:34:30.374882+00:00\",\\n\"input_tokens\": 8,\\n\"output_tokens\": 12,\\n\"token_details\": null,\\n\"conversation_name\": \"hi\",\\n(continues on next page)\\n122 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n\"conversation_model\": \"anthropic/claude-3-haiku-20240307\",\\n\"attachments\": []\\n}\\n]\\n-s/–short mode\\nUse -s/--shortto see a shortened YAML log with truncated prompts and no responses:\\nllm logs -n 2 --short\\nExample output:\\n- model: deepseek-reasoner\\ndatetime: \\'2025-02-02T06:39:53\\'\\nconversation: 01jk2pk05xq3d0vgk0202zrsg1\\nprompt: H01 There are five huts. H02 The Scotsman lives in the purple hut. H03 The␣\\n˓→Welshman owns the parrot. H04 Kombucha is...\\n- model: o3-mini\\ndatetime: \\'2025-02-02T19:03:05\\'\\nconversation: 01jk40qkxetedzpf1zd8k9bgww\\nsystem: Formatting re-enabled. Write a detailed README with extensive usage examples.\\nprompt: <documents> <document index=\"1\"> <source>./Cargo.toml</source> <document_\\n˓→content> [package] name = \"py-limbo\" version...\\nInclude -u/--usageto include token usage information:\\nllm logs -n 1 --short --usage\\nExample output:\\n- model: o3-mini\\ndatetime: \\'2025-02-16T23:00:56\\'\\nconversation: 01jm8fxxnef92n1663c6ays8xt\\nsystem: Produce Python code that demonstrates every possible usage of yaml.dump\\nwith all of the arguments it can take, especi...\\nprompt: <documents> <document index=\"1\"> <source>./setup.py</source> <document_content>\\nNAME = \\'PyYAML\\' VERSION = \\'7.0.0.dev0...\\nusage:\\ninput: 74793\\noutput: 3550\\ndetails:\\ncompletion_tokens_details:\\nreasoning_tokens: 2240\\n2.13. Logging to SQLite 123\\nLLM documentation, Release 0.26-31-g0bf655a\\nLogs for a conversation\\nTo view the logs for the most recentconversationyou have had with a model, use-c:\\nllm logs -c\\nTo see logs for a specific conversation based on its ID, use--cid IDor --conversation ID:\\nllm logs --cid 01h82n0q9crqtnzmf13gkyxawg\\nSearching the logs\\nYou can search the logs for a search term in thepromptor theresponsecolumns.\\nllm logs -q \\'cheesecake\\'\\nThe most relevant results will be shown first.\\nTo switch to sorting with most recent first, add-l/--latest. This can be combined with-n to limit the number of\\nresults shown:\\nllm logs -q \\'cheesecake\\' -l -n 3\\nFiltering past a specific ID\\nIfyouwanttoretrieveallofthelogsthatwererecordedsinceaspecificresponseIDyoucandosousingtheseoptions:\\n• --id-gt $ID- every record with an ID greater than $ID\\n• --id-gte $ID- every record with an ID greater than or equal to $ID\\nIDs are always issued in ascending order by time, so this provides a useful way to see everything that has happened\\nsince a particular record.\\nThis can be particularly useful whenworking with schema data, where you might want to access every record that you\\nhave created using a specific--schemabut exclude records you have previously processed.\\nFiltering by model\\nYou can filter to logs just for a specific model (or model alias) using-m/--model:\\nllm logs -m chatgpt\\nFiltering by prompts that used specific fragments\\nThe-f/--fragment Xoptionwillfilterforjustresponsesthatwerecreatedusingthespecified fragmenthashoralias\\nor URL or filename.\\nFragmentsaredisplayedinthelogsastheirhashID.Add -e/--expandtodisplayfragmentsastheirfullcontent-this\\noption works for both the default Markdown and the--jsonmode:\\nllm logs -f https://llm.datasette.io/robots.txt --expand\\nYoucandisplayjustthecontentforaspecificfragmenthashID(oralias)usingthe llm fragments showcommand:\\n124 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm fragments show 993fd38d898d2b59fd2d16c811da5bdac658faa34f0f4d411edde7c17ebb0680\\nIf you provide multiple fragments you will get back responses that usedall of those fragments.\\nFiltering by prompts that used specific tools\\nYou can filter for responses that used tools from specific fragments with the--tool/-Toption:\\nllm logs -T simple_eval\\nThis will match responses that involved aresult from that tool. If the tool was not executed it will not be included in\\nthe filtered responses.\\nPass--tool/-Tmultiple times for responses that used all of the specified tools.\\nUse the llm logs --tools flag to see all responses that involved at least one tool result, including from\\n--functions:\\nllm logs --tools\\nBrowsing data collected using schemas\\nThe--schema Xoptioncanbeusedtoviewresponsesthatusedthespecifiedschema,usinganyofthe waystospecify\\na schema:\\nllm logs --schema \\'name, age int, bio\\'\\nThis can be combined with--data and --data-array and --data-key to extract just the returned JSON data -\\nconsult theschemas documentationfor details.\\n2.13.2 Browsing logs using Datasette\\nYou can also use Datasette to browse your logs like this:\\ndatasette \"$(llm logs path)\"\\n2.13.3 Backing up your database\\nYou can backup your logs to another file using thellm logs backupcommand:\\nllm logs backup /tmp/backup.db\\nThis uses SQLite VACUUM INTO under the hood.\\n2.13. Logging to SQLite 125\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.13.4 SQL schema\\nHere’s the SQL schema used by thelogs.dbdatabase:\\nCREATE TABLE [conversations] (\\n[id] TEXT PRIMARY KEY,\\n[name] TEXT,\\n[model] TEXT\\n);\\nCREATE TABLE [schemas] (\\n[id] TEXT PRIMARY KEY,\\n[content] TEXT\\n);\\nCREATE TABLE \"responses\" (\\n[id] TEXT PRIMARY KEY,\\n[model] TEXT,\\n[prompt] TEXT,\\n[system] TEXT,\\n[prompt_json] TEXT,\\n[options_json] TEXT,\\n[response] TEXT,\\n[response_json] TEXT,\\n[conversation_id] TEXT REFERENCES [conversations]([id]),\\n[duration_ms] INTEGER,\\n[datetime_utc] TEXT,\\n[input_tokens] INTEGER,\\n[output_tokens] INTEGER,\\n[token_details] TEXT,\\n[schema_id] TEXT REFERENCES [schemas]([id]),\\n[resolved_model] TEXT\\n);\\nCREATE VIRTUAL TABLE [responses_fts] USING FTS5 (\\n[prompt],\\n[response],\\ncontent=[responses]\\n);\\nCREATE TABLE [attachments] (\\n[id] TEXT PRIMARY KEY,\\n[type] TEXT,\\n[path] TEXT,\\n[url] TEXT,\\n[content] BLOB\\n);\\nCREATE TABLE [prompt_attachments] (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[attachment_id] TEXT REFERENCES [attachments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[attachment_id])\\n);\\nCREATE TABLE [fragments] (\\n[id] INTEGER PRIMARY KEY,\\n[hash] TEXT,\\n(continues on next page)\\n126 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n[content] TEXT,\\n[datetime_utc] TEXT,\\n[source] TEXT\\n);\\nCREATE TABLE [fragment_aliases] (\\n[alias] TEXT PRIMARY KEY,\\n[fragment_id] INTEGER REFERENCES [fragments]([id])\\n);\\nCREATE TABLE \"prompt_fragments\" (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[fragment_id] INTEGER REFERENCES [fragments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[fragment_id],\\n[order])\\n);\\nCREATE TABLE \"system_fragments\" (\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[fragment_id] INTEGER REFERENCES [fragments]([id]),\\n[order] INTEGER,\\nPRIMARY KEY ([response_id],\\n[fragment_id],\\n[order])\\n);\\nCREATE TABLE [tools] (\\n[id] INTEGER PRIMARY KEY,\\n[hash] TEXT,\\n[name] TEXT,\\n[description] TEXT,\\n[input_schema] TEXT,\\n[plugin] TEXT\\n);\\nCREATE TABLE [tool_responses] (\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[response_id] TEXT REFERENCES [responses]([id]),\\nPRIMARY KEY ([tool_id],\\n[response_id])\\n);\\nCREATE TABLE [tool_calls] (\\n[id] INTEGER PRIMARY KEY,\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[name] TEXT,\\n[arguments] TEXT,\\n[tool_call_id] TEXT\\n);\\nCREATE TABLE \"tool_results\" (\\n[id] INTEGER PRIMARY KEY,\\n[response_id] TEXT REFERENCES [responses]([id]),\\n[tool_id] INTEGER REFERENCES [tools]([id]),\\n[name] TEXT,\\n[output] TEXT,\\n(continues on next page)\\n2.13. Logging to SQLite 127\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n[tool_call_id] TEXT,\\n[instance_id] INTEGER REFERENCES [tool_instances]([id]),\\n[exception] TEXT\\n);\\nCREATE TABLE [tool_instances] (\\n[id] INTEGER PRIMARY KEY,\\n[plugin] TEXT,\\n[name] TEXT,\\n[arguments] TEXT\\n);\\nresponses_fts configures SQLite full-text search against theprompt and response columns in theresponses\\ntable.\\n2.14 Related tools\\nThe following tools are designed to be used with LLM:\\n2.14.1 strip-tags\\nstrip-tags is a command for stripping tags from HTML. This is useful when working with LLMs because HTML tags\\ncan use up a lot of your token budget.\\nHere’showtosummarizethefrontpageoftheNewYorkTimes,bybothstrippingtagsandfilteringtojusttheelements\\nwith class=\"story-wrapper\":\\ncurl -s https://www.nytimes.com/ \\\\\\n| strip-tags .story-wrapper \\\\\\n| llm -s \\'summarize the news\\'\\nllm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs describes ways to usestrip-tagsin\\nmore detail.\\n2.14.2 ttok\\nttok is a command-line tool for counting OpenAI tokens. You can use it to check if input is likely to fit in the token\\nlimit for GPT 3.5 or GPT4:\\ncat my-file.txt | ttok\\n125\\nIt can also truncate input down to a desired number of tokens:\\nttok This is too many tokens -t 3\\nThis is too\\nThis is useful for truncating a large document down to a size where it can be processed by an LLM.\\n128 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.14.3 Symbex\\nSymbex is a tool for searching for symbols in Python codebases. It’s useful for extracting just the code for a specific\\nproblem and then piping that into LLM for explanation, refactoring or other tasks.\\nHere’s how to use it to find all functions that matchtest*csv* and use those to guess what the software under test\\ndoes:\\nsymbex \\'test*csv*\\' | \\\\\\nllm --system \\'based on these tests guess what this tool does\\'\\nIt can also be used to export symbols in a format that can be piped tollm embed-multiin order to create embeddings:\\nsymbex \\'*\\' \\'*:*\\' --nl | \\\\\\nllm embed-multi symbols - \\\\\\n--format nl --database embeddings.db --store\\nFor more examples see Symbex: search Python code for functions and classes, then pipe them into a LLM.\\n2.15 CLI reference\\nThis page lists the--helpoutput for all of thellmcommands.\\n2.15.1 llm –help\\nUsage: llm [OPTIONS] COMMAND [ARGS]...\\nAccess Large Language Models from the command-line\\nDocumentation: https://llm.datasette.io/\\nLLM can run models from many different providers. Consult the plugin directory\\nfor a list of available models:\\nhttps://llm.datasette.io/en/stable/plugins/directory.html\\nTo get started with OpenAI, obtain an API key from them and:\\n$ llm keys set openai\\nEnter key: ...\\nThen execute a prompt like this:\\nllm \\'Five outrageous names for a pet pelican\\'\\nFor a full list of prompting options run:\\nllm prompt --help\\nOptions:\\n--version Show the version and exit.\\n(continues on next page)\\n2.15. CLI reference 129\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n-h, --help Show this message and exit.\\nCommands:\\nprompt* Execute a prompt\\naliases Manage model aliases\\nchat Hold an ongoing chat with a model.\\ncollections View and manage collections of embeddings\\nembed Embed text and store or return the result\\nembed-models Manage available embedding models\\nembed-multi Store embeddings for multiple strings at once in the...\\nfragments Manage fragments that are stored in the database\\ninstall Install packages from PyPI into the same environment as LLM\\nkeys Manage stored API keys for different models\\nlogs Tools for exploring logged prompts and responses\\nmodels Manage available models\\nopenai Commands for working directly with the OpenAI API\\nplugins List installed plugins\\nschemas Manage stored schemas\\nsimilar Return top N similar IDs from a collection using cosine...\\ntemplates Manage stored prompt templates\\ntools Manage tools that can be made available to LLMs\\nuninstall Uninstall Python packages from the LLM environment\\nllm prompt –help\\nUsage: llm prompt [OPTIONS] [PROMPT]\\nExecute a prompt\\nDocumentation: https://llm.datasette.io/en/stable/usage.html\\nExamples:\\nllm \\'Capital of France?\\'\\nllm \\'Capital of France?\\' -m gpt-4o\\nllm \\'Capital of France?\\' -s \\'answer in Spanish\\'\\nMulti-modal models can be called with attachments like this:\\nllm \\'Extract text from this image\\' -a image.jpg\\nllm \\'Describe\\' -a https://static.simonwillison.net/static/2024/pelicans.jpg\\ncat image | llm \\'describe image\\' -a -\\n# With an explicit mimetype:\\ncat image | llm \\'describe image\\' --at - image/jpeg\\nThe -x/--extract option returns just the content of the first ``` fenced code\\nblock, if one is present. If none are present it returns the full response.\\nllm \\'JavaScript function for reversing a string\\' -x\\n(continues on next page)\\n130 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-d, --database FILE Path to log database\\n-q, --query TEXT Use first model matching these strings\\n-a, --attachment ATTACHMENT Attachment path or URL or -\\n--at, --attachment-type <TEXT TEXT>...\\nAttachment with explicit mimetype,\\n--at image.jpg image/jpeg\\n-T, --tool TEXT Name of a tool to make available to the model\\n--functions TEXT Python code block or file path defining\\nfunctions to register as tools\\n--td, --tools-debug Show full details of tool executions\\n--ta, --tools-approve Manually approve every tool execution\\n--cl, --chain-limit INTEGER How many chained tool responses to allow,\\ndefault 5, set 0 for unlimited\\n-o, --option <TEXT TEXT>... key/value options for the model\\n--schema TEXT JSON schema, filepath or ID\\n--schema-multi TEXT JSON schema to use for multiple results\\n-f, --fragment TEXT Fragment (alias, URL, hash or file path) to\\nadd to the prompt\\n--sf, --system-fragment TEXT Fragment to add to system prompt\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters for template\\n--no-stream Do not stream output\\n-n, --no-log Don \\'t log to database\\n--log Log prompt and response to the database\\n-c, --continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation with the given ID.\\n--key TEXT API key to use\\n--save TEXT Save prompt with this template name\\n--async Run prompt asynchronously\\n-u, --usage Show token usage\\n-x, --extract Extract first fenced code block\\n--xl, --extract-last Extract last fenced code block\\n-h, --help Show this message and exit.\\nllm chat –help\\nUsage: llm chat [OPTIONS]\\nHold an ongoing chat with a model.\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-c, --continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation with the given ID.\\n-f, --fragment TEXT Fragment (alias, URL, hash or file path) to add\\nto the prompt\\n(continues on next page)\\n2.15. CLI reference 131\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n--sf, --system-fragment TEXT Fragment to add to system prompt\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters for template\\n-o, --option <TEXT TEXT>... key/value options for the model\\n-d, --database FILE Path to log database\\n--no-stream Do not stream output\\n--key TEXT API key to use\\n-T, --tool TEXT Name of a tool to make available to the model\\n--functions TEXT Python code block or file path defining\\nfunctions to register as tools\\n--td, --tools-debug Show full details of tool executions\\n--ta, --tools-approve Manually approve every tool execution\\n--cl, --chain-limit INTEGER How many chained tool responses to allow,\\ndefault 5, set 0 for unlimited\\n-h, --help Show this message and exit.\\nllm keys –help\\nUsage: llm keys [OPTIONS] COMMAND [ARGS]...\\nManage stored API keys for different models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List names of all stored keys\\nget Return the value of a stored key\\npath Output the path to the keys.json file\\nset Save a key in the keys.json file\\nllm keys list –help\\nUsage: llm keys list [OPTIONS]\\nList names of all stored keys\\nOptions:\\n-h, --help Show this message and exit.\\n132 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm keys path –help\\nUsage: llm keys path [OPTIONS]\\nOutput the path to the keys.json file\\nOptions:\\n-h, --help Show this message and exit.\\nllm keys get –help\\nUsage: llm keys get [OPTIONS] NAME\\nReturn the value of a stored key\\nExample usage:\\nexport OPENAI_API_KEY=$(llm keys get openai)\\nOptions:\\n-h, --help Show this message and exit.\\nllm keys set –help\\nUsage: llm keys set [OPTIONS] NAME\\nSave a key in the keys.json file\\nExample usage:\\n$ llm keys set openai\\nEnter key: ...\\nOptions:\\n--value TEXT Value to set\\n-h, --help Show this message and exit.\\nllm logs –help\\nUsage: llm logs [OPTIONS] COMMAND [ARGS]...\\nTools for exploring logged prompts and responses\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\n(continues on next page)\\n2.15. CLI reference 133\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nlist* Show logged prompts and their responses\\nbackup Backup your logs database to this file\\noff Turn off logging for all prompts\\non Turn on logging for all prompts\\npath Output the path to the logs.db file\\nstatus Show current status of database logging\\nllm logs path –help\\nUsage: llm logs path [OPTIONS]\\nOutput the path to the logs.db file\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs status –help\\nUsage: llm logs status [OPTIONS]\\nShow current status of database logging\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs backup –help\\nUsage: llm logs backup [OPTIONS] PATH\\nBackup your logs database to this file\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs on –help\\nUsage: llm logs on [OPTIONS]\\nTurn on logging for all prompts\\nOptions:\\n-h, --help Show this message and exit.\\n134 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm logs off –help\\nUsage: llm logs off [OPTIONS]\\nTurn off logging for all prompts\\nOptions:\\n-h, --help Show this message and exit.\\nllm logs list –help\\nUsage: llm logs list [OPTIONS]\\nShow logged prompts and their responses\\nOptions:\\n-n, --count INTEGER Number of entries to show - defaults to 3, use 0\\nfor all\\n-d, --database FILE Path to log database\\n-m, --model TEXT Filter by model or model alias\\n-q, --query TEXT Search for logs matching this string\\n-f, --fragment TEXT Filter for prompts using these fragments\\n-T, --tool TEXT Filter for prompts with results from these tools\\n--tools Filter for prompts with results from any tools\\n--schema TEXT JSON schema, filepath or ID\\n--schema-multi TEXT JSON schema used for multiple results\\n-l, --latest Return latest results matching search query\\n--data Output newline-delimited JSON data for schema\\n--data-array Output JSON array of data for schema\\n--data-key TEXT Return JSON objects from array in this key\\n--data-ids Attach corresponding IDs to JSON objects\\n-t, --truncate Truncate long strings in output\\n-s, --short Shorter YAML output with truncated prompts\\n-u, --usage Include token usage\\n-r, --response Just output the last response\\n-x, --extract Extract first fenced code block\\n--xl, --extract-last Extract last fenced code block\\n-c, --current Show logs from the current conversation\\n--cid, --conversation TEXT Show logs for this conversation ID\\n--id-gt TEXT Return responses with ID > this\\n--id-gte TEXT Return responses with ID >= this\\n--json Output logs as JSON\\n-e, --expand Expand fragments to show their content\\n-h, --help Show this message and exit.\\n2.15. CLI reference 135\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm models –help\\nUsage: llm models [OPTIONS] COMMAND [ARGS]...\\nManage available models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available models\\ndefault Show or set the default model\\noptions Manage default options for models\\nllm models list –help\\nUsage: llm models list [OPTIONS]\\nList available models\\nOptions:\\n--options Show options for each model, if available\\n--async List async models\\n--schemas List models that support schemas\\n--tools List models that support tools\\n-q, --query TEXT Search for models matching these strings\\n-m, --model TEXT Specific model IDs\\n-h, --help Show this message and exit.\\nllm models default –help\\nUsage: llm models default [OPTIONS] [MODEL]\\nShow or set the default model\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options –help\\nUsage: llm models options [OPTIONS] COMMAND [ARGS]...\\nManage default options for models\\nOptions:\\n-h, --help Show this message and exit.\\n(continues on next page)\\n136 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nCommands:\\nlist* List default options for all models\\nclear Clear default option(s) for a model\\nset Set a default option for a model\\nshow List default options set for a specific model\\nllm models options list –help\\nUsage: llm models options list [OPTIONS]\\nList default options for all models\\nExample usage:\\nllm models options list\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options show –help\\nUsage: llm models options show [OPTIONS] MODEL\\nList default options set for a specific model\\nExample usage:\\nllm models options show gpt-4o\\nOptions:\\n-h, --help Show this message and exit.\\nllm models options set –help\\nUsage: llm models options set [OPTIONS] MODEL KEY VALUE\\nSet a default option for a model\\nExample usage:\\nllm models options set gpt-4o temperature 0.5\\nOptions:\\n-h, --help Show this message and exit.\\n2.15. CLI reference 137\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm models options clear –help\\nUsage: llm models options clear [OPTIONS] MODEL [KEY]\\nClear default option(s) for a model\\nExample usage:\\nllm models options clear gpt-4o\\n# Or for a single option\\nllm models options clear gpt-4o temperature\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates –help\\nUsage: llm templates [OPTIONS] COMMAND [ARGS]...\\nManage stored prompt templates\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available prompt templates\\nedit Edit the specified prompt template using the default $EDITOR\\nloaders Show template loaders registered by plugins\\npath Output the path to the templates directory\\nshow Show the specified prompt template\\nllm templates list –help\\nUsage: llm templates list [OPTIONS]\\nList available prompt templates\\nOptions:\\n-h, --help Show this message and exit.\\n138 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm templates show –help\\nUsage: llm templates show [OPTIONS] NAME\\nShow the specified prompt template\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates edit –help\\nUsage: llm templates edit [OPTIONS] NAME\\nEdit the specified prompt template using the default $EDITOR\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates path –help\\nUsage: llm templates path [OPTIONS]\\nOutput the path to the templates directory\\nOptions:\\n-h, --help Show this message and exit.\\nllm templates loaders –help\\nUsage: llm templates loaders [OPTIONS]\\nShow template loaders registered by plugins\\nOptions:\\n-h, --help Show this message and exit.\\nllm schemas –help\\nUsage: llm schemas [OPTIONS] COMMAND [ARGS]...\\nManage stored schemas\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\n(continues on next page)\\n2.15. CLI reference 139\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nlist* List stored schemas\\ndsl Convert LLM \\'s schema DSL to a JSON schema\\nshow Show a stored schema\\nllm schemas list –help\\nUsage: llm schemas list [OPTIONS]\\nList stored schemas\\nOptions:\\n-d, --database FILE Path to log database\\n-q, --query TEXT Search for schemas matching this string\\n--full Output full schema contents\\n--json Output as JSON\\n--nl Output as newline-delimited JSON\\n-h, --help Show this message and exit.\\nllm schemas show –help\\nUsage: llm schemas show [OPTIONS] SCHEMA_ID\\nShow a stored schema\\nOptions:\\n-d, --database FILE Path to log database\\n-h, --help Show this message and exit.\\nllm schemas dsl –help\\nUsage: llm schemas dsl [OPTIONS] INPUT\\nConvert LLM\\'s schema DSL to a JSON schema\\nllm schema dsl \\'name, age int, bio: their bio\\'\\nOptions:\\n--multi Wrap in an array\\n-h, --help Show this message and exit.\\n140 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm tools –help\\nUsage: llm tools [OPTIONS] COMMAND [ARGS]...\\nManage tools that can be made available to LLMs\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available tools that have been provided by plugins\\nllm tools list –help\\nUsage: llm tools list [OPTIONS]\\nList available tools that have been provided by plugins\\nOptions:\\n--json Output as JSON\\n--functions TEXT Python code block or file path defining functions to\\nregister as tools\\n-h, --help Show this message and exit.\\nllm aliases –help\\nUsage: llm aliases [OPTIONS] COMMAND [ARGS]...\\nManage model aliases\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List current aliases\\npath Output the path to the aliases.json file\\nremove Remove an alias\\nset Set an alias for a model\\nllm aliases list –help\\nUsage: llm aliases list [OPTIONS]\\nList current aliases\\nOptions:\\n--json Output as JSON\\n-h, --help Show this message and exit.\\n2.15. CLI reference 141\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm aliases set –help\\nUsage: llm aliases set [OPTIONS] ALIAS [MODEL_ID]\\nSet an alias for a model\\nExample usage:\\nllm aliases set mini gpt-4o-mini\\nAlternatively you can omit the model ID and specify one or more -q options.\\nThe first model matching all of those query strings will be used.\\nllm aliases set mini -q 4o -q mini\\nOptions:\\n-q, --query TEXT Set alias for model matching these strings\\n-h, --help Show this message and exit.\\nllm aliases remove –help\\nUsage: llm aliases remove [OPTIONS] ALIAS\\nRemove an alias\\nExample usage:\\n$ llm aliases remove turbo\\nOptions:\\n-h, --help Show this message and exit.\\nllm aliases path –help\\nUsage: llm aliases path [OPTIONS]\\nOutput the path to the aliases.json file\\nOptions:\\n-h, --help Show this message and exit.\\n142 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm fragments –help\\nUsage: llm fragments [OPTIONS] COMMAND [ARGS]...\\nManage fragments that are stored in the database\\nFragments are reusable snippets of text that are shared across multiple\\nprompts.\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List current fragments\\nloaders Show fragment loaders registered by plugins\\nremove Remove a fragment alias\\nset Set an alias for a fragment\\nshow Display the fragment stored under an alias or hash\\nllm fragments list –help\\nUsage: llm fragments list [OPTIONS]\\nList current fragments\\nOptions:\\n-q, --query TEXT Search for fragments matching these strings\\n--aliases Show only fragments with aliases\\n--json Output as JSON\\n-h, --help Show this message and exit.\\nllm fragments set –help\\nUsage: llm fragments set [OPTIONS] ALIAS FRAGMENT\\nSet an alias for a fragment\\nAccepts an alias and a file path, URL, hash or \\'-\\' for stdin\\nExample usage:\\nllm fragments set mydocs ./docs.md\\nOptions:\\n-h, --help Show this message and exit.\\n2.15. CLI reference 143\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm fragments show –help\\nUsage: llm fragments show [OPTIONS] ALIAS_OR_HASH\\nDisplay the fragment stored under an alias or hash\\nllm fragments show mydocs\\nOptions:\\n-h, --help Show this message and exit.\\nllm fragments remove –help\\nUsage: llm fragments remove [OPTIONS] ALIAS\\nRemove a fragment alias\\nExample usage:\\nllm fragments remove docs\\nOptions:\\n-h, --help Show this message and exit.\\nllm fragments loaders –help\\nUsage: llm fragments loaders [OPTIONS]\\nShow fragment loaders registered by plugins\\nOptions:\\n-h, --help Show this message and exit.\\nllm plugins –help\\nUsage: llm plugins [OPTIONS]\\nList installed plugins\\nOptions:\\n--all Include built- in default plugins\\n--hook TEXT Filter for plugins that implement this hook\\n-h, --help Show this message and exit.\\n144 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm install –help\\nUsage: llm install [OPTIONS] [PACKAGES]...\\nInstall packages from PyPI into the same environment as LLM\\nOptions:\\n-U, --upgrade Upgrade packages to latest version\\n-e, --editable TEXT Install a project in editable mode from this path\\n--force-reinstall Reinstall all packages even if they are already up-to-\\ndate\\n--no-cache-dir Disable the cache\\n--pre Include pre-release and development versions\\n-h, --help Show this message and exit.\\nllm uninstall –help\\nUsage: llm uninstall [OPTIONS] PACKAGES...\\nUninstall Python packages from the LLM environment\\nOptions:\\n-y, --yes Don \\'t ask for confirmation\\n-h, --help Show this message and exit.\\nllm embed –help\\nUsage: llm embed [OPTIONS] [COLLECTION] [ID]\\nEmbed text and store or return the result\\nOptions:\\n-i, --input PATH File to embed\\n-m, --model TEXT Embedding model to use\\n--store Store the text itself in the database\\n-d, --database FILE\\n-c, --content TEXT Content to embed\\n--binary Treat input as binary data\\n--metadata TEXT JSON object metadata to store\\n-f, --format [json|blob|base64|hex]\\nOutput format\\n-h, --help Show this message and exit.\\n2.15. CLI reference 145\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm embed-multi –help\\nUsage: llm embed-multi [OPTIONS] COLLECTION [INPUT_PATH]\\nStore embeddings for multiple strings at once in the specified collection.\\nInput data can come from one of three sources:\\n1. A CSV, TSV, JSON or JSONL file:\\n- CSV/TSV: First column is ID, remaining columns concatenated as content\\n- JSON: Array of objects with \"id\" field and content fields\\n- JSONL: Newline-delimited JSON objects\\nExamples:\\nllm embed-multi docs input.csv\\ncat data.json | llm embed-multi docs -\\nllm embed-multi docs input.json --format json\\n2. A SQL query against a SQLite database:\\n- First column returned is used as ID\\n- Other columns concatenated to form content\\nExamples:\\nllm embed-multi docs --sql \"SELECT id, title, body FROM posts\"\\nllm embed-multi docs --attach blog blog.db --sql \"SELECT id, content FROM blog.\\n˓→posts\"\\n3. Files in directories matching glob patterns:\\n- Each file becomes one embedding\\n- Relative file paths become IDs\\nExamples:\\nllm embed-multi docs --files docs \\'**/*.md\\'\\nllm embed-multi images --files photos \\'*.jpg\\' --binary\\nllm embed-multi texts --files texts \\'*.txt\\' --encoding utf-8 --encoding latin-1\\nOptions:\\n--format [json|csv|tsv|nl] Format of input file - defaults to auto-detect\\n--files <DIRECTORY TEXT>... Embed files in this directory - specify directory\\nand glob pattern\\n--encoding TEXT Encodings to try when reading --files\\n--binary Treat --files as binary data\\n--sql TEXT Read input using this SQL query\\n--attach <TEXT FILE>... Additional databases to attach - specify alias\\nand file path\\n--batch-size INTEGER Batch size to use when running embeddings\\n--prefix TEXT Prefix to add to the IDs\\n-m, --model TEXT Embedding model to use\\n--prepend TEXT Prepend this string to all content before\\nembedding\\n--store Store the text itself in the database\\n-d, --database FILE\\n-h, --help Show this message and exit.\\n146 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm similar –help\\nUsage: llm similar [OPTIONS] COLLECTION [ID]\\nReturn top N similar IDs from a collection using cosine similarity.\\nExample usage:\\nllm similar my-collection -c \"I like cats\"\\nOr to find content similar to a specific stored ID:\\nllm similar my-collection 1234\\nOptions:\\n-i, --input PATH File to embed for comparison\\n-c, --content TEXT Content to embed for comparison\\n--binary Treat input as binary data\\n-n, --number INTEGER Number of results to return\\n-p, --plain Output in plain text format\\n-d, --database FILE\\n--prefix TEXT Just IDs with this prefix\\n-h, --help Show this message and exit.\\nllm embed-models –help\\nUsage: llm embed-models [OPTIONS] COMMAND [ARGS]...\\nManage available embedding models\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* List available embedding models\\ndefault Show or set the default embedding model\\nllm embed-models list –help\\nUsage: llm embed-models list [OPTIONS]\\nList available embedding models\\nOptions:\\n-q, --query TEXT Search for embedding models matching these strings\\n-h, --help Show this message and exit.\\n2.15. CLI reference 147\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm embed-models default –help\\nUsage: llm embed-models default [OPTIONS] [MODEL]\\nShow or set the default embedding model\\nOptions:\\n--remove-default Reset to specifying no default model\\n-h, --help Show this message and exit.\\nllm collections –help\\nUsage: llm collections [OPTIONS] COMMAND [ARGS]...\\nView and manage collections of embeddings\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nlist* View a list of collections\\ndelete Delete the specified collection\\npath Output the path to the embeddings database\\nllm collections path –help\\nUsage: llm collections path [OPTIONS]\\nOutput the path to the embeddings database\\nOptions:\\n-h, --help Show this message and exit.\\nllm collections list –help\\nUsage: llm collections list [OPTIONS]\\nView a list of collections\\nOptions:\\n-d, --database FILE Path to embeddings database\\n--json Output as JSON\\n-h, --help Show this message and exit.\\n148 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm collections delete –help\\nUsage: llm collections delete [OPTIONS] COLLECTION\\nDelete the specified collection\\nExample usage:\\nllm collections delete my-collection\\nOptions:\\n-d, --database FILE Path to embeddings database\\n-h, --help Show this message and exit.\\nllm openai –help\\nUsage: llm openai [OPTIONS] COMMAND [ARGS]...\\nCommands for working directly with the OpenAI API\\nOptions:\\n-h, --help Show this message and exit.\\nCommands:\\nmodels List models available to you from the OpenAI API\\nllm openai models –help\\nUsage: llm openai models [OPTIONS]\\nList models available to you from the OpenAI API\\nOptions:\\n--json Output as JSON\\n--key TEXT OpenAI API key\\n-h, --help Show this message and exit.\\n2.16 Contributing\\nTo contribute to this tool, first checkout the code. Then create a new virtual environment:\\ncd llm\\npython -m venv venv\\nsource venv/bin/activate\\nOr if you are usingpipenv:\\n2.16. Contributing 149\\nLLM documentation, Release 0.26-31-g0bf655a\\npipenv shell\\nNow install the dependencies and test dependencies:\\npip install -e \\'.[test]\\'\\nTo run the tests:\\npytest\\n2.16.1 Updating recorded HTTP API interactions and associated snapshots\\nThisprojectusespytest-recordingtorecordOpenAIAPIresponsesforsomeofthetests,andsyrupytocapturesnapshots\\nof their results.\\nIf you add a new test that calls the API you can capture the API response and snapshot like this:\\nPYTEST_OPENAI_API_KEY=\"$(llm keys get openai)\" pytest --record-mode once --snapshot-\\n˓→update\\nThen review the new snapshots intests/__snapshots__/to make sure they look correct.\\n2.16.2 Debugging tricks\\nThedefaultOpenAIpluginhasadebuggingmechanismforshowingtheexactrequestsandresponsesthatweresentto\\nthe OpenAI API.\\nSet theLLM_OPENAI_SHOW_RESPONSESenvironment variable like this:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt \\'three word slogan for an an otter-run bakery\\'\\nThis will output details of the API requests and responses to the console.\\nUse --no-streamto see a more readable version of the body that avoids streaming the response:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt --no-stream \\\\\\n\\'three word slogan for an an otter-run bakery\\'\\n2.16.3 Documentation\\nDocumentation for this project uses MyST - it is written in Markdown and rendered using Sphinx.\\nTo build the documentation locally, run the following:\\ncd docs\\npip install -r requirements.txt\\nmake livehtml\\nThis will start a live preview server, using sphinx-autobuild.\\nThe CLI--helpexamples in the documentation are managed using Cog. Update those files like this:\\njust cog\\n150 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nYou’ll need Just installed to run this command.\\n2.16.4 Release process\\nTo release a new version:\\n1. Updatedocs/changelog.mdwith the new changes.\\n2. Update the version number inpyproject.toml\\n3. Create a GitHub release for the new version.\\n4. Wait for the package to push to PyPI and then...\\n5. Run the regenerate.yaml workflow to update the Homebrew tap to the latest version.\\n2.17 Changelog\\n2.17.1 0.26 (2025-05-27)\\nTool supportis finally here! This release adds support exposingtools to LLMs, previously described in the release\\nnotes for0.26a0 and 0.26a1.\\nRead Large Language Models can run tools in your terminal with LLM 0.26for a detailed overview of the new\\nfeatures.\\nAlso in this release:\\n• Two newdefault tools: llm_version()and llm_time(). #1096, #1103\\n• Documentation onhow to add tool supports to a model plugin. #1000\\n• Added aprominent warningabout the risk of prompt injection when using tools. #1097\\n• SwitchedtousingmonotonicULIDsfortheresponseIDsinthelogs,fixingsomeintermittenttestfailures. #1099\\n• Newtool_instancestable records details of Toolbox instances created while executing a prompt. #1089\\n• llm.get_key()is now adocumented utility function. #1094\\n2.17.2 0.26a1 (2025-05-25)\\nHopefully the last alpha before a stable release that includes tool support.\\nFeatures\\n• Plugin-provided tools can now be grouped into “Toolboxes”.\\n– Toolboxes(llm.Toolboxclasses)allowpluginstoexposemultiplerelatedtoolsthatsharestateorconfig-\\nuration (e.g., aMemorytool orFilesystemtool). (#1059, #1086)\\n• Tool support forllm chat.\\n– The llm chat command now accepts--tool and --functions arguments, allowing interactive chat\\nsessions to use tools. (#1004, #1062)\\n• Tools can now execute asynchronously.\\n2.17. Changelog 151\\nLLM documentation, Release 0.26-31-g0bf655a\\n– Models that implementAsyncModel can now run tools, including tool functions defined asasync def.\\n(#1063)\\n• llm chat now supports adding fragments during a session.\\n– Usethenew !fragment <id>commandwhilechattingtoinsertcontentfromafragment. Initialfragments\\ncan also be passed tollm chatusing -f or --sf. Thanks, Dan Turkel. (#1044, #1048)\\n• Filter llm logs by tools.\\n– New--tool <name>optiontofilterlogstoshowonlyresponsesthatinvolvedaspecifictool(e.g., --tool\\nsimple_eval).\\n– The --toolsflag shows all responses that used any tool. (#1013, #1072)\\n• llm schemas list can output JSON.\\n– Added--jsonand --nl(newline-delimitedJSON)optionsto llm schemas listforprogrammaticac-\\ncess to saved schema definitions. (#1070)\\n• Filter llm similar results by ID prefix.\\n– The new--prefix option forllm similar allows searching for similar items only within IDs that start\\nwithaspecifiedstring(e.g., llm similar my-collection --prefix \\'docs/\\'). Thanks,DanTurkel.\\n(#1052)\\n• Control chained tool execution limit.\\n– New --chain-limit <N> (or --cl) option forllm prompt and llm chat to specify the maximum\\nnumberofconsecutivetoolcallsallowedforasingleprompt. Defaultsto5; setto0forunlimited. (#1025)\\n• llm plugins --hook <NAME> option.\\n– Filter the list of installed plugins to only show those that implement a specific plugin hook. (#1047)\\n• llm tools listnow shows toolboxes and their methods. (#1013)\\n• llm promptandllm chatnowautomaticallyre-enableplugin-providedtoolswhencontinuingaconversation\\n(-cor --cid). (#1020)\\n• The --tools-debugoption now pretty-prints JSON tool results for improved readability. (#1083)\\n• NewLLM_TOOLS_DEBUGenvironment variable to permanently enable--tools-debug. (#1045)\\n• llm chat sessions now correctly respect default model options configured withllm models set-options.\\nThanks, André Arko. (#985)\\n• New--preoption forllm installto allow installing pre-release packages. (#1060)\\n• OpenAI models (gpt-4o, gpt-4o-mini) now explicitly declare support for tools and vision. (#1037)\\n• The supports_toolsparameter is now supported inextra-openai-models.yaml. Thanks, Mahesh Hegde\\n. (#1068)\\n152 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nBug fixes\\n• Fixedabugwherethe nameparameterin register(function, name=\"name\")wasignoredfortoolplugins.\\n(#1032)\\n• Ensure pathlib.Path objects are cast to str before passing to click.edit in llm templates edit.\\nThanks, Abizer Lokhandwala. (#1031)\\n2.17.3 0.26a0 (2025-05-13)\\nThis is the first alpha to introducesupport for tools! Models with tool capability (which includes the default OpenAI\\nmodel family) can now be granted access to execute Python functions as part of responding to a prompt.\\nTools are supported bythe command-line interface:\\nllm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\n\\' \\'what is 34234 * 213345\\'\\nAnd inthe Python API, using a newmodel.chain()method for executing multiple prompts in a sequence:\\nimport llm\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y\\nmodel = llm.get_model(\"gpt-4.1-mini\")\\nresponse = model.chain(\\n\"What is 34234 * 213345?\",\\ntools=[multiply]\\n)\\nprint(response.text())\\nNew tools can also be defined using theregister_tools() plugin hook. They can then be called by name from the\\ncommand-line like this:\\nllm -T multiply \\'What is 34234 * 213345?\\'\\nTool support is currently underactive development. Consult this milestone for the latest status.\\n2.17.4 0.25 (2025-05-04)\\n• New plugin feature:register_fragment_loaders(register)plugins can now return a mixture of fragments and\\nattachments. The llm-video-frames plugin is the first to take advantage of this mechanism. #972\\n• New OpenAI models:gpt-4.1,gpt-4.1-mini, gpt-41-nano,o3,o4-mini. #945, #965, #976.\\n• Newenvironmentvariables: LLM_MODELandLLM_EMBEDDING_MODELforsettingthemodeltousewithoutneed-\\ning to specify-m model_idevery time. #932\\n• New command:llm fragments loaders, to list all currently available fragment loader prefixes provided by\\nplugins. #941\\n2.17. Changelog 153\\nLLM documentation, Release 0.26-31-g0bf655a\\n• llm fragmentscommand now shows fragments ordered by the date they were first used. #973\\n• llm chatnowincludesa !editcommandforeditingapromptusingyourdefaultterminaltexteditor. Thanks,\\nBenedikt Willi. #969\\n• Allow-tand --systemto be used at the same time. #916\\n• Fixed a bug where accessing a model via its alias would fail to respect any default options set for that model.\\n#968\\n• Improved documentation forextra-openai-models.yaml. Thanks, Rahim Nathwani and Dan Guido. #950, #957\\n• llm -c/--continue now works correctly with the-d/--database option. llm chat now accepts that-d/\\n--databaseoption. Thanks, Sukhbinder Singh. #933\\n2.17.5 0.25a0 (2025-04-10)\\n• llm models --options now shows keys and environment variables for models that use API keys. Thanks,\\nSteve Morin. #903\\n• Added py.typed marker file so LLM can now be used as a dependency in projects that usemypy without a\\nwarning. #887\\n• $characters can now be used in templates by escaping them as$$. Thanks, @guspix. #904\\n• LLM now usespyproject.tomlinstead ofsetup.py. #908\\n2.17.6 0.24.2 (2025-04-08)\\n• Fixed a bug on Windows with the newllm -t path/to/file.yamlfeature. #901\\n2.17.7 0.24.1 (2025-04-08)\\n• Templates can now be specified as a path to a file on disk, usingllm -t path/to/file.yaml. This makes\\nthem consistent with how-f fragments are loaded. #897\\n• llm logs backup /tmp/backup.dbcommand forbacking up yourlogs.dbdatabase. #879\\n2.17.8 0.24 (2025-04-07)\\nSupport forfragmentsto help assemble prompts for long context models. Improved support fortemplatesto support\\nattachments and fragments. New plugin hooks for providing custom loaders for both templates and fragments. See\\nLong context support in LLM 0.24 using fragments and template plugins for more on this release.\\nThe new llm-docs plugin demonstrates these new features. Install it like this:\\nllm install llm-docs\\nNow you can ask questions of the LLM documentation like this:\\nllm -f docs: \\'How do I save a new template?\\'\\nThe docs: prefix is registered by the plugin. The plugin fetches the LLM documentation for your installed version\\n(from the docs-for-llms repository) and uses that as a prompt fragment to help answer your question.\\nTwo more new plugins are llm-templates-github and llm-templates-fabric.\\n154 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm-templates-githubletsyoushareandusetemplatesonGitHub. YoucanrunmyPelicanridingabicyclebench-\\nmark against a model like this:\\nllm install llm-templates-github\\nllm -t gh:simonw/pelican-svg -m o3-mini\\nThis executes this pelican-svg.yaml template stored in my simonw/llm-templates repository, using a new repository\\nnaming convention.\\nTo share your own templates, create a repository on GitHub under your user account calledllm-templatesand start\\nsaving.yamlfiles to it.\\nllm-templates-fabric provides a similar mechanism for loading templates from Daniel Miessler’s fabric collection:\\nllm install llm-templates-fabric\\ncurl https://simonwillison.net/2025/Apr/6/only-miffy/ | \\\\\\nllm -t f:extract_main_idea\\nMajor new features:\\n• Newfragmentsfeature. Fragmentscanbeusedtoassemblelongpromptsfrommultipleexistingpieces-URLs,\\nfile paths or previously used fragments. These will be stored de-duplicated in the database avoiding wasting\\nspace storing multiple long context pieces. Example usage:llm -f https://llm.datasette.io/robots.\\ntxt \\'explain this file\\'. #617\\n• The llm logs file now accepts-f fragment references too, and will show just logged prompts that used those\\nfragments.\\n• register_template_loaders()pluginhook allowingpluginstoregisternew prefix:valuecustomtemplateload-\\ners. #809\\n• register_fragment_loaders()pluginhook allowingpluginstoregisternew prefix:valuecustomfragmentload-\\ners. #886\\n• llm fragmentsfamily of commands for browsing fragments that have been previously logged to the database.\\n• Thenewllm-openaipluginprovidessupportfor o1-pro(whichisnotsupportedbytheOpenAImechanismused\\nby LLM core). Future OpenAI features will migrate to this plugin instead of LLM core itself.\\nImprovements to templates:\\n• llm -t $URLoption can now take a URL to a YAML template. #856\\n• Templates can now store default model options. #845\\n• Executing a template that does not use the$inputvariable no longer blocks LLM waiting for input, so prompt\\ntemplates can now be used to try different models usingllm -t pelican-svg -m model_id. #835\\n• llm templatescommand no longer crashes if one of the listed template files contains invalid YAML. #880\\n• Attachments can now be stored in templates. #826\\nOther changes:\\n• Newllm models optionsfamily of commands for setting default options for particular models. #829\\n• llm logs list,llm schemas listandllm schemas showallnowtakea -d/--databaseoptionwithan\\noptionalpathtoaSQLitedatabase. Theyusedtotake -p/--pathbutthatwasinconsistentwithothercommands.\\n-p/--pathstill works but is excluded from--helpand will be removed in a future LLM release. #857\\n• llm logs -e/--expandoption for expanding fragments. #881\\n• llm prompt -d path-to-sqlite.db option can now be used to write logs to a custom SQLite database.\\n#858\\n2.17. Changelog 155\\nLLM documentation, Release 0.26-31-g0bf655a\\n• llm similar -p/--plainoption providing more human-readable output than the default JSON. #853\\n• llm logs -s/--shortnow truncates to include the end of the prompt too. Thanks, Sukhbinder Singh. #759\\n• Setthe LLM_RAISE_ERRORS=1environmentvariabletoraiseerrorsduringpromptsratherthansuppressingthem,\\nwhichmeansyoucanrun python -i -m llm \\'prompt\\' andthendropintoadebuggeronerrorswith import\\npdb; pdb.pm(). #817\\n• Improved –help output forllm embed-multi. #824\\n• llm models -m X option which can be passed multiple times with model IDs to see the details of just those\\nmodels. #825\\n• OpenAI models now accept PDF attachments. #834\\n• llm prompt -q gpt -q 4ooption - pass-q searchtermone or more times to execute a prompt against the\\nfirst model that matches all of those strings - useful for if you can’t remember the full model ID. #841\\n• OpenAIcompatible modelsconfiguredusing extra-openai-models.yamlnowsupport supports_schema:\\ntrue, vision: true and audio: true options. Thanks @adaitche and @giuli007. #819, #843\\n2.17.9 0.24a1 (2025-04-06)\\n• New Fragments feature. #617\\n• register_fragment_loaders()plugin hook. #809\\n2.17.10 0.24a0 (2025-02-28)\\n• Alpha release with experimentalregister_template_loaders()plugin hook. #809\\n2.17.11 0.23 (2025-02-28)\\nSupport forschemas, for getting supported models to output JSON that matches a specified JSON schema. See also\\nStructured data extraction from unstructured content using LLM schemas for background on this feature. #776\\n• New llm prompt --schema \\'{JSON schema goes here} option for specifying a schema that should be\\nused for the output from the model. Theschemas documentationhas more details and a tutorial.\\n• Schemascanalsobedefinedusinga conciseschemaspecification ,forexample llm prompt --schema \\'name,\\nbio, age int\\'. #790\\n• Schemas can also be specified by passing a filename and throughseveral other methods. #780\\n• Newllmschemasfamilyofcommands : llm schemas list,llm schemas show,and llm schemas dslfor\\ndebugging the new concise schema language. #781\\n• Schemas can now be saved to templates usingllm --schema X --save template-name or through modi-\\nfying thetemplate YAML. #778\\n• Thellmlogs commandnowhasnewoptionsforextractingdatacollectedusingschemas: --data,--data-key,\\n--data-array, --data-ids. #782\\n• Newllm logs --id-gt Xand --id-gte Xoptions. #801\\n• Newllm models --schemasoption for listing models that support schemas. #797\\n• model.prompt(..., schema={...}) parameter for specifying a schema from Python. This accepts either a\\ndictionary JSON schema definition or a PydanticBaseModelsubclass, seeschemas in the Python API docs.\\n156 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n• The default OpenAI plugin now enables schemas across all supported models. Runllm models --schemas\\nfor a list of these.\\n• The llm-anthropic and llm-gemini plugins have been upgraded to add schema support for those models. Here’s\\ndocumentation on how toadd schema support to a model plugin.\\nOther smaller changes:\\n• GPT-4.5 preview is now a supported model:llm -m gpt-4.5 \\'a joke about a pelican and a wolf\\'\\n#795\\n• The prompt string is now optional when calling model.prompt() from the Python API, so model.\\nprompt(attachments=llm.Attachment(url=url)))now works. #784\\n• extra-openai-models.yamlnowsupportsa reasoning: true option. Thanks,KasperPrimdalLauritzen.\\n#766\\n• LLM now depends on Pydantic v2 or higher. Pydantic v1 is no longer supported. #520\\n2.17.12 0.22 (2025-02-16)\\nSee also LLM 0.22, the annotated release notes.\\n• Plugins that provide models that use API keys can now subclass the new llm.KeyModel and llm.\\nAsyncKeyModelclasses. ThisresultsintheAPIkeybeingpassedasanew keyparametertotheir .execute()\\nmethods, and means that Python users can pass a key as themodel.prompt(..., key=)- seePassing an API\\nkey. Plugin developers should consult the new documentation on writingModels that accept API keys. #744\\n• New OpenAI model:chatgpt-4o-latest. This model ID accesses the current model being used to power\\nChatGPT, which can change without warning. #752\\n• New llm logs -s/--short flag, which returns a greatly shortened version of the matching log entries in\\nYAML format with a truncated prompt and without including the response. #737\\n• Bothllm modelsandllm embed-modelsnowtakemultiple -qsearchfragments. Youcannowsearchforall\\nmodels matching “gemini” and “exp” usingllm models -q gemini -q exp. #748\\n• New llm embed-multi --prepend X option for prepending a string to each value before it is embed-\\nded - useful for models such as nomic-embed-text-v2-moe that require passages to start with a string like\\n\"search_document: \" . #745\\n• The response.json()and response.usage()methods arenow documented.\\n• Fixed a bug where conversations that were loaded from the database could not be continued usingasyncio\\nprompts. #742\\n• New plugin for macOS users: llm-mlx, which provides extremely high performance access to a wide range of\\nlocal models using Apple’s MLX framework.\\n• The llm-claude-3plugin has been renamed to llm-anthropic.\\n2.17. Changelog 157\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.13 0.21 (2025-01-31)\\n• New model:o3-mini. #728\\n• The o3-mini and o1 models now support areasoning_effort option which can be set tolow, medium or\\nhigh.\\n• llm promptandllm logsnowhavea --xl/--extract-lastoptionforextractingthelastfencedcodeblock\\nin the response - a complement to the existing--x/--extractoption. #717\\n2.17.14 0.20 (2025-01-22)\\n• New model,o1. This model does not yet support streaming. #676\\n• o1-previewand o1-minimodels now support streaming.\\n• New models,gpt-4o-audio-previewand gpt-4o-mini-audio-preview. #677\\n• llm prompt -x/--extractoption,whichreturnsjustthecontentofthefirstfencedcodeblockintheresponse.\\nTryllm prompt -x \\'Python function to reverse a string\\'. #681\\n– Creatingatemplateusing llm ... --save xnowsupportsthe -x/--extractoption,whichissavedto\\nthe template. YAML templates can set this option usingextract: true .\\n– New llm logs -x/--extract option extracts the first fenced code block from matching logged re-\\nsponses.\\n• Newllm models -q \\'search\\' option returning models that case-insensitively match the search query. #700\\n• Installation documentation now also includesuv. Thanks, Ariel Marcus. #690 and #702\\n• llm models command now shows the current default model at the bottom of the listing. Thanks, Amjith Ra-\\nmanujam. #688\\n• Plugin directorynow includesllm-venice,llm-bedrock, llm-deepseekand llm-cmd-comp.\\n• Fixed bug where some dependency version combinations could cause a Client.__init__() got an\\nunexpected keyword argument \\'proxies\\' error. #709\\n• OpenAI embedding models are now available using their full names of text-embedding-ada-002,\\ntext-embedding-3-smallandtext-embedding-3-large-thepreviousnamesarestillsupportedasaliases.\\nThanks, web-sst. #654\\n2.17.15 0.19.1 (2024-12-05)\\n• FIxedbugwhere llm.get_models()andllm.get_async_models()returnedthesamemodelmultipletimes.\\n#667\\n2.17.16 0.19 (2024-12-01)\\n• Tokens used by a response are now logged to newinput_tokens and output_tokens integer columns and\\na token_details JSON string column, for the default OpenAI models and models from other plugins that\\nimplement this feature. #610\\n• llm promptnow takes a-u/--usageflag to display token usage at the end of the response.\\n• llm logs -u/--usageshows token usage information for logged responses.\\n• llm prompt ... --asyncresponses are now logged to the database. #641\\n158 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n• llm.get_models()and llm.get_async_models()functions,documented here. #640\\n• response.usage() and async responseawait response.usage() methods, returning aUsage(input=2,\\noutput=1, details=None)dataclass. #644\\n• response.on_done(callback)and await response.on_done(callback)methodsforspecifying acall-\\nback to be executed when a response has completed,documented here. #653\\n• Fix for bug runningllm chaton Windows 11. Thanks, Sukhbinder Singh. #495\\n2.17.17 0.19a2 (2024-11-20)\\n• llm.get_models()and llm.get_async_models()functions,documented here. #640\\n2.17.18 0.19a1 (2024-11-19)\\n• response.usage() and async responseawait response.usage() methods, returning aUsage(input=2,\\noutput=1, details=None)dataclass. #644\\n2.17.19 0.19a0 (2024-11-19)\\n• Tokens used by a response are now logged to newinput_tokens and output_tokens integer columns and\\na token_details JSON string column, for the default OpenAI models and models from other plugins that\\nimplement this feature. #610\\n• llm promptnow takes a-u/--usageflag to display token usage at the end of the response.\\n• llm logs -u/--usageshows token usage information for logged responses.\\n• llm prompt ... --asyncresponses are now logged to the database. #641\\n2.17.20 0.18 (2024-11-17)\\n• Initial support for async models. Plugins can now provide anAsyncModelsubclass that can be accessed in the\\nPython API using the newllm.get_async_model(model_id) method. Seeasync models in the Python API\\ndocs andimplementing async models in plugins. #507\\n• OpenAI models all now include async models, so function calls such as llm.\\nget_async_model(\"gpt-4o-mini\")will return an async model.\\n• gpt-4o-audio-previewmodel can be used to send audio attachments to the GPT-4o audio model. #608\\n• Attachments can now be sent without requiring a prompt. #611\\n• llm models --optionsnow includes information on whether a model supports attachments. #612\\n• llm models --asyncshows available async models.\\n• Custom OpenAI-compatible models can now be marked ascan_stream: false in the YAML if they do not\\nsupport streaming. Thanks, Chris Mungall. #600\\n• Fixed bug where OpenAI usage data was incorrectly serialized to JSON. #614\\n• Standardized onaudio/wavMIME type for audio attachments rather thanaudio/wave. #603\\n2.17. Changelog 159\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.21 0.18a1 (2024-11-14)\\n• Fixed bug where conversations did not work for async OpenAI models. #632\\n• __repr__methods forResponseand AsyncResponse.\\n2.17.22 0.18a0 (2024-11-13)\\nAlpha support forasync models. #507\\nMultiple smaller changes.\\n2.17.23 0.17 (2024-10-29)\\nSupport forattachments, allowing multi-modal models to accept images, audio, video and other formats. #578\\nThe default OpenAIgpt-4oand gpt-4o-minimodels can both now be prompted with JPEG, GIF, PNG and WEBP\\nimages.\\nAttachmentsin the CLIcan be URLs:\\nllm -m gpt-4o \"describe this image\" \\\\\\n-a https://static.simonwillison.net/static/2024/pelicans.jpg\\nOr file paths:\\nllm -m gpt-4o-mini \"extract text\" -a image1.jpg -a image2.jpg\\nOr binary data, which may need to use--attachment-typeto specify the MIME type:\\ncat image | llm -m gpt-4o-mini \"extract text\" --attachment-type - image/jpeg\\nAttachments are also availablein the Python API:\\nmodel = llm.get_model(\"gpt-4o-mini\")\\nresponse = model.prompt(\\n\"Describe these images\",\\nattachments=[\\nllm.Attachment(path=\"pelican.jpg\"),\\nllm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\"),\\n]\\n)\\nPlugins that provide alternative models can support attachments, seeAttachments for multi-modal modelsfor details.\\nThe latestllm-claude-3 plugin now supports attachments for Anthropic’s Claude 3 and 3.5 models. Thellm-gemini\\nplugin supports attachments for Google’s Gemini 1.5 models.\\nAlso in this release: OpenAI models now record their\"usage\" data in the database even when the response was\\nstreamed. These records can be viewed usingllm logs --json. #591\\n160 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.24 0.17a0 (2024-10-28)\\nAlpha support forattachments. #578\\n2.17.25 0.16 (2024-09-12)\\n• OpenAImodelsnowusetheinternal self.get_key()mechanism,whichmeanstheycanbeusedfromPython\\ncode in a way that will pick up keys that have been configured usingllm keys set or theOPENAI_API_KEY\\nenvironment variable. #552. This code now works correctly:\\nimport llm\\nprint(llm.get_model(\"gpt-4o-mini\").prompt(\"hi\"))\\n• New documented API methods: llm.get_default_model(), llm.set_default_model(alias), llm.\\nget_default_embedding_model(alias), llm.set_default_embedding_model(). #553\\n• Support for OpenAI’s new o1 family of preview models, llm -m o1-preview \"prompt\" and llm -m\\no1-mini \"prompt\". These models are currently only available to tier 5 OpenAI API users, though this may\\nchange in the future. #570\\n2.17.26 0.15 (2024-07-18)\\n• Support for OpenAI’s new GPT-4o mini model: llm -m gpt-4o-mini \\'rave about pelicans in\\nFrench\\' #536\\n• gpt-4o-miniisnowthedefaultmodelifyoudonot specifyyourowndefault ,replacingGPT-3.5Turbo. GPT-4o\\nmini is both cheaper and better than GPT-3.5 Turbo.\\n• Fixedabugwhere llm logs -q \\'flourish\\' -m haikucouldnotcombineboththe -qsearchqueryandthe\\n-mmodel specifier. #515\\n2.17.27 0.14 (2024-05-13)\\n• Support for OpenAI’s new GPT-4o model:llm -m gpt-4o \\'say hi in Spanish\\' #490\\n• The gpt-4-turbo alias is now a model ID, which indicates the latest version of OpenAI’s GPT-4 Turbo\\ntext and image model. Your existinglogs.db database may contain records under the previous model ID of\\ngpt-4-turbo-preview. #493\\n• New llm logs -r/--response option for outputting just the last captured response, without wrapping it in\\nMarkdown and accompanying it with the prompt. #431\\n• Nine newplugins since version 0.13:\\n– llm-claude-3 supporting Anthropic’s Claude 3 family of models.\\n– llm-command-r supporting Cohere’s Command R and Command R Plus API models.\\n– llm-rekasupports the Reka family of models via their API.\\n– llm-perplexity by Alexandru Geana supporting the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-onlinewhichcansearchforthingsonlineand llama-3-70b-instruct.\\n– llm-groqby Moritz Angermann providing access to fast models hosted by Groq.\\n– llm-fireworkssupporting models hosted by Fireworks AI.\\n– llm-togetheradds support for the Together AI extensive family of hosted openly licensed models.\\n2.17. Changelog 161\\nLLM documentation, Release 0.26-31-g0bf655a\\n– llm-embed-onnx provides seven embedding models that can be executed using the ONNX model frame-\\nwork.\\n– llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so\\nyou can review it, edit it and then hit<enter>to execute orctrl+cto cancel, see this post for details.\\n2.17.28 0.13.1 (2024-01-26)\\n• Fix forNo module named \\'readline\\' error on Windows. #407\\n2.17.29 0.13 (2024-01-26)\\nSee also LLM 0.13: The annotated release notes.\\n• Added support for new OpenAI embedding models:3-small and 3-large and three variants of those with\\ndifferent dimension sizes,3-small-512, 3-large-256 and 3-large-1024. See OpenAI embedding models\\nfor details. #394\\n• Thedefault gpt-4-turbomodelaliasnowpointsto gpt-4-turbo-preview,whichusesthemostrecentOpe-\\nnAI GPT-4 turbo model (currentlygpt-4-0125-preview). #396\\n• New OpenAI model aliasesgpt-4-1106-previewand gpt-4-0125-preview.\\n• OpenAI models now support a-o json_object 1 option which will cause their output to be returned as a\\nvalid JSON object. #373\\n• Newplugins since the last release include llm-mistral, llm-gemini, llm-ollama and llm-bedrock-meta.\\n• The keys.jsonfile for storing API keys is now created with600file permissions. #351\\n• Documentedapattern forinstallingpluginsthatdependonPyTorchusingtheHomebrewversionofLLM,despite\\nHomebrewusingPython3.12whenPyTorchhavenotyetreleasedastablepackageforthatPythonversion. #397\\n• Underlying OpenAI Python library has been upgraded to>1.0. It is possible this could cause compatibility\\nissues with LLM plugins that also depend on that library. #325\\n• Arrow keys now work inside thellm chatcommand. #376\\n• LLM_OPENAI_SHOW_RESPONSES=1 environment variable now outputs much more detailed information about\\nthe HTTP request and response made to OpenAI (and OpenAI-compatible) APIs. #404\\n• Dropped support for Python 3.7.\\n2.17.30 0.12 (2023-11-06)\\n• SupportforthenewGPT-4TurbomodelfromOpenAI.Tryitusing llm chat -m gpt-4-turboor llm chat\\n-m 4t. #323\\n• New-o seed 1optionforOpenAImodelswhichsetsaseedthatcanattempttoevaluatethepromptdetermin-\\nistically. #324\\n162 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.31 0.11.2 (2023-11-06)\\n• Pin to version of OpenAI Python library prior to 1.0 to avoid breaking. #327\\n2.17.32 0.11.1 (2023-10-31)\\n• Fixed a bug wherellm embed -c \"text\" did not correctly pick up the configureddefault embedding model.\\n#317\\n• New plugins: llm-python, llm-bedrock-anthropic and llm-embed-jina (described in Execute Jina embeddings\\nwith a CLI using llm-embed-jina).\\n• llm-gpt4all now uses the new GGUF model format. simonw/llm-gpt4all#16\\n2.17.33 0.11 (2023-09-18)\\nLLMnowsupportsthenewOpenAI gpt-3.5-turbo-instructmodel,andOpenAIcompletion(asopposedtochat\\ncompletion) models in general. #284\\nllm -m gpt-3.5-turbo-instruct \\'Reasons to tame a wild beaver:\\'\\nOpenAI completion models like this support a-o logprobs 3option, which accepts a number between 1 and 5 and\\nwill include the log probabilities (for each produced token, what were the top 3 options considered by the model) in\\nthe logged response.\\nllm -m gpt-3.5-turbo-instruct \\'Say hello succinctly\\' -o logprobs 3\\nYou can then view thelogprobsthat were recorded in the SQLite logs database like this:\\nsqlite-utils \"$(llm logs path)\" \\\\\\n\\'select * from responses order by id desc limit 1\\' | \\\\\\njq \\'.[0].response_json\\' -r | jq\\nTruncated output looks like this:\\n[\\n{\\n\"text\": \"Hi\",\\n\"top_logprobs\": [\\n{\\n\"Hi\": -0.13706253,\\n\"Hello\": -2.3714375,\\n\"Hey\": -3.3714373\\n}\\n]\\n},\\n{\\n\"text\": \" there\",\\n\"top_logprobs\": [\\n{\\n\" there\": -0.96057636,\\n\"!\\\\\"\": -0.5855763,\\n\".\\\\\"\": -3.2574513\\n(continues on next page)\\n2.17. Changelog 163\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\n}\\n]\\n}\\n]\\nAlso in this release:\\n• The llm.user_dir()function, used by plugins, now ensures the directory exists before returning it. #275\\n• New LLM_OPENAI_SHOW_RESPONSES=1 environment variable for displaying the full HTTP response returned\\nby OpenAI compatible APIs. #286\\n• The llm embed-multi command now has a--batch-size X option for setting the batch size to use when\\nprocessing embeddings - useful if you have limited memory available. #273\\n• The collection.embed_multi()method also now accepts an optionalbatch_size=intargument.\\n• Fixed two bugs withllm embed-multi --filesrelating to handling of directories. Thanks, ealvar3z. #274,\\n#280\\n2.17.34 0.10 (2023-09-12)\\nThe two major features in this release are thellm chat commandand support forembedding binary data.\\nSeeBuildanimagesearchenginewithllm-clip,chatwithmodelswithllmchatformorebackgroundonthesefeatures.\\nllm chat\\nThe newllm chat command starts an ongoing chat conversation with a model in your terminal. It works with all\\nmodels supported by LLM and itsplugins, including locally installed models such as Llama 2. #231\\nThis offers a big performance boost for local models, since they don’t need to be freshly loaded into memory for each\\nprompt.\\nHere’s an example chat with Llama 2 13B, provided by the llm-mlc plugin.\\nllm chat -m mlc-chat-Llama-2-13b-chat-hf-q4f16_1\\nThis starts a chat session:\\nType \\'exit\\' or \\'quit\\' to exit\\nType \\'!multi\\' to enter multiple lines, then \\'!end\\' to finish\\nType \\'!edit\\' to open your default editor and modify the prompt.\\n> Who are you?\\nHello! I\\'m just an AI, here to assist you with any questions you may have.\\nMy name is LLaMA, and I\\'m a large language model trained to provide helpful\\nand informative responses to a wide range of questions and topics. I\\'m here\\nto help you find the information you need, while ensuring a safe and\\nrespectful conversation. Please feel free to ask me anything, and I\\'ll do my\\nbest to provide a helpful and accurate response.\\n> Tell me a joke about otters\\nSure, here\\'s a joke about otters:\\nWhy did the otter go to the party?\\n(continues on next page)\\n164 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nBecause he heard it was a \"whale\" of a time!\\n(Get it? Whale, like a big sea mammal, but also a \"wild\" or \"fun\" time.\\nOtters are known for their playful and social nature, so it\\'s a lighthearted\\nand silly joke.)\\nI hope that brought a smile to your face! Do you have any other questions or\\ntopics you\\'d like to discuss?\\n> exit\\nChat sessions arelogged to SQLite- usellm logs to view them. They can accept system prompts, templates and\\nmodel options - consultthe chat documentationfor details.\\nBinary embedding support\\nLLM’sembeddings featurehas been expanded to provide support for embedding binary data, in addition to text. #254\\nThis enables models like CLIP, supported by the newllm-clipplugin.\\nCLIP is a multi-modal embedding model which can embed images and text into the same vector space. This means\\nyoucanuseittocreateanembeddingindexofphotos,andthensearchfortheembeddingvectorfor“ahappydog”and\\nget back images that are semantically closest to that string.\\nTo create embeddings for every JPEG in a directory stored in aphotoscollection, run:\\nllm install llm-clip\\nllm embed-multi photos --files photos/ \\'*.jpg\\' --binary -m clip\\nNow you can search for photos of raccoons using:\\nllm similar photos -c \\'raccoon\\'\\nThis spits out a list of images, ranked by how similar they are to the string “raccoon”:\\n{\"id\": \"IMG_4801.jpeg\", \"score\": 0.28125139257127457, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_4656.jpeg\", \"score\": 0.26626441704164294, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_2944.jpeg\", \"score\": 0.2647445926996852, \"content\": null, \"metadata\": null}\\n...\\nAlso in this release\\n• The LLM_LOAD_PLUGINS environment variablecan be used to control which plugins are loaded whenllm\\nstarts running. #256\\n• The llm plugins --alloption includes builtin plugins in the list of plugins. #259\\n• The llm embed-dbfamily of commands has been renamed tollm collections. #229\\n• llm embed-multi --files now has an--encoding option and defaults to falling back tolatin-1 if a file\\ncannot be processed asutf-8. #225\\n2.17. Changelog 165\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.35 0.10a1 (2023-09-11)\\n• Support for embedding binary data. #254\\n• llm chatnow works for models with API keys. #247\\n• llm chat -ofor passing options to a model. #244\\n• llm chat --no-streamoption. #248\\n• LLM_LOAD_PLUGINSenvironment variable. #256\\n• llm plugins --alloption for including builtin plugins. #259\\n• llm embed-dbhas been renamed tollm collections. #229\\n• Fixed bug wherellm embed -coption was treated as a filepath, not a string. Thanks, mhalle. #263\\n2.17.36 0.10a0 (2023-09-04)\\n• Newllm chatcommand for starting an interactive terminal chat with a model. #231\\n• llm embed-multi --files now has an--encoding option and defaults to falling back tolatin-1 if a file\\ncannot be processed asutf-8. #225\\n2.17.37 0.9 (2023-09-03)\\nThe big new feature in this release is support forembeddings. See LLM now provides tools for working with embed-\\ndings for additional details.\\nEmbedding modelstake a piece of text - a word, sentence, paragraph or even a whole article, and convert that into an\\narray of floating point numbers. #185\\nThis embedding vector can be thought of as representing a position in many-dimensional-space, where the distance\\nbetweentwovectorsrepresentshowsemanticallysimilartheyaretoeachotherwithinthecontentofalanguagemodel.\\nEmbeddingscanbeusedtofind relateddocuments,andalsotoimplement semanticsearch -whereausercansearch\\nforaphraseandgetbackresultsthataresemanticallysimilartothatphraseeveniftheydonotshareanyexactkeywords.\\nLLM now provides both CLI and Python APIs for working with embeddings. Embedding models are defined by\\nplugins, so you can install additional models using theplugins mechanism.\\nThe first two embedding models supported by LLM are:\\n• OpenAI’sada-002embeddingmodel,availableviaaninexpensiveAPIifyousetanOpenAIkeyusing llm keys\\nset openai.\\n• The sentence-transformers family of models, available via the new llm-sentence-transformers plugin.\\nSee Embedding with the CLIfor detailed instructions on working with embeddings using LLM.\\nThe new commands for working with embeddings are:\\n• llm embed-calculateembeddingsforcontentandreturnthemtotheconsoleorstoretheminaSQLitedatabase.\\n• llm embed-multi- run bulk embeddings for multiple strings, using input from a CSV, TSV or JSON file, data\\nfrom a SQLite database or data found by scanning the filesystem. #215\\n• llm similar- run similarity searches against your stored embeddings - starting with a search phrase or finding\\ncontent related to a previously stored vector. #190\\n• llm embed-models- list available embedding models.\\n166 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n• llm embed-db- commands for inspecting and working with the default embeddings SQLite database.\\nThere’s also a newllm.Collection class for creating and searching collections of embedding from Python code, and a\\nllm.get_embedding_model()interface for embedding strings directly. #191\\n2.17.38 0.8.1 (2023-08-31)\\n• Fixedbugwherefirstpromptwouldshowanerrorifthe io.datasette.llmdirectoryhadnotyetbeencreated.\\n#193\\n• Updateddocumentationtorecommendadifferent llm-gpt4allmodelsincetheonewewereusingisnolonger\\navailable. #195\\n2.17.39 0.8 (2023-08-20)\\n• The output format forllm logs has changed. Previously it was JSON - it’s now a much more readable Mark-\\ndown format suitable for pasting into other documents. #160\\n– The newllm logs --jsonoption can be used to get the old JSON format.\\n– Passllm logs --conversation IDor --cid IDto see the full logs for a specific conversation.\\n• You can now combine piped input and a prompt in a single command:cat script.py | llm \\'explain\\nthis code\\'. This works even for models that do not supportsystem prompts. #153\\n• Additional OpenAI-compatible modelscan now be configured with custom HTTP headers. This enables plat-\\nforms such as openrouter.ai to be used with LLM, which can provide Claude access even without an Anthropic\\nAPI key.\\n• Keys set inkeys.jsonare now used in preference to environment variables. #158\\n• The documentation now includes aplugin directorylisting all available plugins for LLM. #173\\n• Newrelated toolssection in the documentation describingttok,strip-tagsand symbex. #111\\n• The llm models, llm aliases and llm templates commands now default to running the same command\\nasllm models listand llm aliases listand llm templates list. #167\\n• Newllm keys(aka llm keys list) command for listing the names of all configured keys. #174\\n• Two new Python API functions,llm.set_alias(alias, model_id)and llm.remove_alias(alias)can\\nbe used to configure aliases from within Python code. #154\\n• LLM is now compatible with both Pydantic 1 and Pydantic 2. This means you can installllm as a Python\\ndependency in a project that depends on Pydantic 1 without running into dependency conflicts. Thanks, Chris\\nMungall. #147\\n• llm.get_model(model_id)is now documented as raisingllm.UnknownModelErrorif the requested model\\ndoes not exist. #155\\n2.17. Changelog 167\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.40 0.7.1 (2023-08-19)\\n• Fixedabugwheresomeuserswouldseean AlterError: No such column: log.id errorwhenattempt-\\ning to use this tool, after upgrading to the latest sqlite-utils 3.35 release. #162\\n2.17.41 0.7 (2023-08-12)\\nThe newModel aliasescommands can be used to configure additional aliases for models, for example:\\nllm aliases set turbo gpt-3.5-turbo-16k\\nNow you can run the 16,000 tokengpt-3.5-turbo-16kmodel like this:\\nllm -m turbo \\'An epic Greek-style saga about a cheesecake that builds a SQL database␣\\n˓→from scratch\\'\\nUse llm aliases listto see a list of aliases andllm aliases remove turboto remove one again. #151\\nNotable new plugins\\n• llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n• llm-llama-cpp uses llama.cpp to run models published in the GGML format. See Run Llama 2 on your own\\nMac using LLM and Homebrew for more details.\\nAlso in this release\\n• OpenAI models now have min and max validation on their floating point options. Thanks, Pavel Král. #115\\n• Fix for bug wherellm templates listraised an error if a template had an empty prompt. Thanks, Sherwin\\nDaganato. #132\\n• Fixed bug inllm install --editableoption which prevented installation of.[test]. #136\\n• llm install --no-cache-dirand --force-reinstalloptions. #146\\n2.17.42 0.6.1 (2023-07-24)\\n• LLM can now be installed directly from Homebrew core:brew install llm. #124\\n• Python API documentation now coversSystem prompts.\\n• Fixed incorrect example in theTemplatesdocumentation. Thanks, Jorge Cabello. #125\\n168 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.43 0.6 (2023-07-18)\\n• ModelshostedonReplicatecannowbeaccessedusingthellm-replicateplugin,includingthenewLlama2model\\nfrom Meta AI. More details here: Accessing Llama 2 from the command-line with the llm-replicate plugin.\\n• ModelprovidersthatexposeanAPIthatiscompatiblewiththeOpenAPIAPIformat,includingself-hostedmodel\\nserverssuchasLocalAI,cannowbeaccessedusing additionalconfiguration forthedefaultOpenAIplugin. #106\\n• OpenAI models that are not yet supported by LLM can also be configured using the new\\nextra-openai-models.yamlconfiguration file. #107\\n• Thellmlogscommand nowacceptsa -m model_idoptiontofilterlogstoaspecificmodel. Aliasescanbeused\\nhere in addition to model IDs. #108\\n• Logs now have a SQLite full-text search index against their prompts and responses, and thellm logs -q\\nSEARCHoption can be used to return logs that match a search term. #109\\n2.17.44 0.5 (2023-07-12)\\nLLMnowsupports additionallanguagemodels ,thankstoanew pluginsmechanism forinstallingadditionalmodels.\\nPlugins are available for 19 models in addition to the default OpenAI ones:\\n• llm-gpt4alladdssupportfor17modelsthatcandownloadandrunonyourowndevice,includingVicuna,Falcon\\nand wizardLM.\\n• llm-mpt30b adds support for the MPT-30B model, a 19GB download.\\n• llm-palm adds support for Google’s PaLM 2 via the Google API.\\nA comprehensive tutorial,writing a plugin to support a new modeldescribes how to add new models by building\\nplugins in detail.\\nNew features\\n• Python APIdocumentation for using LLM models, including models from plugins, directly from Python. #75\\n• Messages are now logged to the database by default - no need to run thellm init-db command any more,\\nwhich has been removed. Instead, you can toggle this behavior off usingllm logs off or turn it on again\\nusingllm logs on. Thellm logs statuscommandshowsthecurrentstatusofthelogdatabase. Iflogging\\nis turned off, passing--logto thellm promptcommand will cause that prompt to be logged anyway. #98\\n• Newdatabaseschemaforloggedmessages,with conversationsandresponsestables. Ifyouhavepreviously\\nused the oldlogstable it will continue to exist but will no longer be written to. #91\\n• New-o/--option name valuesyntax for setting options for models, such as temperature. Available options\\ndiffer for different models. #63\\n• llm models list --optionscommand for viewing all available model options. #82\\n• llm \"prompt\" --save templateoption for saving a prompt directly to a template. #55\\n• Prompt templates can now specifydefault valuesfor parameters. Thanks, Chris Mungall. #57\\n• llm openai modelscommand to list all available OpenAI models from their API. #70\\n• llm models default MODEL_IDtosetadifferentmodelasthedefaulttobeusedwhen llmisrunwithoutthe\\n-m/--modeloption. #31\\n2.17. Changelog 169\\nLLM documentation, Release 0.26-31-g0bf655a\\nSmaller improvements\\n• llm -sis now a shortcut forllm --system. #69\\n• llm -m 4-32kalias forgpt-4-32k.\\n• llm install -e directorycommand for installing a plugin from a local directory.\\n• The LLM_USER_PATH environment variable now controls the location of the directory in which LLM stores its\\ndata. This replaces the oldLLM_KEYS_PATHand LLM_LOG_PATHand LLM_TEMPLATES_PATHvariables. #76\\n• Documentation coveringUtility functions for plugins.\\n• Documentation site now uses Plausible for analytics. #79\\n2.17.45 0.4.1 (2023-06-17)\\n• LLM can now be installed using Homebrew:brew install simonw/llm/llm. #50\\n• llmis now styled LLM in the documentation. #45\\n• Examples in documentation now include a copy button. #43\\n• llm templatescommand no longer has its display disrupted by newlines. #42\\n• llm templatescommand now includes system prompt, if set. #44\\n2.17.46 0.4 (2023-06-17)\\nThis release includes some backwards-incompatible changes:\\n• The -4option for GPT-4 is now-m 4.\\n• The --codeoption has been removed.\\n• The -soption has been removed as streaming is now the default. Use--no-streamto opt out of streaming.\\nPrompt templates\\nTemplatesis a new feature that allows prompts to be saved as templates and re-used with different variables.\\nTemplates can be created using thellm templates editcommand:\\nllm templates edit summarize\\nTemplates are YAML - the following template defines summarization using a system prompt:\\nsystem: Summarize this text\\nThe template can then be executed like this:\\ncat myfile.txt | llm -t summarize\\nTemplatescanincludebothsystemprompts,regularpromptsandindicatethemodeltheyshoulduse. Theycanreference\\nvariables such as$input for content piped to the tool, or other variables that are passed using the new-p/--param\\noption.\\nThis example adds avoiceparameter:\\n170 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nsystem: Summarize this text in the voice of $voice\\nThen to run it (via strip-tags to remove HTML tags from the input):\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio\\' | \\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nExample output:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\nThe Templatesdocumentation provides more detailed examples.\\nContinue previous chat\\nYoucannowuse llmtocontinueapreviousconversationwiththeOpenAIchatmodels( gpt-3.5-turboandgpt-4).\\nThis will include your previous prompts and responses in the prompt sent to the API, allowing the model to continue\\nwithin the same context.\\nUse the new-c/--continueoption to continue from the previous message thread:\\nllm \"Pretend to be a witty gerbil, say hi briefly\"\\nGreetings, dear human! Iam aclever gerbil, ready toentertain you withmy quick witand endless energy.\\nllm \"What do you think of snacks?\" -c\\nOh, how I adore snacks, dear human! Crunchy carrot sticks, sweet apple slices, and chewy yogurt drops\\nare some of my favorite treats. I could nibble on them all day long!\\nThe -coption will continue from the most recent logged message.\\nTo continue a different chat, pass an integer ID to the--chat option. This should be the ID of a previously logged\\nmessage. You can find these IDs using thellm logscommand.\\nThanks Amjith Ramanujam for contributing to this feature. #6\\nNew mechanism for storing API keys\\nAPIkeysforlanguagemodelssuchasthosebyOpenAIcannowbesavedusingthenew llm keysfamilyofcommands.\\nTo set the default key to be used for the OpenAI APIs, run this:\\nllm keys set openai\\nThen paste in your API key.\\nKeyscanalsobepassedusingthenew --keycommandlineoption-thiscanbeafullkeyorthealiasofakeythathas\\nbeen previously stored.\\nSee API key managementfor more. #13\\n2.17. Changelog 171\\nLLM documentation, Release 0.26-31-g0bf655a\\nNew location for the logs.db database\\nThe logs.db database that stores a history of executed prompts no longer lives at~/.llm/log.db - it can now be\\nfound in a location that better fits the host operating system, which can be seen using:\\nllm logs path\\nOn macOS this is~/Library/Application Support/io.datasette.llm/logs.db.\\nTo open that database using Datasette, run this:\\ndatasette \"$(llm logs path)\"\\nYou can upgrade your existing installation by copying your database to the new location like this:\\ncp ~/.llm/log.db \"$(llm logs path)\"\\nrm -rf ~/.llm # To tidy up the now obsolete directory\\nThe database schema has changed, and will be updated automatically the first time you run the command.\\nThat schema is included in the documentation. #35\\nOther changes\\n• Newllm logs --truncateoption(shortcut -t)whichtruncatesthedisplayedpromptstomakethelogoutput\\neasier to read. #16\\n• Documentation now spans multiple pages and lives at https://llm.datasette.io/ #21\\n• Default llm chatgptcommand has been renamed tollm prompt. #17\\n• Removed--codeoption in favour of new prompt templates mechanism. #24\\n• Responses are now streamed by default, if the model supports streaming. The-s/--stream option has been\\nremoved. A new--no-streamoption can be used to opt-out of streaming. #25\\n• The -4/--gpt4 option has been removed in favour of-m 4 or -m gpt4, using a new mechanism that allows\\nmodels to have additional short names.\\n• The newgpt-3.5-turbo-16k model with a 16,000 token context length can now also be accessed using-m\\nchatgpt-16kor -m 3.5-16k. Thanks, Benjamin Kirkbride. #37\\n• Improved display of error messages from OpenAI. #15\\n2.17.47 0.3 (2023-05-17)\\n• llm logscommand for browsing logs of previously executed completions. #3\\n• llm \"Python code to output factorial 10\" --code option which sets a system prompt designed to\\nencourage code to be output without any additional explanatory text. #5\\n• Tool can now accept a prompt piped directly to standard input. #11\\n172 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n2.17.48 0.2 (2023-04-01)\\n• If a SQLite database exists in~/.llm/log.db all prompts and responses are logged to that file. Thellm\\ninit-dbcommand can be used to create this file. #2\\n2.17.49 0.1 (2023-04-01)\\n• Initial prototype release. #1\\n2.17. Changelog 173\\nLLM documentation, Release 0.26-31-g0bf655a\\n174 Chapter 2. Contents\\nINDEX\\nT\\nTemplate(class in llm), 85\\nTool(class in llm), 102\\nToolCall(class in llm), 102\\nToolResult(class in llm), 102\\n175']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80e87855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dfffd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter1 = RecursiveCharacterTextSplitter(chunk_size =100, chunk_overlap = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b290063",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = text_splitter1.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e5bc547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LLM documentation\\nRelease 0.26-31-g0bf655a\\nSimon Willison\\nJun 20, 2025',\n",
       " 'CONTENTS\\n1 Quick start 3\\n2 Contents 5',\n",
       " '2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 5',\n",
       " '2.1.1 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 5',\n",
       " '2.1.2 Upgrading to the latest version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 5',\n",
       " '2.1.3 Using uvx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 6',\n",
       " '2.1.4 A note about Homebrew and PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6',\n",
       " '2.1.5 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 6',\n",
       " '2.1.6 API key management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 7',\n",
       " '2.1.7 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 8',\n",
       " '2.2 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 9',\n",
       " '2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 9',\n",
       " '2.2.2 Starting an interactive chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 16',\n",
       " '2.2.3 Listing available models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 18',\n",
       " '2.2.4 Setting default options for models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 30',\n",
       " '2.3 OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . 30',\n",
       " '2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 30',\n",
       " '2.3.2 OpenAI language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 31',\n",
       " '2.3.3 Model features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 31',\n",
       " '2.3.4 OpenAI embedding models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '32',\n",
       " '2.3.5 OpenAI completion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 32',\n",
       " '2.3.6 Adding more OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '32',\n",
       " '2.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 33',\n",
       " '2.4.1 Installing and using a local model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 33',\n",
       " '2.4.2 OpenAI-compatible models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 34',\n",
       " '2.5 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 35',\n",
       " '2.5.1 How tools work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 35',\n",
       " '2.5.2 Trying out tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 36',\n",
       " '2.5.3 LLM’s implementation of tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 36',\n",
       " '2.5.4 Default tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 37',\n",
       " '2.5.5 Tips for implementing tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 37',\n",
       " '2.6 Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . 37',\n",
       " '2.6.1 Schemas tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 37',\n",
       " '2.6.2 Using JSON schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 45',\n",
       " '2.6.3 Ways to specify a schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 46',\n",
       " '2.6.4 Concise LLM schema syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '46',\n",
       " '2.6.5 Saving reusable schemas in templates . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 47',\n",
       " '2.6.6 Browsing logged JSON objects created using schemas . . . . . . . . . . . . . . . . . . . . 48',\n",
       " '2.7 Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 49',\n",
       " 'i',\n",
       " '2.7.1 Getting started with –save . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 50',\n",
       " '2.7.2 Using a template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 51',\n",
       " '2.7.3 Listing available templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 51',\n",
       " '2.7.4 Templates as YAML files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 51',\n",
       " '2.7.5 Template loaders from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 56',\n",
       " '2.8 Fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . 56',\n",
       " '2.8.1 Using fragments in a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 57',\n",
       " '2.8.2 Using fragments in chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 57',\n",
       " '2.8.3 Browsing fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 58',\n",
       " '2.8.4 Setting aliases for fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 58',\n",
       " '2.8.5 Viewing fragments in your logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 59',\n",
       " '2.8.6 Using fragments from plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 59',\n",
       " '2.8.7 Listing available fragment prefixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 60',\n",
       " '2.9 Model aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 60',\n",
       " '2.9.1 Listing aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 60',\n",
       " '2.9.2 Adding a new alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 61',\n",
       " '2.9.3 Removing an alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 62',\n",
       " '2.9.4 Viewing the aliases file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 62',\n",
       " '2.10 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . 62',\n",
       " '2.10.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 62',\n",
       " '2.10.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '71',\n",
       " '2.10.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '74',\n",
       " '2.10.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 76',\n",
       " '2.11 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . . 76',\n",
       " '2.11.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 76',\n",
       " '2.11.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 78',\n",
       " '2.11.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 81',\n",
       " '2.11.4 Developing a model plugin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 87',\n",
       " '2.11.5 Advanced model plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. 99',\n",
       " '2.11.6 Utility functions for plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 105',\n",
       " '2.12 Python API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 107',\n",
       " '2.12.1 Basic prompt execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 107',\n",
       " '2.12.2 Async models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 116',\n",
       " '2.12.3 Conversations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 117',\n",
       " '2.12.4 Listing models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 118',\n",
       " '2.12.5 Running code when a response has completed . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '119',\n",
       " '2.12.6 Other functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 120',\n",
       " '2.13 Logging to SQLite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 121',\n",
       " '2.13.1 Viewing the logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 122',\n",
       " '2.13.2 Browsing logs using Datasette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 125',\n",
       " '2.13.3 Backing up your database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 125',\n",
       " '2.13.4 SQL schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 126',\n",
       " '2.14 Related tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . . 128',\n",
       " '2.14.1 strip-tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 128',\n",
       " '2.14.2 ttok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 128',\n",
       " '2.14.3 Symbex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 129',\n",
       " '2.15 CLI reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . . 129',\n",
       " '2.15.1 llm –help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 129',\n",
       " '2.16 Contributing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 149',\n",
       " '2.16.1 Updating recorded HTTP API interactions and associated snapshots . . . . . . . . . . . . .',\n",
       " '150',\n",
       " '2.16.2 Debugging tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 150',\n",
       " '2.16.3 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 150',\n",
       " '2.16.4 Release process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 151',\n",
       " '2.17 Changelog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . . . . 151',\n",
       " 'ii',\n",
       " '2.17.1 0.26 (2025-05-27) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 151',\n",
       " '2.17.2 0.26a1 (2025-05-25) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 151',\n",
       " '2.17.3 0.26a0 (2025-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 153',\n",
       " '2.17.4 0.25 (2025-05-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 153',\n",
       " '2.17.5 0.25a0 (2025-04-10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 154',\n",
       " '2.17.6 0.24.2 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 154',\n",
       " '2.17.7 0.24.1 (2025-04-08) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 154',\n",
       " '2.17.8 0.24 (2025-04-07) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 154',\n",
       " '2.17.9 0.24a1 (2025-04-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 156',\n",
       " '2.17.10 0.24a0 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 156',\n",
       " '2.17.11 0.23 (2025-02-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 156',\n",
       " '2.17.12 0.22 (2025-02-16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 157',\n",
       " '2.17.13 0.21 (2025-01-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 158',\n",
       " '2.17.14 0.20 (2025-01-22) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 158',\n",
       " '2.17.15 0.19.1 (2024-12-05) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 158',\n",
       " '2.17.16 0.19 (2024-12-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 158',\n",
       " '2.17.17 0.19a2 (2024-11-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 159',\n",
       " '2.17.18 0.19a1 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 159',\n",
       " '2.17.19 0.19a0 (2024-11-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 159',\n",
       " '2.17.20 0.18 (2024-11-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 159',\n",
       " '2.17.21 0.18a1 (2024-11-14) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 160',\n",
       " '2.17.22 0.18a0 (2024-11-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 160',\n",
       " '2.17.23 0.17 (2024-10-29) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 160',\n",
       " '2.17.24 0.17a0 (2024-10-28) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 161',\n",
       " '2.17.25 0.16 (2024-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 161',\n",
       " '2.17.26 0.15 (2024-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 161',\n",
       " '2.17.27 0.14 (2024-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 161',\n",
       " '2.17.28 0.13.1 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 162',\n",
       " '2.17.29 0.13 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 162',\n",
       " '2.17.30 0.12 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 162',\n",
       " '2.17.31 0.11.2 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 163',\n",
       " '2.17.32 0.11.1 (2023-10-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 163',\n",
       " '2.17.33 0.11 (2023-09-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 163',\n",
       " '2.17.34 0.10 (2023-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 164',\n",
       " '2.17.35 0.10a1 (2023-09-11) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 166',\n",
       " '2.17.36 0.10a0 (2023-09-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . 166',\n",
       " '2.17.37 0.9 (2023-09-03) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 166',\n",
       " '2.17.38 0.8.1 (2023-08-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 167',\n",
       " '2.17.39 0.8 (2023-08-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 167',\n",
       " '2.17.40 0.7.1 (2023-08-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 168',\n",
       " '2.17.41 0.7 (2023-08-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 168',\n",
       " '2.17.42 0.6.1 (2023-07-24) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 168',\n",
       " '2.17.43 0.6 (2023-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 169',\n",
       " '2.17.44 0.5 (2023-07-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 169',\n",
       " '2.17.45 0.4.1 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 170',\n",
       " '2.17.46 0.4 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 170',\n",
       " '2.17.47 0.3 (2023-05-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 172',\n",
       " '2.17.48 0.2 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 173',\n",
       " '2.17.49 0.1 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " '. . . 173',\n",
       " 'Index 175\\niii\\niv\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " 'A CLI tool and Python library for interacting withOpenAI, Anthropic’s Claude, Google’s Gemini,',\n",
       " 'Meta’s Llama',\n",
       " 'and dozens of other Large Language Models, both via remote APIs and with models that can be',\n",
       " 'installed and run on',\n",
       " 'your own machine.',\n",
       " 'WatchLanguage models on the command-lineon YouTube for a demo or read the accompanying detailed',\n",
       " 'notes.',\n",
       " 'With LLM you can:\\n• Run prompts from the command-line\\n• Store prompts and responses in SQLite',\n",
       " '• Generate and store embeddings\\n• Extract structured content from text and images',\n",
       " '• Grant models the ability to execute tools\\n• ... and much, much more\\nCONTENTS 1',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n2 CONTENTS\\nCHAPTER\\nONE\\nQUICK START',\n",
       " 'First, install LLM usingpipor Homebrew orpipxor uv:\\npip install llm',\n",
       " 'Or with Homebrew (seewarning note):\\nbrew install llm\\nOr with pipx:\\npipx install llm\\nOr with uv',\n",
       " 'uv tool install llm\\nIf you have an OpenAI API key key you can run this:',\n",
       " '# Paste your OpenAI API key into this\\nllm keys set openai',\n",
       " '# Run a prompt (with the default gpt-4o-mini model)\\nllm \"Ten fun names for a pet pelican\"',\n",
       " '# Extract text from an image\\nllm \"extract text\" -a scanned-document.jpg',\n",
       " '# Use a system prompt against a file\\ncat myfile.py | llm -s \"Explain this code\"',\n",
       " 'Run prompts against Gemini or Anthropic with their respective plugins:\\nllm install llm-gemini',\n",
       " 'llm keys set gemini\\n# Paste Gemini API key here',\n",
       " \"llm -m gemini-2.0-flash 'Tell me fun facts about Mountain View'\\nllm install llm-anthropic\",\n",
       " 'llm keys set anthropic\\n# Paste Anthropic API key here',\n",
       " \"llm -m claude-4-opus 'Impress me with wild facts about turnips'\\n3\",\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a',\n",
       " 'You can alsoinstall a pluginto access models that can run on your local device. If you use Ollama:',\n",
       " '# Install the plugin\\nllm install llm-ollama',\n",
       " '# Download and run a prompt against the Orca Mini 7B model\\nollama pull llama3.2:latest',\n",
       " \"llm -m llama3.2:latest 'What is the capital of France?'\",\n",
       " 'To startan interactive chatwith a model, usellm chat:\\nllm chat -m gpt-4.1\\nChatting with gpt-4.1',\n",
       " \"Type 'exit' or 'quit' to exit\\nType '!multi' to enter multiple lines, then '!end' to finish\",\n",
       " \"Type '!edit' to open your default editor and modify the prompt.\",\n",
       " \"Type '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\",\n",
       " \"> Tell me a joke about a pelican\\nWhy don't pelicans like to tip waiters?\",\n",
       " 'Because they always have a big bill!\\nMore background on this project:',\n",
       " '• llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs',\n",
       " '• The LLM CLI tool now supports self-hosted language models via plugins',\n",
       " '• LLM now provides tools for working with embeddings',\n",
       " '• Build an image search engine with llm-clip, chat with models with llm chat',\n",
       " '• You can now run prompts against images, audio and video in your terminal using LLM',\n",
       " '• Structured data extraction from unstructured content using LLM schemas',\n",
       " '• Long context support in LLM 0.24 using fragments and template plugins',\n",
       " 'See also the llm tag on my blog.\\n4 Chapter 1. Quick start\\nCHAPTER\\nTWO\\nCONTENTS\\n2.1 Setup',\n",
       " '2.1.1 Installation\\nInstall this tool usingpip:\\npip install llm\\nOr using pipx:\\npipx install llm',\n",
       " 'Or using uv (more tips below):\\nuv tool install llm\\nOr using Homebrew (seewarning note):',\n",
       " 'brew install llm\\n2.1.2 Upgrading to the latest version\\nIf you installed usingpip:',\n",
       " 'pip install -U llm\\nFor pipx:\\npipx upgrade llm\\nFor uv:\\nuv tool upgrade llm\\nFor Homebrew:',\n",
       " 'brew upgrade llm',\n",
       " 'If the latest version is not yet available on Homebrew you can upgrade like this instead:\\n5',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nllm install -U llm\\n2.1.3 Using uvx',\n",
       " 'If you have uv installed you can also use theuvxcommand to try LLM without first installing it like',\n",
       " 'this:',\n",
       " \"export OPENAI_API_KEY='sx-...'\\nuvx llm 'fun facts about skunks'\",\n",
       " 'This will install and run LLM using a temporary virtual environment.',\n",
       " 'You can use the--withoption to add extra plugins. To use Anthropic’s models, for example:',\n",
       " \"export ANTHROPIC_API_KEY='...'\",\n",
       " \"uvx --with llm-anthropic llm -m claude-3.5-haiku 'fun facts about skunks'\",\n",
       " 'All of the usual LLM commands will work withuvx llm. Here’s how to set your OpenAI key without',\n",
       " 'needing an',\n",
       " 'environment variable for example:\\nuvx llm keys set openai\\n# Paste key here',\n",
       " '2.1.4 A note about Homebrew and PyTorch',\n",
       " 'TheversionofLLMpackagedforHomebrewcurrentlyusesPython3.12. ThePyTorchprojectdonotyethaveastable',\n",
       " 'release of PyTorch for that version of Python.',\n",
       " 'This means that LLM plugins that depend on PyTorch such as llm-sentence-transformers may not',\n",
       " 'install cleanly with',\n",
       " 'the Homebrew version of LLM.',\n",
       " 'You can workaround this by manually installing PyTorch before installingllm-sentence-transformers:',\n",
       " 'llm install llm-python\\nllm python -m pip install \\\\\\n--pre torch torchvision \\\\',\n",
       " '--index-url https://download.pytorch.org/whl/nightly/cpu\\nllm install llm-sentence-transformers',\n",
       " 'This should produce a working installation of that plugin.\\n2.1.5 Installing plugins',\n",
       " 'Plugins can be used to add support for other language models, including models that can run on your',\n",
       " 'own device.',\n",
       " 'For example, the llm-gpt4all plugin adds support for 17 new models that can be installed on your',\n",
       " 'own machine. You',\n",
       " 'can install that like so:\\nllm install llm-gpt4all\\n6 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n2.1.6 API key management',\n",
       " 'ManyLLMmodelsrequireanAPIkey. TheseAPIkeyscanbeprovidedtothistoolusingseveraldifferentmechanisms.',\n",
       " 'You can obtain an API key for OpenAI’s language models from the API keys page on their site.',\n",
       " 'Saving and using stored keys\\nThe easiest way to store an API key is to use thellm keys setcommand:',\n",
       " 'llm keys set openai\\nYou will be prompted to enter the key like this:\\n% llm keys set openai',\n",
       " 'Enter key:\\nOnce stored, this key will be automatically used for subsequent calls to the API:',\n",
       " 'llm \"Five ludicrous names for a pet lobster\"',\n",
       " 'You can list the names of keys that have been set using this command:\\nllm keys',\n",
       " 'Keysthatarestoredinthiswayliveinafilecalled keys.json. Thisfileislocatedatthepathshownwhenyourunthe',\n",
       " 'following command:\\nllm keys path',\n",
       " 'OnmacOSthiswillbe ~/Library/Application Support/io.datasette.llm/keys.json. OnLinuxitmaybe',\n",
       " 'something like~/.config/io.datasette.llm/keys.json.\\nPassing keys using the –key option',\n",
       " 'Keys can be passed directly using the--keyoption, like this:',\n",
       " 'llm \"Five names for pet weasels\" --key sk-my-key-goes-here',\n",
       " 'Youcanalsopass thealiasofakeystored inthe keys.jsonfile. Forexample, ifyouwanttomaintainapersonal',\n",
       " 'API',\n",
       " 'key you could add that like this:\\nllm keys set personal\\nAnd then use it for prompts like so:',\n",
       " 'llm \"Five friendly names for a pet skunk\" --key personal\\n2.1. Setup 7',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nKeys in environment variables',\n",
       " 'Keys can also be set using an environment variable. These are different for different models.',\n",
       " 'For OpenAI models the key will be read from theOPENAI_API_KEYenvironment variable.',\n",
       " 'The environment variable will be used if no--keyoption is passed to the command and there is not a',\n",
       " 'key configured',\n",
       " 'in keys.json\\nTo use an environment variable in place of thekeys.jsonkey run the prompt like this:',\n",
       " \"llm 'my prompt' --key $OPENAI_API_KEY\\n2.1.7 Configuration\",\n",
       " 'You can configure LLM in a number of different ways.\\nSetting a custom default model',\n",
       " 'The model used when callingllm without the-m/--model option defaults togpt-4o-mini - the fastest',\n",
       " 'and least',\n",
       " 'expensive OpenAI model.',\n",
       " 'You can use thellm models default command to set a different default model. For GPT-4o (slower and',\n",
       " 'more',\n",
       " 'expensive, but more capable) run this:\\nllm models default gpt-4o',\n",
       " 'You can view the current model by running this:\\nllm models default',\n",
       " 'Any of the supported aliases for a model can be passed to this command.',\n",
       " 'Setting a custom directory location',\n",
       " 'This tool stores various files - prompt templates, stored keys, preferences, a database of logs -',\n",
       " 'in a directory on your',\n",
       " 'computer.\\nOn macOS this is~/Library/Application Support/io.datasette.llm/.',\n",
       " 'On Linux it may be something like~/.config/io.datasette.llm/.',\n",
       " 'You can set a custom location for this directory by setting theLLM_USER_PATHenvironment variable:',\n",
       " 'export LLM_USER_PATH=/path/to/my/custom/directory\\n8 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nTurning SQLite logging on and off',\n",
       " 'By default, LLM will log every prompt and response you make to a SQLite database - seeLogging to',\n",
       " 'SQLitefor more',\n",
       " 'details.\\nYou can turn this behavior off by default by running:\\nllm logs off',\n",
       " 'Or turn it back on again with:\\nllm logs on',\n",
       " 'Runllm logs statusto see the current states of the setting.\\n2.2 Usage',\n",
       " \"The command to run a prompt isllm prompt 'your prompt'. This is the default command, so you can\",\n",
       " 'usellm',\n",
       " \"'your prompt' as a shortcut.\\n2.2.1 Executing a prompt\",\n",
       " 'These examples use the default OpenAIgpt-4o-minimodel, which requires you to firstset an OpenAI API',\n",
       " 'key.',\n",
       " 'Youcan installLLMplugins',\n",
       " 'tousemodelsfromotherproviders,includingopenlylicensedmodelsyoucanrundirectly',\n",
       " 'on your own computer.\\nTo run a prompt, streaming tokens as they come in:',\n",
       " \"llm 'Ten names for cheesecakes'\",\n",
       " 'To disable streaming and only return the response once it has completed:',\n",
       " \"llm 'Ten names for cheesecakes' --no-stream\\nTo switch from ChatGPT 4o-mini (the default) to GPT-4o:\",\n",
       " \"llm 'Ten names for cheesecakes' -m gpt-4o\\nYou can use-m 4oas an even shorter shortcut.\",\n",
       " 'Pass--model <model name>to use a different model. Runllm modelsto see a list of available models.',\n",
       " 'Orifyouknowthenameistoolongtotype, use -qonceormoretoprovidesearchterms-themodelwiththeshortest',\n",
       " 'model ID that matches all of those terms (as a lowercase substring) will be used:',\n",
       " \"llm 'Ten names for cheesecakes' -q 4o -q mini\",\n",
       " 'To change the default model for the current session, set theLLM_MODELenvironment variable:',\n",
       " \"export LLM_MODEL=gpt-4.1-mini\\nllm 'Ten names for cheesecakes' # Uses gpt-4.1-mini\",\n",
       " 'You can send a prompt directly to standard input like this:\\n2.2. Usage 9',\n",
       " \"LLM documentation, Release 0.26-31-g0bf655a\\necho 'Ten names for cheesecakes' | llm\",\n",
       " 'Ifyousendtexttostandardinputandprovidearguments,theresultingpromptwillconsistofthepipedcontentfollo',\n",
       " 'wed',\n",
       " \"by the arguments:\\ncat myscript.py | llm 'explain this code'\\nWill run a prompt of:\",\n",
       " '<contents of myscript.py> explain this code',\n",
       " 'For models that support them,system promptsare a better tool for this kind of prompting.',\n",
       " 'Model options',\n",
       " 'Somemodelssupportoptions. Youcanpasstheseusing -o/--option name value-forexample, tosetthetemper-',\n",
       " \"ature to 1.5 run this:\\nllm 'Ten names for cheesecakes' -o temperature 1.5\",\n",
       " 'Use thellm models --optionscommand to see which options are supported by each model.',\n",
       " 'You can alsoconfigure default optionsfor a model using thellm models optionscommands.\\nAttachments',\n",
       " 'Some models are multi-modal, which means they can accept input in more than just text. GPT-4o and',\n",
       " 'GPT-4o mini',\n",
       " 'can accept images, and models such as Google Gemini 1.5 can accept audio and video as well.',\n",
       " 'LLM calls theseattachments. You can pass attachments using the-aoption like this:',\n",
       " 'llm \"describe this image\" -a https://static.simonwillison.net/static/2024/pelicans.jpg',\n",
       " 'Attachments can be passed using URLs or file paths, and you can attach more than one attachment to',\n",
       " 'a single prompt:',\n",
       " 'llm \"extract text\" -a image1.jpg -a image2.jpg',\n",
       " 'You can also pipe an attachment to LLM by using-as the filename:',\n",
       " 'cat image.jpg | llm \"describe this image\" -a -',\n",
       " 'LLM will attempt to automatically detect the content type of the image. If this doesn’t work you',\n",
       " 'can instead use the',\n",
       " '--attachment-typeoption (--atfor short) which takes the URL/path plus an explicit content type:',\n",
       " 'cat myfile | llm \"describe this image\" --at - image/jpeg\\n10 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nSystem prompts',\n",
       " \"You can use-s/--system '...' to set a system prompt.\\nllm 'SQL to calculate total sales by month' \\\\\",\n",
       " \"--system 'You are an exaggerated sentient cheesecake that knows SQL and talks about␣\",\n",
       " \"˓→cheesecake a lot'\\nThis is useful for piping content to standard input, for example:\",\n",
       " \"curl -s 'https://simonwillison.net/2023/May/15/per-interpreter-gils/' | \\\\\",\n",
       " \"llm -s 'Suggest topics for this post as a JSON array'\",\n",
       " 'Or to generate a description of changes made to a Git repository since the last commit:',\n",
       " \"git diff | llm -s 'Describe these changes'\",\n",
       " 'Different models support system prompts in different ways.',\n",
       " 'TheOpenAImodelsareparticularlygoodatusingsystempromptsasinstructionsforhowtheyshouldprocessaddition',\n",
       " 'al',\n",
       " 'input sent as part of the regular prompt.',\n",
       " 'Other models might use system prompts change the default voice and attitude of the model.',\n",
       " 'System prompts can be saved astemplates to create reusable tools. For example, you can create a',\n",
       " 'template called',\n",
       " \"pytestlike this:\\nllm -s 'write pytest tests for this code' --save pytest\",\n",
       " 'And then use the new template like this:\\ncat llm/utils.py | llm -t pytest',\n",
       " 'See prompt templatesfor more.\\nTools',\n",
       " 'Manymodelssupporttheabilitytocall externaltools. Toolscanbeprovided byplugins oryoucanpassa',\n",
       " '--functions',\n",
       " 'CODEoption to LLM to define one or more Python functions that the model can then call.',\n",
       " 'llm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y',\n",
       " \"' 'what is 34234 * 213345'\",\n",
       " 'Add --td/--tools-debug to see full details of the tools that are being executed. You can also set',\n",
       " 'the',\n",
       " \"LLM_TOOLS_DEBUGenvironment variable to1to enable this for all prompts.\\nllm --functions '\",\n",
       " 'def multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y',\n",
       " \"' 'what is 34234 * 213345' --td\\nOutput:\\n2.2. Usage 11\\nLLM documentation, Release 0.26-31-g0bf655a\",\n",
       " \"Tool call: multiply({'x': 34234, 'y': 213345})\\n7303652730\",\n",
       " '34234 multiplied by 213345 is 7,303,652,730.',\n",
       " 'Or add--ta/--tools-approveto approve each tool call interactively before it is executed:',\n",
       " 'llm --functions \\'\\ndef multiply(x: int, y: int) -> int:\\n\"\"\"Multiply two numbers.\"\"\"\\nreturn x * y',\n",
       " \"' 'what is 34234 * 213345' --ta\\nOutput:\\nTool call: multiply({'x': 34234, 'y': 213345})\",\n",
       " 'Approve tool call? [y/N]:',\n",
       " 'The --functions option can be passed more than once, and can also point to the filename of a.py',\n",
       " 'file containing',\n",
       " 'one or more functions.',\n",
       " 'If you have any tools that have been made available via plugins you can add them to the prompt',\n",
       " 'using--tool/-T',\n",
       " 'option. For example, using llm-tools-simpleeval like this:\\nllm install llm-tools-simpleeval',\n",
       " 'llm --tool simple_eval \"4444 * 233423\" --td',\n",
       " 'Run this command to see a list of available tools from plugins:\\nllm tools',\n",
       " 'If you run a prompt that uses tools from plugins (as opposed to tools provided using the--functions',\n",
       " 'option) con-',\n",
       " 'tinuing that conversation usingllm -c will reuse the tools from the first prompt. Runningllm chat',\n",
       " '-c will start a',\n",
       " 'chat that continues using those same tools. For example:\\nllm -T simple_eval \"12345 * 12345\" --td',\n",
       " \"Tool call: simple_eval({'expression': '12345 * 12345'})\\n152399025\",\n",
       " '12345 multiplied by 12345 equals 152,399,025.\\nllm -c \"that * 6\" --td',\n",
       " \"Tool call: simple_eval({'expression': '152399025 * 6'})\\n914394150\",\n",
       " '152,399,025 multiplied by 6 equals 914,394,150.\\nllm chat -c --td\\nChatting with gpt-4.1-mini',\n",
       " \"Type 'exit' or 'quit' to exit\\nType '!multi' to enter multiple lines, then '!end' to finish\",\n",
       " \"Type '!edit' to open your default editor and modify the prompt\\n> / 123\",\n",
       " \"Tool call: simple_eval({'expression': '914394150 / 123'})\\n7434098.780487805\",\n",
       " '914,394,150 divided by 123 is approximately 7,434,098.78.',\n",
       " 'Some tools are bundled in a configurable collection of tools called atoolbox. This means a',\n",
       " 'single--tooloption can',\n",
       " 'load multiple related tools.\\nllm-tools-datasette is one example. Using a toolbox looks like this:',\n",
       " '12 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\nllm install llm-tools-datasette',\n",
       " 'llm -T \\'Datasette(\"https://datasette.io/content\")\\' \"Show tables\" --td',\n",
       " 'Toolboxesalwaysstartwithacapitalletter.',\n",
       " 'Theycanbeconfiguredbypassingatoolspecification,whichshouldfitthe',\n",
       " 'following patterns:\\n• Empty: ToolboxNameor ToolboxName()- has no configuration arguments',\n",
       " '• JSON object:ToolboxName({\"key\": \"value\", \"other\": 42})',\n",
       " '• Single JSON value:ToolboxName(\"hello\")or ToolboxName([1,2,3])',\n",
       " '• Key-value pairs:ToolboxName(name=\"test\", count=5, items=[1,2]) - treated the same as{\"name\":',\n",
       " '\"test\", \"count\": 5, \"items\": [1, 2]} , all values must be valid JSON',\n",
       " 'Toolboxes are not currently supported with thellm -coption, but they work well withllm chat. Try',\n",
       " 'chatting with',\n",
       " 'the Datasette content database like this:',\n",
       " 'llm chat -T \\'Datasette(\"https://datasette.io/content\")\\' --td\\nChatting with gpt-4.1-mini',\n",
       " \"Type 'exit' or 'quit' to exit\\n...\\n> show tables\\nExtracting fenced code blocks\",\n",
       " 'If you are using an LLM to generate code it can be useful to retrieve just the code it produces',\n",
       " 'without any of the',\n",
       " 'surrounding explanatory text.',\n",
       " 'The -x/--extractoption will scan the response for the first instance of a Markdown fenced code',\n",
       " 'block - something',\n",
       " 'that looks like this:\\n```python\\ndef my_function():\\n# ...\\n```',\n",
       " 'It will extract and returns just the content of that block, excluding the fenced coded delimiters.',\n",
       " 'If there are no fenced',\n",
       " 'code blocks it will return the full response.',\n",
       " 'Use --xl/--extract-lastto return the last fenced code block instead of the first.',\n",
       " 'The entire response including explanatory text is still logged to the database, and can be viewed',\n",
       " 'usingllm logs -c.',\n",
       " 'Schemas',\n",
       " 'Some models include the ability to return JSON that matches a provided JSON schema. Models from',\n",
       " 'OpenAI, An-',\n",
       " 'thropic and Google Gemini all include this capability.',\n",
       " 'Take a look at theschemas documentationfor a detailed guide to using this feature.',\n",
       " 'You can pass JSON schemas directly to the--schemaoption:\\n2.2. Usage 13',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nllm --schema \\'{\\n\"type\": \"object\",\\n\"properties\": {',\n",
       " '\"dogs\": {\\n\"type\": \"array\",\\n\"items\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"name\": {\\n\"type\": \"string\"',\n",
       " '},\\n\"bio\": {\\n\"type\": \"string\"\\n}\\n}\\n}\\n}\\n}\\n}\\' -m gpt-4o-mini \\'invent two dogs\\'',\n",
       " \"Or use LLM’s customconcise schema syntaxlike this:\\nllm --schema 'name,bio' 'invent a dog'\",\n",
       " 'Two use the same concise schema for multiple items use--schema-multi:',\n",
       " \"llm --schema-multi 'name,bio' 'invent two dogs'\",\n",
       " 'You can also save the JSON schema to a file and reference the filename using--schema:',\n",
       " \"llm --schema dogs.schema.json 'invent two dogs'\\nOr save your schemato a templatelike this:\",\n",
       " \"llm --schema dogs.schema.json --save dogs\\n# Then to use it:\\nllm -t dogs 'invent two dogs'\",\n",
       " 'Be warned that different models may support different dialects of the JSON schema specification.',\n",
       " 'See Browsing logged JSON objects created using schemasfor tips on using thellm logs --schema',\n",
       " 'Xcommand to',\n",
       " 'access JSON objects you have previously logged using this option.\\nFragments',\n",
       " 'Youcanusethe -f/--fragmentoptiontoreferencefragmentsofcontextthatyouwouldliketoloadintoyourprompt.',\n",
       " 'Fragments can be specified as URLs, file paths or as aliases to previously saved fragments.',\n",
       " 'Fragmentsaredesignedforrunninglongerprompts. LLM storespromptsinadatabase ,andthesamepromptrepeated',\n",
       " 'manytimescanendupstoredasmultiplecopies,wastingdiskspace.',\n",
       " 'Afragmentwillbestoredjustonceandreferenced',\n",
       " 'by all of the prompts that use it.',\n",
       " 'The -f option can accept a path to a file on disk, a URL or the hash or alias of a previous',\n",
       " 'fragment.',\n",
       " 'For example, to ask a question about therobots.txtfile onllm.datasette.io:\\n14 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a',\n",
       " \"llm -f https://llm.datasette.io/robots.txt 'explain this'\",\n",
       " 'For a poem inspired by some Python code on disk:',\n",
       " \"llm -f cli.py 'a short snappy poem inspired by this code'\",\n",
       " 'You can use as many-f options as you like - the fragments will be concatenated together in the',\n",
       " 'order you provided,',\n",
       " 'with any additional prompt added at the end.',\n",
       " 'Fragments can also be used for the system prompt using the--sf/--system-fragment option. If you',\n",
       " 'have a file',\n",
       " 'called explain_code.txtcontaining this:',\n",
       " 'Explain this code in detail. Include copies of the code quoted in the explanation.',\n",
       " 'You can run it as the system prompt like this:\\nllm -f cli.py --sf explain_code.txt',\n",
       " 'You can use thellm fragments setcommand to load a fragment and give it an alias for use in future',\n",
       " 'queries:',\n",
       " \"llm fragments set cli cli.py\\n# Then\\nllm -f cli 'explain this code'\",\n",
       " 'Use llm fragmentsto list all fragments that have been stored:\\nllm fragments',\n",
       " 'Youcansearchbypassingoneormore -q Xsearchstrings.',\n",
       " 'Thiswillreturnresultsmatchingallofthosestrings,across',\n",
       " 'the source, hash, aliases and content:\\nllm fragments -q pytest -q asyncio',\n",
       " 'The llm fragments remove command removes an alias. It does not delete the fragment record itself as',\n",
       " 'those are',\n",
       " 'linked to previous prompts and responses and cannot be deleted independently of them.',\n",
       " 'llm fragments remove cli\\nContinuing a conversation',\n",
       " 'By default, the tool will start a new conversation each time you run it.',\n",
       " 'You can opt to continue the previous conversation by passing the-c/--continueoption:',\n",
       " \"llm 'More names' -c\",\n",
       " 'This will re-send the prompts and responses for the previous conversation as part of the call to',\n",
       " 'the language model.',\n",
       " 'Note that this can add up quickly in terms of tokens, especially if you are using expensive models.',\n",
       " '--continue will automatically use the same model as the conversation that you are continuing, even',\n",
       " 'if you omit the',\n",
       " '-m/--modeloption.',\n",
       " 'To continue a conversation that is not the most recent one, use the--cid/--conversation <id>option:',\n",
       " \"llm 'More names' --cid 01h53zma5txeby33t1kbe3xk8q\",\n",
       " 'You can find these conversation IDs using thellm logscommand.\\n2.2. Usage 15',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nTips for using LLM with Bash or Zsh',\n",
       " 'To learn more about your computer’s operating system based on the output ofuname -a, run this:',\n",
       " 'llm \"Tell me about my operating system: $(uname -a)\"',\n",
       " 'This pattern of using$(command)inside a double quoted string is a useful way to quickly assemble',\n",
       " 'prompts.',\n",
       " 'Completion prompts',\n",
       " 'Some models are completion models - rather than being tuned to respond to chat style prompts, they',\n",
       " 'are designed to',\n",
       " 'complete a sentence or paragraph.\\nAn example of this is thegpt-3.5-turbo-instructOpenAI model.',\n",
       " 'You can prompt that model the same way as the chat models, but be aware that the prompt format that',\n",
       " 'works best is',\n",
       " \"likely to differ.\\nllm -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver:'\",\n",
       " '2.2.2 Starting an interactive chat',\n",
       " 'The llm chatcommand starts an ongoing interactive chat with a model.',\n",
       " 'This is particularly useful for models that run on your own machine, since it saves them from',\n",
       " 'having to be loaded into',\n",
       " 'memory each time a new prompt is added to a conversation.',\n",
       " 'Runllm chat, optionally with a-m model_id, to start a chat conversation:\\nllm chat -m chatgpt',\n",
       " 'Each chat starts a new conversation. A record of each conversation can be accessed throughthe logs.',\n",
       " 'You can pass-c to start a conversation as a continuation of your most recent prompt. This will',\n",
       " 'automatically use the',\n",
       " 'most recently used model:\\nllm chat -c',\n",
       " 'For models that support them, you can pass options using-o/--option:',\n",
       " 'llm chat -m gpt-4 -o temperature 0.5',\n",
       " 'You can pass a system prompt to be used for your chat conversation:',\n",
       " \"llm chat -m gpt-4 -s 'You are a sentient cheesecake'\",\n",
       " 'You can also passa template- useful for creating chat personas that you wish to return to.',\n",
       " 'Here’s how to create a template for your GPT-4 powered cheesecake:',\n",
       " \"llm --system 'You are a sentient cheesecake' -m gpt-4 --save cheesecake\",\n",
       " 'Now you can start a new chat with your cheesecake any time you like using this:',\n",
       " 'llm chat -t cheesecake\\n16 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " \"Chatting with gpt-4\\nType 'exit' or 'quit' to exit\",\n",
       " \"Type '!multi' to enter multiple lines, then '!end' to finish\",\n",
       " \"Type '!edit' to open your default editor and modify the prompt\",\n",
       " \"Type '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\",\n",
       " '> who are you?\\nI am a sentient cheesecake, meaning I am an artificial',\n",
       " 'intelligence embodied in a dessert form, specifically a',\n",
       " \"cheesecake. However, I don't consume or prepare foods\",\n",
       " 'like humans do, I communicate, learn and help answer\\nyour queries.',\n",
       " 'Typequitor exitfollowed by<enter>to end a chat session.',\n",
       " 'Sometimes you may want to paste multiple lines of text into a chat at once - for example when',\n",
       " 'debugging an error',\n",
       " 'message.',\n",
       " 'To do that, type!multito start a multi-line input. Type or paste your text, then type!endand',\n",
       " 'hit<enter>to finish.',\n",
       " 'Ifyourpastedtextmightitselfcontaina !endline,youcansetacustomdelimiterusing !multi abcfollowedby',\n",
       " '!end',\n",
       " \"abcat the end:\\nChatting with gpt-4\\nType 'exit' or 'quit' to exit\",\n",
       " \"Type '!multi' to enter multiple lines, then '!end' to finish\",\n",
       " \"Type '!edit' to open your default editor and modify the prompt.\",\n",
       " \"Type '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\",\n",
       " '> !multi custom-end\\nExplain this error:',\n",
       " 'File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣',\n",
       " '˓→1391, in https_open\\nreturn self.do_open(http.client.HTTPSConnection, req,',\n",
       " 'File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣',\n",
       " '˓→1351, in do_open\\nraise URLError(err)',\n",
       " 'urllib.error.URLError: <urlopen error [Errno 8] nodename nor servname provided, or not␣\\n˓→known>',\n",
       " '!end custom-end',\n",
       " 'You can also use!editto open your default editor and modify the prompt before sending it to the',\n",
       " 'model.',\n",
       " \"Chatting with gpt-4\\nType 'exit' or 'quit' to exit\",\n",
       " \"Type '!multi' to enter multiple lines, then '!end' to finish\",\n",
       " \"Type '!edit' to open your default editor and modify the prompt.\",\n",
       " \"Type '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\\n> !edit\",\n",
       " 'llm chattakesthesame --tool/-Tand--functionsoptionsas llm prompt. Youcanusethistostartachatwith',\n",
       " 'the specifiedtoolsenabled.\\n2.2. Usage 17\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '2.2.3 Listing available models',\n",
       " 'The llm models command lists every model that can be used with LLM, along with their aliases. This',\n",
       " 'includes',\n",
       " 'models that have been installed usingplugins.\\nllm models\\nExample output:',\n",
       " 'OpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)',\n",
       " 'OpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nGeminiPro: gemini-1.5-pro-002',\n",
       " 'GeminiPro: gemini-1.5-flash-002\\n...',\n",
       " 'Add one or more-q termoptions to search for models matching all of those search terms:',\n",
       " 'llm models -q gpt-4o\\nllm models -q 4o -q mini',\n",
       " 'Use one or more-moptions to indicate specific models, either by their model ID or one of their',\n",
       " 'aliases:',\n",
       " 'llm models -m gpt-4o -m gemini-1.5-pro-002',\n",
       " 'Add--optionsto also see documentation for the options supported by each model:\\nllm models --options',\n",
       " 'Output:\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOptions:\\ntemperature: float',\n",
       " 'What sampling temperature to use, between 0 and 2. Higher values like',\n",
       " '0.8 will make the output more random, while lower values like 0.2 will',\n",
       " 'make it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.',\n",
       " 'top_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,',\n",
       " 'where the model considers the results of the tokens with top_p',\n",
       " 'probability mass. So 0.1 means only the tokens comprising the top 10%',\n",
       " 'probability mass are considered. Recommended to use top_p or\\ntemperature but not both.',\n",
       " 'frequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based',\n",
       " \"on their existing frequency in the text so far, decreasing the model's\",\n",
       " 'likelihood to repeat the same line verbatim.\\npresence_penalty: float',\n",
       " 'Number between -2.0 and 2.0. Positive values penalize new tokens based',\n",
       " \"on whether they appear in the text so far, increasing the model's\",\n",
       " 'likelihood to talk about new topics.\\n(continues on next page)\\n18 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nstop: str',\n",
       " 'A string where the API will stop generating further tokens.\\nlogit_bias: dict, str',\n",
       " 'Modify the likelihood of specified tokens appearing in the completion.',\n",
       " 'Pass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int',\n",
       " 'Integer seed to attempt to sample deterministically\\njson_object: boolean',\n",
       " 'Output a valid JSON object {...}. Prompt must mention JSON.\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str',\n",
       " 'seed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n(continues on next page)',\n",
       " '2.2. Usage 19\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)',\n",
       " '- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4o-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'audio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4o-audio-preview-2024-10-01\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\n(continues on next page)',\n",
       " '20 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'audio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4o-mini-audio-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nAttachment types:\\naudio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'audio/mpeg, audio/wav\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOptions:\\n(continues on next page)\\n2.2. Usage 21',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\ntemperature: float',\n",
       " 'max_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str',\n",
       " 'logit_bias: dict, str\\nseed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str',\n",
       " 'seed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str',\n",
       " 'seed: int\\njson_object: boolean\\nAttachment types:\\n(continues on next page)\\n22 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str',\n",
       " 'seed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai',\n",
       " 'env_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\n(continues on next page)\\n2.2. Usage 23',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)\\nseed: int',\n",
       " 'json_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4-1106-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4-0125-preview\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " '(continues on next page)\\n24 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '(continued from previous page)\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:\\nkey: openai',\n",
       " 'env_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOptions:\\ntemperature: float',\n",
       " 'max_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str',\n",
       " 'logit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOptions:\\ntemperature: float',\n",
       " 'max_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str',\n",
       " 'logit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: gpt-4.5-preview-2025-02-27\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\n(continues on next page)',\n",
       " '2.2. Usage 25\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)',\n",
       " 'Attachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:',\n",
       " '- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOptions:\\ntemperature: float\\nmax_tokens: int',\n",
       " 'top_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str',\n",
       " 'seed: int\\njson_object: boolean\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " 'stop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nreasoning_effort: str',\n",
       " 'Attachment types:\\napplication/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\n(continues on next page)',\n",
       " '26 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a\\n(continued from previous page)',\n",
       " 'OpenAI Chat: o1-2024-12-17\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nreasoning_effort: str\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- schemas\\n- tools\\n- async',\n",
       " 'Keys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-preview\\nOptions:\\ntemperature: float',\n",
       " 'max_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str',\n",
       " 'logit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async\\nKeys:',\n",
       " 'key: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o1-mini\\nOptions:\\ntemperature: float',\n",
       " 'max_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str',\n",
       " 'logit_bias: dict, str\\nseed: int\\njson_object: boolean\\nFeatures:\\n- streaming\\n- async',\n",
       " '(continues on next page)\\n2.2. Usage 27\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '(continued from previous page)\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o3-mini',\n",
       " 'Options:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float',\n",
       " 'presence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean',\n",
       " 'reasoning_effort: str\\nFeatures:\\n- streaming\\n- schemas\\n- tools\\n- async\\nKeys:\\nkey: openai',\n",
       " 'env_var: OPENAI_API_KEY\\nOpenAI Chat: o3\\nOptions:\\ntemperature: float\\nmax_tokens: int\\ntop_p: float',\n",
       " 'frequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int',\n",
       " 'json_object: boolean\\nreasoning_effort: str\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY\\nOpenAI Chat: o4-mini\\nOptions:',\n",
       " 'temperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float',\n",
       " '(continues on next page)\\n28 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '(continued from previous page)\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean',\n",
       " 'reasoning_effort: str\\nAttachment types:',\n",
       " 'application/pdf, image/gif, image/jpeg, image/png, image/webp\\nFeatures:\\n- streaming\\n- schemas',\n",
       " '- tools\\n- async\\nKeys:\\nkey: openai\\nenv_var: OPENAI_API_KEY',\n",
       " 'OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nOptions:',\n",
       " 'temperature: float\\nWhat sampling temperature to use, between 0 and 2. Higher values like',\n",
       " '0.8 will make the output more random, while lower values like 0.2 will',\n",
       " 'make it more focused and deterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.',\n",
       " 'top_p: float\\nAn alternative to sampling with temperature, called nucleus sampling,',\n",
       " 'where the model considers the results of the tokens with top_p',\n",
       " 'probability mass. So 0.1 means only the tokens comprising the top 10%',\n",
       " 'probability mass are considered. Recommended to use top_p or\\ntemperature but not both.',\n",
       " 'frequency_penalty: float\\nNumber between -2.0 and 2.0. Positive values penalize new tokens based',\n",
       " \"on their existing frequency in the text so far, decreasing the model's\",\n",
       " 'likelihood to repeat the same line verbatim.\\npresence_penalty: float',\n",
       " 'Number between -2.0 and 2.0. Positive values penalize new tokens based',\n",
       " \"on whether they appear in the text so far, increasing the model's\",\n",
       " 'likelihood to talk about new topics.\\nstop: str',\n",
       " 'A string where the API will stop generating further tokens.\\nlogit_bias: dict, str',\n",
       " 'Modify the likelihood of specified tokens appearing in the completion.',\n",
       " 'Pass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100}\\'\\nseed: int',\n",
       " 'Integer seed to attempt to sample deterministically\\nlogprobs: int',\n",
       " 'Include the log probabilities of most likely N per token\\nFeatures:\\n- streaming\\nKeys:\\nkey: openai',\n",
       " 'env_var: OPENAI_API_KEY\\n2.2. Usage 29\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " 'When running a prompt you can pass the full model name or any of the aliases to',\n",
       " 'the-m/--modeloption:',\n",
       " \"llm -m 4o \\\\\\n'As many names for cheesecakes as you can think of, with detailed descriptions'\",\n",
       " '2.2.4 Setting default options for models',\n",
       " 'To configure a default option for a specific model, use thellm models options setcommand:',\n",
       " 'llm models options set gpt-4o temperature 0.5',\n",
       " 'This option will then be applied automatically any time you run a prompt through thegpt-4omodel.',\n",
       " 'Default options are stored in themodel_options.jsonfile in the LLM configuration directory.',\n",
       " 'You can list all default options across all models using thellm models options listcommand:',\n",
       " 'llm models options list',\n",
       " 'Or show them for an individual model withllm models options show <model_id>:',\n",
       " 'llm models options show gpt-4o\\nTo clear a default option, use thellm models options clearcommand:',\n",
       " 'llm models options clear gpt-4o temperature\\nOr clear all default options for a model like this:',\n",
       " 'llm models options clear gpt-4o',\n",
       " 'Defaultmodel optionsare respectedby boththe llm promptandthe llm chatcommands. Theywill notbe',\n",
       " 'applied',\n",
       " 'when you use LLM as aPython library.\\n2.3 OpenAI models',\n",
       " 'LLM ships with a default plugin for talking to OpenAI’s API. OpenAI offer both language models and',\n",
       " 'embedding',\n",
       " 'models, and LLM can access both types.\\n2.3.1 Configuration',\n",
       " 'All OpenAI models are accessed using an API key. You can obtain one from the API keys page on their',\n",
       " 'site.',\n",
       " 'Once you have created a key, configure LLM to use it by running:\\nllm keys set openai',\n",
       " 'Then paste in the API key.\\n30 Chapter 2. Contents\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '2.3.2 OpenAI language models',\n",
       " 'Runllm modelsfor a full list of available models. The OpenAI models supported by LLM are:',\n",
       " 'OpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)',\n",
       " 'OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: gpt-4o-audio-preview',\n",
       " 'OpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01',\n",
       " 'OpenAI Chat: gpt-4o-mini-audio-preview\\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'OpenAI Chat: gpt-4.1 (aliases: 4.1)\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)',\n",
       " 'OpenAI Chat: gpt-4.1-nano (aliases: 4.1-nano)\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)',\n",
       " 'OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)',\n",
       " 'OpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)',\n",
       " 'OpenAI Chat: gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09',\n",
       " 'OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)',\n",
       " 'OpenAI Chat: gpt-4.5-preview-2025-02-27\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)',\n",
       " 'OpenAI Chat: o1\\nOpenAI Chat: o1-2024-12-17\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini',\n",
       " 'OpenAI Chat: o3-mini\\nOpenAI Chat: o3\\nOpenAI Chat: o4-mini',\n",
       " 'OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)',\n",
       " 'See the OpenAI models documentation for details of each of these.',\n",
       " 'gpt-4o-mini(aliased to4o-mini) is the least expensive model, and is the default for if you don’t',\n",
       " 'specify a model at',\n",
       " 'all. Consult OpenAI’s model documentation for details of the other models.',\n",
       " 'o1-pro is not available through the Chat Completions API used by LLM’s default OpenAI plugin. You',\n",
       " 'can install the',\n",
       " 'new llm-openai-plugin plugin to access that model.\\n2.3.3 Model features',\n",
       " 'The following features work with OpenAI models:',\n",
       " '• System promptscan be used to provide instructions that have a higher weight than the prompt',\n",
       " 'itself.',\n",
       " '• Attachments. Many OpenAI models support image inputs - check which ones usingllm models',\n",
       " '--options.',\n",
       " 'Any model that accepts images can also accept PDFs.',\n",
       " '• Schemascan be used to influence the JSON structure of the model output.',\n",
       " '• Model optionscan be used to set parameters liketemperature. Usellm models --optionsfor a full',\n",
       " 'list of',\n",
       " 'supported options.\\n2.3. OpenAI models 31\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " '2.3.4 OpenAI embedding models',\n",
       " 'Runllm embed-modelsfor a list ofembedding models. The following OpenAI embedding models are',\n",
       " 'supported by',\n",
       " 'LLM:\\nada-002 (aliases: ada, oai)\\n3-small\\n3-large\\n3-small-512\\n3-large-256\\n3-large-1024',\n",
       " 'The3-smallmodeliscurrentlythemostinexpensive. 3-largecostsmorebutismorecapable-seeNewembedding',\n",
       " 'models and API updates on the OpenAI blog for details and benchmarks.',\n",
       " 'An important characteristic of any embedding model is the size of the vector it returns. Smaller',\n",
       " 'vectors cost less to',\n",
       " 'store and query, but may be less accurate.',\n",
       " 'OpenAI3-smalland3-largevectorscanbesafelytruncatedtolowerdimensionswithoutlosingtoomuchaccuracy.',\n",
       " 'The -int models provided by LLM are pre-configured to do this, so3-large-256 is the3-large model',\n",
       " 'truncated',\n",
       " 'to 256 dimensions.\\nThe vector size of the supported OpenAI embedding models are as follows:',\n",
       " 'Model Size\\nada-002 1536\\n3-small 1536\\n3-large 3072\\n3-small-512 512\\n3-large-256 256\\n3-large-1024 1024',\n",
       " '2.3.5 OpenAI completion models',\n",
       " 'Thegpt-3.5-turbo-instructmodelisalittledifferent-itisacompletionmodelratherthanachatmodel,described',\n",
       " 'in the OpenAI completions documentation.',\n",
       " 'Completion models can be called with the-o logprobs 3 option (not supported by chat models) which',\n",
       " 'will cause',\n",
       " 'LLM to store 3 log probabilities for each returned token in the SQLite database. Consult this issue',\n",
       " 'for details on how',\n",
       " 'to read these values.\\n2.3.6 Adding more OpenAI models',\n",
       " 'OpenAI occasionally release new models with new names. LLM aims to ship new releases to support',\n",
       " 'these, but you',\n",
       " 'can also configure them directly, by adding them to aextra-openai-models.yamlconfiguration file.',\n",
       " 'Run this command to find the directory in which this file should be created:',\n",
       " 'dirname \"$(llm logs path)\"\\nOn my Mac laptop I get this:',\n",
       " '~/Library/Application Support/io.datasette.llm\\n32 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a',\n",
       " 'Create a file in that directory calledextra-openai-models.yaml.',\n",
       " 'Let’s say OpenAI have just released thegpt-3.5-turbo-0613 model and you want to use it, despite LLM',\n",
       " 'not yet',\n",
       " 'shipping support. You could configure that by adding this to the file:',\n",
       " '- model_id: gpt-3.5-turbo-0613\\nmodel_name: gpt-3.5-turbo-0613\\naliases: [\"0613\"]',\n",
       " 'Themodel_idistheidentifierthatwillberecordedintheLLMlogs. Youcanusethistospecifythemodel,oryoucan',\n",
       " 'optionallyincludealistofaliasesforthatmodel. The',\n",
       " 'model_nameistheactualmodelidentifierthatwillbepassedto',\n",
       " 'the API, which must match exactly what the API expects.',\n",
       " 'If the model is a completion model (such asgpt-3.5-turbo-instruct) addcompletion: true to the',\n",
       " 'configu-',\n",
       " 'ration.',\n",
       " 'If the model supports structured extraction using json_schema, addsupports_schema: true to the',\n",
       " 'configuration.',\n",
       " 'For reasoning models likeo1or o3-miniadd reasoning: true .',\n",
       " 'With this configuration in place, the following command should run a prompt against the new model:',\n",
       " \"llm -m 0613 'What is the capital of France?'\",\n",
       " 'Runllm modelsto confirm that the new model is now available:\\nllm models\\nExample output:',\n",
       " 'OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)',\n",
       " 'OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)',\n",
       " 'OpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)',\n",
       " 'OpenAI Chat: gpt-3.5-turbo-0613 (aliases: 0613)',\n",
       " 'Runningllm logs -n 1should confirm that the prompt and response has been correctly logged to the',\n",
       " 'database.',\n",
       " '2.4 Other models',\n",
       " 'LLM supports OpenAI models by default. You can installplugins to add support for other models. You',\n",
       " 'can also add',\n",
       " 'additional OpenAI-API-compatible modelsusing a configuration file.',\n",
       " '2.4.1 Installing and using a local model',\n",
       " 'LLM pluginscan provide local models that run on your machine.',\n",
       " 'To installllm-gpt4all, providing 17 models from the GPT4All project, run this:',\n",
       " 'llm install llm-gpt4all\\nRunllm modelsto see the expanded list of available models.',\n",
       " 'To run a prompt through one of the models from GPT4All specify it using-m/--model:',\n",
       " '2.4. Other models 33\\nLLM documentation, Release 0.26-31-g0bf655a',\n",
       " \"llm -m orca-mini-3b-gguf2-q4_0 'What is the capital of France?'\",\n",
       " 'The model will be downloaded and cached the first time you use it.',\n",
       " 'Check theplugin directoryfor the latest list of available plugins for other models.',\n",
       " '2.4.2 OpenAI-compatible models',\n",
       " 'ProjectssuchasLocalAIofferaRESTAPIthatimitatestheOpenAIAPIbutcanbeusedtorunothermodels,including',\n",
       " 'models that can be installed on your own machine. These can be added using the same configuration',\n",
       " 'mechanism.',\n",
       " 'The model_id is the name LLM will use for the model. Themodel_name is the name which needs to be',\n",
       " 'passed to',\n",
       " 'the API - this might differ from themodel_id, especially if themodel_idcould potentially clash with',\n",
       " 'other installed',\n",
       " 'models.\\nThe api_basekey can be used to point the OpenAI client library at a different API endpoint.',\n",
       " 'Toaddthe orca-mini-3bmodelhostedbyalocalinstallationofLocalAI,addthistoyour extra-openai-models.',\n",
       " 'yamlfile:\\n- model_id: orca-openai-compat\\nmodel_name: orca-mini-3b.ggmlv3',\n",
       " 'api_base: \"http://localhost:8080\"',\n",
       " 'If theapi_baseis set, the existing configuredopenaiAPI key will not be sent by default.',\n",
       " 'You can setapi_key_nameto the name of a key stored using theAPI key managementfeature.',\n",
       " 'Add completion: true if the model is a completion model that uses a /completion as opposed to a /',\n",
       " 'completion/chatendpoint.',\n",
       " 'If a model does not support streaming, addcan_stream: false to disable the streaming option.',\n",
       " 'If a model supports structured output via JSON schemas, you can addsupports_schema: true to support',\n",
       " 'this',\n",
       " 'feature.',\n",
       " 'If a model is a vision model, you can addvision: true to support this feature and use image',\n",
       " 'attachments.',\n",
       " 'If a model is an audio model, you can addaudio: true to support this feature and use audio',\n",
       " 'attachments.',\n",
       " 'Having configured the model like this, runllm modelsto check that it installed correctly. You can',\n",
       " 'then run prompts',\n",
       " \"against it like so:\\nllm -m orca-openai-compat 'What is the capital of France?'\",\n",
       " 'And confirm they were logged correctly with:\\nllm logs -n 1\\n34 Chapter 2. Contents',\n",
       " 'LLM documentation, Release 0.26-31-g0bf655a\\nExtra HTTP headers',\n",
       " 'Some providers such as openrouter.ai may require the setting of additional HTTP headers. You can',\n",
       " 'set those using the',\n",
       " 'headers: key like this:\\n- model_id: claude\\nmodel_name: anthropic/claude-2',\n",
       " 'api_base: \"https://openrouter.ai/api/v1\"\\napi_key_name: openrouter\\nheaders:',\n",
       " 'HTTP-Referer: \"https://llm.datasette.io/\"\\nX-Title: LLM\\n2.5 Tools',\n",
       " 'Many Large Language Models have been trained to execute tools as part of responding to a prompt.',\n",
       " 'LLM supports',\n",
       " 'tool usage with both the command-line interface and the Python API.',\n",
       " 'Exposing tools to LLMscarries risks! Be sure to read thewarning below.\\n2.5.1 How tools work',\n",
       " 'A tool is effectively a function that the model can request to be executed. Here’s how that works:',\n",
       " '1. The initial prompt to the model includes a list of available tools, containing their names,',\n",
       " 'descriptions and pa-',\n",
       " 'rameters.',\n",
       " '2. The model can choose to call one (or sometimes more than one) of those tools, returning a',\n",
       " 'request for the tool',\n",
       " 'to execute.',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bd44363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (0.3.26)\n",
      "Collecting openai\n",
      "  Downloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "     -------------------------------------- 765.0/765.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "     -------------------------------------- 470.2/470.2 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl (209 kB)\n",
      "     -------------------------------------- 209.2/209.2 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from openai) (4.14.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "     -------------------------------------- 216.1/216.1 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "     ---------------------------------------- 10.7/10.7 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "     ---------------------------------------- 38.6/38.6 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "     -------------------------------------- 515.3/515.3 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.6/199.6 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.9/134.9 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "     -------------------------------------- 308.9/308.9 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     -------------------------------------- 307.7/307.7 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, scipy, safetensors, Pillow, networkx, MarkupSafe, joblib, jiter, fsspec, filelock, distro, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, openai, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 distro-1.9.0 filelock-3.18.0 fsspec-2025.7.0 huggingface-hub-0.33.4 jinja2-3.1.6 jiter-0.10.0 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 openai-1.97.0 safetensors-0.5.3 scikit-learn-1.7.0 scipy-1.16.0 sentence_transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 tqdm-4.67.1 transformers-4.53.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain openai sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6baac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7226eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samreen Fathima\\AppData\\Local\\Temp\\ipykernel_27148\\1179391892.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeded = HuggingFaceEmbeddings(model_name= \"all-MiniLM-L6-V2\")\n",
      "c:\\Users\\Samreen Fathima\\OneDrive\\Desktop\\Python_Tutorial\\samreen\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Samreen Fathima\\OneDrive\\Desktop\\Python_Tutorial\\samreen\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Samreen Fathima\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-V2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "embeded = HuggingFaceEmbeddings(model_name= \"all-MiniLM-L6-V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37d9542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "textembed = \"This is the one of most wonderful genarative AI Course\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f915856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfaceemdeb = embeded.embed_query(textembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad9c82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.007735162042081356, 0.04069599509239197, 0.03372839465737343, -0.03800322487950325, 0.013358603231608868]\n"
     ]
    }
   ],
   "source": [
    "print(huggingfaceemdeb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e638c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-CPU\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 14.9/14.9 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from faiss-CPU) (2.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from faiss-CPU) (24.2)\n",
      "Installing collected packages: faiss-CPU\n",
      "Successfully installed faiss-CPU-1.11.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9eb52987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e579ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c777c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Python is the best language for AI\", \"This is course is for full beginers\", \"THis will teach you all about GEN AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efca5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_new = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21926068",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(text, embeded_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2833589",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"My_First_VetoreDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ceb1f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement chroma-CPU (from versions: none)\n",
      "ERROR: No matching distribution found for chroma-CPU\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install chroma-CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03ada20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.15-cp39-abi3-win_amd64.whl (19.5 MB)\n",
      "     ---------------------------------------- 19.5/19.5 MB 5.6 MB/s eta 0:00:00\n",
      "Collecting build>=1.0.3\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Collecting pybase64>=1.4.1\n",
      "  Downloading pybase64-1.4.1-cp311-cp311-win_amd64.whl (36 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.4/66.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (2.3.1)\n",
      "Collecting posthog<6.0.0,>=2.4.0\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "     -------------------------------------- 105.4/105.4 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (4.14.0)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl (12.7 MB)\n",
      "     ---------------------------------------- 12.7/12.7 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.6/65.6 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "     -------------------------------------- 119.4/119.4 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (0.21.2)\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "     -------------------------------------- 152.8/152.8 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.3/46.3 kB ? eta 0:00:00\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "     -------------------------------------- 243.2/243.2 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting jsonschema>=4.19.0\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.7/88.7 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.26.0-cp311-cp311-win_amd64.whl (231 kB)\n",
      "     -------------------------------------- 231.7/231.7 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "     -------------------------------------- 216.1/216.1 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "     -------------------------------------- 160.1/160.1 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Collecting durationpy>=0.7\n",
      "  Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "     -------------------------------------- 435.3/435.3 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "     -------------------------------------- 294.5/294.5 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.35.0\n",
      "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 72.5/72.5 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "     -------------------------------------- 201.6/201.6 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.33.4)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "     -------------------------------------- 102.2/102.2 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "     ---------------------------------------- 88.6/88.6 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl (292 kB)\n",
      "     -------------------------------------- 292.6/292.6 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "     -------------------------------------- 176.8/176.8 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samreen fathima\\onedrive\\desktop\\python_tutorial\\samreen\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.2/83.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.1/83.1 kB 4.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=b1232b19ecdb626a92aafa4d3a3b61384b673930545bc01b05f74772c4572735\n",
      "  Stored in directory: c:\\users\\samreen fathima\\appdata\\local\\pip\\cache\\wheels\\da\\b1\\64\\178bd739d19ac4e7b567619cde8454c523f972963ced4d48c4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, zipp, websockets, websocket-client, shellingham, rpds-py, pyreadline3, pyproject_hooks, pybase64, pyasn1, protobuf, overrides, oauthlib, mmh3, mdurl, importlib-resources, httptools, grpcio, click, cachetools, bcrypt, backoff, watchfiles, uvicorn, rsa, requests-oauthlib, referencing, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, humanfriendly, googleapis-common-protos, build, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonschema-specifications, google-auth, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.15 click-8.2.1 coloredlogs-15.0.1 durationpy-0.10 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.73.1 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 overrides-7.7.0 posthog-5.4.0 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.26.0 rsa-4.9.1 shellingham-1.5.4 typer-0.16.0 uvicorn-0.35.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1432298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab015a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d25e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Python is the best language for AI\",\n",
    "    \"This course is for complete beginners\",\n",
    "    \"This will teach you all about Gen AI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aba1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f6d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts(\n",
    "    texts,embedding=embed_model,\n",
    "    persist_directory=\"My_First_ChromaDB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54bf7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samreen Fathima\\AppData\\Local\\Temp\\ipykernel_4552\\398866168.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samreen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
